oql[<phrase>c++</phrase>]: extending <phrase>c++</phrase> with an object query capability.
transaction <phrase>management</phrase> in multidatabase systems.
overview of the adds system.
<phrase>multimedia</phrase> <phrase>information</phrase> systems: issues and approaches.
active <phrase>database systems</phrase>.
where <phrase>object-oriented</phrase> dbmss should do better: a critique based on early experiences.
distributed <phrase>databases</phrase>.
an <phrase>object-oriented</phrase> <phrase>dbms</phrase> <phrase>war</phrase> story: developing a <phrase>genome</phrase> mapping <phrase>database</phrase> in <phrase>c++</phrase>.
<phrase>cooperative</phrase> transactions for multiuser environments.
schema <phrase>architecture</phrase> of the unisql/m multidatabase system
physical object <phrase>management</phrase>.
introduction to part 1: next-generation <phrase>database</phrase> <phrase>technology</phrase>.
<phrase>object-oriented</phrase> <phrase>database systems</phrase>: promises, <phrase>reality</phrase>, and future.
introduction to part 2: <phrase>technology</phrase> for interoperating legacy <phrase>databases</phrase>.
on resolving schematic heterogeneity in multidatabase systems.
requirements for a performance benchmark for <phrase>object-oriented</phrase> <phrase>database systems</phrase>.
on view support in <phrase>object-oriented</phrase> <phrase>databases</phrase> systems.
the posc <phrase>solution</phrase> to managing <phrase>e</phrase>&p <phrase>data</phrase>.
<phrase>c++</phrase> bindings to an <phrase>object database</phrase>.
authorization in <phrase>object-oriented</phrase> <phrase>databases</phrase>.
<phrase>query processing</phrase> in multidatabase systems.
<phrase>management</phrase> of uncerainty in <phrase>database systems</phrase>.
parallel <phrase>relational database</phrase> systems.
<phrase>query processing</phrase> in <phrase>object-oriented</phrase> <phrase>database systems</phrase>.
specification and execution of transactional workflows.
spatial <phrase>data structures</phrase>.
an overview is presented of the use of spatial <phrase>data structures</phrase> in spatial <phrase>databases</phrase>. the focus is on hierarchical <phrase>data structures</phrase>, including a number of variants of quadtrees, which sort the <phrase>data</phrase> with respect to the space occupied by it. such techniques are known as spatial indexing methods. hierarchical <phrase>data structures</phrase> are based on the principle of recursive decomposition. they are attractive because they are compact and depending on the <phrase>nature</phrase> of the <phrase>data</phrase> they save space as well as time and also facilitate operations such as search. examples are given of the use of these <phrase>data structures</phrase> in the representation of different <phrase>data</phrase> types such as regions, points, rectangles, lines, and volumes.
spatial <phrase>data</phrase> models and <phrase>query processing</phrase>.
<phrase>pegasus</phrase>: a heterogeneous <phrase>information management</phrase> system.
temporal <phrase>object-oriented</phrase> <phrase>databases</phrase>: a critical comparison.
the <phrase>omg</phrase> object <phrase>model</phrase>.
<phrase>eda</phrase>/<phrase>sql</phrase>.
the changing <phrase>database</phrase> standards <phrase>landscape</phrase>.
modern <phrase>database systems</phrase>: the object <phrase>model</phrase>, <phrase>interoperability</phrase>, and beyond.
inequalities: theory of majorization and its application.
<phrase>version control</phrase> in an <phrase>object-oriented</phrase> <phrase>architecture</phrase>.
the <phrase>gemstone</phrase> <phrase>data management</phrase> system.
storage <phrase>management</phrase> in exodus.
a distributed object <phrase>manager</phrase> for the <phrase>smalltalk</phrase>-80 system.
objects, messages, and rules in <phrase>database design</phrase>.
active objects: ealities and possibilities.
overview of the iris <phrase>dbms</phrase>.
features of the orion <phrase>object-oriented</phrase> <phrase>database</phrase> system.
<phrase>indexing techniques</phrase> for <phrase>object-oriented</phrase> <phrase>databases</phrase>.
my <phrase>cat</phrase> is <phrase>object-oriented</phrase>.
making <phrase>database systems</phrase> fast enough for <phrase>cad</phrase> applications.
optimizing <phrase>smalltalk</phrase> message performance.
the common list <phrase>object-oriented programming language</phrase> standard.
object orientation as <phrase>catalyst</phrase> for <phrase>language</phrase>-<phrase>database</phrase> inegration.
a survey of <phrase>object-oriented</phrase> concepts.
integrated office systems.
<phrase>proteus</phrase>: a frame-based nonmonotonic inference system.
<phrase>concurrency control</phrase> and <phrase>object-oriented</phrase> <phrase>databases</phrase>.
a shared view of sharing: the <phrase>treaty</phrase> of <phrase>orlando</phrase>.
pogo: a declarative representation system for graphics.
concurrent <phrase>object-oriented programming</phrase> languages.
directions in <phrase>object-oriented</phrase> <phrase>research</phrase>.
a proposal for a formal <phrase>model</phrase> of objects.
oz+: <phrase>an object-oriented database</phrase> system.
the commercial <phrase>ingres</phrase> epilogue.
<phrase>design</phrase> of <phrase>relational</phrase> systems (introduction to section 1).
supporting studies on <phrase>relational</phrase> systems (introduction to section 2).
<phrase>distributed database</phrase> systems (introduction to section 3).
the <phrase>design</phrase> and implementation of distributed <phrase>ingres</phrase>.
<phrase>user interfaces</phrase> for <phrase>database systems</phrase> (introduction to section 4).
extended <phrase>semantics</phrase> for the <phrase>relational model</phrase> (introduction to section 5).
<phrase>database design</phrase> (introduction to section 6).
title, preface, contents.
references.
tex: the program
foundations of <phrase>databases</phrase>.
<phrase>latex</phrase>: user's guide & <phrase>reference manual</phrase>
the <phrase>design</phrase> and analysis of computer <phrase>algorithms</phrase>.
specifying systems, the <phrase>tla+</phrase> <phrase>language</phrase> and tools for hardware and <phrase>software</phrase> engineers
from the <phrase>book</phrase>: this <phrase>book</phrase> will teach you how to write specifications of computer systems, using the <phrase>language</phrase> <phrase>tla+</phrase>. it's rather <phrase>long</phrase>, but most people will read only part i, which comprises the first 83 pages. that part contains all that most engineers need to know about writing specifications; it assumes only the <phrase>basic</phrase> background in <phrase>computing</phrase> and <phrase>knowledge</phrase> of <phrase>mathematics</phrase> expected of an <phrase>undergraduate</phrase> studying <phrase>engineering</phrase> or computer <phrase>science</phrase>. part <phrase>ii</phrase> contains more advanced material for more sophisticated readers. the remainder of the <phrase>book</phrase> is a reference manualpart iii for the <phrase>tla+</phrase> tools and part iv for the <phrase>language</phrase> itself. the tla <phrase>world wide web</phrase> page contains material to accompany the <phrase>book</phrase>, including the <phrase>tla+</phrase> tools, exercises, references to the <phrase>literature</phrase>, and a list of corrections. there is a link to the tla web page on http://lamport.org.what is a specification?writing is nature's way of letting you know how sloppy your thinking is.-guindona specification is a written description of what a system is supposed to do. specifying a system helps us understand it. it's a good idea to understand a system before building it, so it's a good idea to write a specification of a system before implementing it.this <phrase>book</phrase> is about specifying the behavioral properties of a systemalso called its functional or logical properties. these are the properties that specify what the system is supposed to do. there are other important kinds of properties that we don't consider, including performance properties. worst-case performance can often be expressed as a behavioral propertyforexample, chapter 9 explains how to specify that a system must react within a certain length of time. however, specifying <phrase>average</phrase> performance is beyond the scope of the methods described here.our <phrase>basic</phrase> tool for writing specifications is <phrase>mathematics</phrase>. <phrase>mathematics</phrase> is nature's way of letting you know how sloppy your writing is. it's hard to be precise in an imprecise <phrase>language</phrase> like <phrase>english</phrase> or <phrase>chinese</phrase>. in <phrase>engineering</phrase>, imprecision can <phrase>lead</phrase> to errors. to avoid errors, <phrase>science</phrase> and <phrase>engineering</phrase> have adopted <phrase>mathematics</phrase> as their language.the <phrase>mathematics</phrase> we use is more formal than the <phrase>math</phrase> you've grown up with. formal <phrase>mathematics</phrase> is nature's way of letting you know how sloppy your <phrase>mathematics</phrase> is. the <phrase>mathematics</phrase> written by most <phrase>mathematicians</phrase> and scientists is not really precise. it's precise in the small, but imprecise in the large. each equation is a precise assertion, but you have to read the accompanying words to understand how the equations relate to one another and exactly what the theorems mean. logicians have developed ways of eliminating those words and making the <phrase>mathematics</phrase> completely formal, and hence completely precise.most <phrase>mathematicians</phrase> and scientists think that formal <phrase>mathematics</phrase>, without words, is <phrase>long</phrase> and tiresome. they're wrong. ordinary <phrase>mathematics</phrase> can be expressed compactly in a precise, completely <phrase>formal language</phrase>. it takes only about two dozen lines to define the <phrase>solution</phrase> to an arbitrary <phrase>differential equation</phrase> in the <phrase>differential equations</phrase> module of chapter 11. but few specifications need such sophisticated <phrase>mathematics</phrase>. most require only simple application of a few standard <phrase>mathematical</phrase> concepts.why <phrase>tla+</phrase>?we specify a system by describing its allowed behaviorswhat it may do in the course of an execution. in 1977, <phrase>amir pnueli</phrase> introduced the use of <phrase>temporal logic</phrase> for describing system behaviors. in principle, a system could be described by a <phrase>single</phrase> <phrase>temporal logic</phrase> formula. in practice, it couldn't. pnueli's <phrase>temporal logic</phrase> was ideal for describing some properties of systems, but awkward for others. so, it was usually combined with a more traditional way of describing systems.in the late 1980s, i invented tla, the <phrase>temporal logic</phrase> of actionsa simple variant of pnueli's original <phrase>logic</phrase>. tla makes it practical to describe a system by a <phrase>single</phrase> formula. most of a tla specification consists of ordinary, nontemporal <phrase>mathematics</phrase>. <phrase>temporal logic</phrase> plays a significant role only in describing those properties that it's good at describing. tla also provides a <phrase>nice</phrase> way to formalize the style of reasoning about systems that has proved to be most effective in practicea style known as assertional reasoning. however, this <phrase>book</phrase> is about specification; it says almost nothing about proofs.temporal <phrase>logic</phrase> assumes an underlying <phrase>logic</phrase> for expressing ordinary <phrase>mathematics</phrase>. there are many ways to formalize ordinary <phrase>math</phrase>. most computer scientists prefer one that resembles their favorite <phrase>programming language</phrase>. i chose instead the one that most <phrase>mathematicians</phrase> preferthe one logicians call <phrase>first-order logic</phrase> and set theory.tla provides a <phrase>mathematical</phrase> foundation for describing systems. to write specifications, we need a complete <phrase>language</phrase> built atop that foundation. i initially thought that this <phrase>language</phrase> should be some sort of abstract <phrase>programming language</phrase> whose <phrase>semantics</phrase> would be based on tla. i didn't know what kind of <phrase>programming language</phrase> constructs would be best, so i decided to start writing specifications directly in tla. i intended to introduce <phrase>programming</phrase> constructs as i needed them. to my surprise, i discovered that i didn't need them. what i needed was a robust <phrase>language</phrase> for writing mathematics.although <phrase>mathematicians</phrase> have developed the <phrase>science</phrase> of writing formulas, they haven't turned that <phrase>science</phrase> into an <phrase>engineering</phrase> discipline. they have developed notations for <phrase>mathematics</phrase> in the small, but not for <phrase>mathematics</phrase> in the large. the specification of a real system can be dozens or even hundreds of pages <phrase>long</phrase>. <phrase>mathematicians</phrase> know how to write 20-line formulas, not 20-page formulas. so, i had to introduce notations for writing <phrase>long</phrase> formulas. what i took from <phrase>programming languages</phrase> were ideas for modularizing large specifications.the <phrase>language</phrase> i came up with is called <phrase>tla+</phrase>. i refined <phrase>tla+</phrase> in the course of writing specifications of disparate systems. but it has changed little in the last few years. i have found <phrase>tla+</phrase> to be quite good for specifying a wide class of systemsfrom program interfaces (apis) to <phrase>distributed systems</phrase>. it can be used to write a precise, formal description of almost any sort of discrete system. it's especially well suited to describing asynchronous systemsthat is, systems with components that do not operate in strict lock-step.about this bookpart i, consisting of chapters 1 through 7, is the core of the <phrase>book</phrase> and is meant to be read from beginning to end. it explains how to specify the class of properties known as <phrase>safety</phrase> properties. these properties, which can be specified with almost no <phrase>temporal logic</phrase>, are all that most engineers need to know about. after <phrase>reading</phrase> part i, you can read as much of part <phrase>ii</phrase> as you like. each of its chapters is <phrase>independent</phrase> of the others. <phrase>temporal logic</phrase> comes to the fore in chapter 8, where it is used to specify the additional class of properties known as liveness properties. chapter 9 describes how to specify real-time properties, and chapter 10 describes how to write specifications as compositions. chapter 11 contains more advanced examples. the three chapters in part iii serve as the <phrase>reference manual</phrase> for three <phrase>tla+</phrase> tools: the <phrase>syntactic</phrase> analyzer, the tlatex <phrase>typesetting</phrase> program, and the tlc <phrase>model</phrase> checker. if you want to use <phrase>tla+</phrase>, then you probably want to use these tools. they are available from the tla web page. tlc is the most sophisticated of them. the examples on the web can get you started using it, but you'll have to read chapter 14 to learn to use tlc effectively.part iv is a <phrase>reference manual</phrase> for the <phrase>tla+</phrase> <phrase>language</phrase>. part i provides a good enough working <phrase>knowledge</phrase> of the <phrase>language</phrase> for most purposes. you need look at part iv only if you have questions about the fine points of the <phrase>syntax</phrase> and <phrase>semantics</phrase>. chapter 15 gives the <phrase>syntax</phrase> of <phrase>tla+</phrase>. chapter 16 describes the precise meanings and the <phrase>general</phrase> forms of all the built-in operators of <phrase>tla+</phrase>; chapter 17 describes the precise meaning of all the higher-level <phrase>tla+</phrase> constructs such as definitions. together, these two chapters specify the <phrase>semantics</phrase> of the <phrase>language</phrase>. chapter 18 describes the standard modulesexcept for module realtime, described in chapter 9, and module tlc, described in chapter 14. you might want to look at this chapter if you're curious about how standard <phrase>elementary</phrase> <phrase>mathematics</phrase> can be formalized in <phrase>tla+</phrase>.part iv does have something you may want to refer to often: a <phrase>mini</phrase>-manual that compactly presents lots of useful <phrase>information</phrase>. pages 268-273 list all <phrase>tla+</phrase> operators, all user-definable symbols, the precedence of all operators, all operators defined in the standard modules, and the <phrase>ascii</phrase> representation of symbols.
<phrase>data structures</phrase> and <phrase>algorithms</phrase>.
<phrase>databases</phrase> and <phrase>transaction processing</phrase>: an application-oriented approach
the <phrase>awk</phrase> <phrase>programming language</phrase>
the <phrase>java virtual machine</phrase> specification
compilers: princiles, techniques, and tools.
<phrase>algorithms</phrase>
this presentation is a tutorial on autoview, an automod extension package that recreates <phrase>model</phrase> <phrase>animation</phrase> according to a user-defined script. autoview allows users to restart <phrase>animation</phrase> and move back and forth in time and 3-d space. <phrase>animation</phrase> is created using text files generated from an automod <phrase>simulation</phrase> <phrase>model</phrase>. by creating a user-defined <phrase>camera</phrase> description file, <phrase>animation</phrase> can take place that allows for panning from one view to another, as well as for attaching the <phrase>camera</phrase> to a simulated load or vehicle and traveling with that entity during portions of the <phrase>animation</phrase>. the <phrase>animation</phrase> is provided at an enhanced response time because it does not have the logical and statistical processes of automod (which can slow <phrase>animation</phrase>). this tutorial demonstrates several <phrase>animation</phrase> scripts previously constructed with autoview, discusses the creation of autoview files from automod, and describes the development of a <phrase>camera</phrase> description file.
the <phrase>java programming language</phrase>
<phrase>algorithms</phrase>, <phrase>2nd edition</phrase>
the <phrase>java programming language</phrase>, second edition
operating system concepts, 4th edition.
the <phrase>java programming language</phrase>, third edition
the texbook
modern <phrase>information retrieval</phrase>
:this is a rigorous and complete <phrase>textbook</phrase> for a first course on <phrase>information retrieval</phrase> from the computer <phrase>science</phrase> (as opposed to a user-centred) perspective. the advent of the <phrase>internet</phrase> and the enormous increase in volume of electronically stored <phrase>information</phrase> generally has <phrase>led</phrase> to substantial work on ir from the computer <phrase>science</phrase> perspective - this <phrase>book</phrase> provides an up-to-date <phrase>student</phrase> oriented treatment of the subject.
conceptual structures: <phrase>information processing</phrase> in mind and machine
<phrase>concurrency control</phrase> and recovery in <phrase>database systems</phrase>.
code <phrase>reading</phrase>: the <phrase>open source</phrase> perspective
intelligent <phrase>database systems</phrase>
:a valuable resource for anyone working on intelligent <phrase>database systems</phrase>. good background coverage of both the field of <phrase>databases</phrase> and that of <phrase>ai</phrase> are provided in the first part of the <phrase>book</phrase>, and later there are some excellent analytical discussions of relevant projects. the authors adopt a very readable style which enables a complex topic to become much more accessible." - jenny carter, <phrase>department</phrase> of computer <phrase>science</phrase>, de montfort <phrase>university</phrase> this <phrase>book</phrase> provides a <phrase>state</phrase> of the <phrase>art</phrase> guide to the new developments in expert <phrase>database systems</phrase>, from the unique perspective of both the <phrase>database</phrase> and <phrase>ai</phrase> areas. it gives complete and detailed coverage of the latest <phrase>research</phrase> and practice, including all the need-to-know technical and theoretical approaches in the <phrase>area</phrase>. <phrase>drawing</phrase> on their extensive experience, the authors evaluate how <phrase>ai</phrase> techniques can be integrated with present and future <phrase>database systems</phrase> and <phrase>knowledge</phrase> based <phrase>management</phrase> systems, incorporating <phrase>ai</phrase> expertise into system <phrase>design</phrase>. the <phrase>book</phrase> also addresses the techniques developed recently which are directly used, or are the basis, for <phrase>data</phrase> retrieval across the <phrase>world wide web</phrase>. if you are a designer or developer working in the <phrase>database</phrase> <phrase>community</phrase>, with <phrase>database</phrase> and <phrase>ai</phrase> <phrase>products</phrase> or applications, this <phrase>book</phrase> will help you to understand crucial <phrase>research</phrase> developments and to apply the <phrase>results</phrase> in practice. includes: &middot; mechanisms for handling <phrase>data</phrase> and <phrase>knowledge</phrase> including <phrase>xml</phrase>, web indexing, search engines & <phrase>data mining</phrase> &middot; <phrase>object oriented</phrase> <phrase>database management systems</phrase> and <phrase>object-relational</phrase> <phrase>dbms</phrase> &middot; <phrase>data modeling</phrase> including techniques suchas entity relationship, functional <phrase>data</phrase> & <phrase>semantic</phrase> <phrase>database</phrase> models, as well as using the notations for modeling (omt & <phrase>uml</phrase>) plus! extended case studies of commercial systems about the authors: <phrase>elisa</phrase> bertino is a well known expert in the integration of <phrase>ai</phrase> and <phrase>database</phrase> techniques, areas of o-o, distributed, <phrase>deductive</phrase> and <phrase>multimedia</phrase> <phrase>databases</phrase> and <phrase>database</phrase> <phrase>security</phrase>. she has chaired and given tutorials at many international conferences and published hundreds of <phrase>journal</phrase> papers and a <phrase>book</phrase>. <phrase>elisa</phrase> is currently a <phrase>professor</phrase> of computer <phrase>science</phrase> at the <phrase>university</phrase> of <phrase>milan</phrase>. barbara <phrase>catania</phrase> is an assistant <phrase>professor</phrase> of computer <phrase>science</phrase> at the <phrase>university</phrase> of <phrase>genova</phrase>, specializing in <phrase>deductive</phrase> and <phrase>multimedia</phrase> <phrase>databases</phrase>, and <phrase>indexing techniques</phrase> in <phrase>object-oriented</phrase> and constraint <phrase>databases</phrase>. she has presented at a number of international conferences and co-authored a <phrase>book</phrase>. gian piero zarri is an internationally renowned consultant and researcher in the areas of <phrase>knowledge-based systems</phrase>, <phrase>natural-language processing</phrase>, <phrase>databases</phrase> and <phrase>information retrieval</phrase> systems. he is on the editorial board of a number of international <phrase>scientific journals</phrase> and on the program committee of many conferences on <phrase>knowledge-based systems</phrase>. gian piero currently works as <phrase>research</phrase> <phrase>director</phrase> for <phrase>cnrs</phrase>, the <phrase>french national centre for scientific research</phrase>.
how to set up and maintain a <phrase>world wide web</phrase> site: the guide for <phrase>information</phrase> providers.
<phrase>object-oriented</phrase> <phrase>database systems</phrase>
advanced <phrase>programming</phrase> in the <phrase>unix</phrase> environment
<phrase>prolog</phrase> <phrase>programming</phrase> for <phrase>artificial intelligence</phrase>, second edition
<phrase>tcp/ip</phrase> illustrated, volume 1: the protocols
<phrase>tcp/ip</phrase> illustrated, volume 1 is a complete and detailed guide to the entire <phrase>tcp/ip</phrase> protocol suite - with an important difference from other books on the subject. rather than just describing what the rfcs say the protocol suite should do, this unique <phrase>book</phrase> uses a popular diagnostic tool so you may actually <phrase>watch</phrase> the protocols in action.by forcing various conditions to occur - such as connection establishment, timeout and retransmission, and fragmentation - and then displaying the <phrase>results</phrase>, <phrase>tcp/ip</phrase> illustrated gives you a much greater understanding of these concepts than words alone could provide. whether you are new to <phrase>tcp/ip</phrase> or you have read other books on the subject, you will come away with an increased understanding of how and why <phrase>tcp/ip</phrase> works the way it does, as well as enhanced skill at developing aplications that run over <phrase>tcp/ip</phrase>.
<phrase>database</phrase> <phrase>security</phrase>.
<phrase>tcp/ip</phrase> illustrated, volume 3: <phrase>tcp</phrase> for transactions, http, <phrase>nntp</phrase>, and the <phrase>unix</phrase> domain protocolls
object <phrase>data management</phrase>: <phrase>object-oriented</phrase> and extended <phrase>relational database</phrase> systems
object <phrase>data management</phrase>: <phrase>object-oriented</phrase> and extended <phrase>relational database</phrase> systems (revised edition)
the <phrase>c++</phrase> <phrase>programming language</phrase>, first edition
designing <phrase>database</phrase> applications with objects and rules: the idea methodology
the <phrase>c++</phrase> <phrase>programming language</phrase>, second edition
the <phrase>java</phrase> developers almanac 1999
introduction to <phrase>data mining</phrase>
the <phrase>java</phrase> class <phrase>libraries</phrase> - an annotated reference
datenmodelle, datenbanksprachen und datenbank-<phrase>management</phrase>-systeme
the <phrase>java</phrase> class <phrase>libraries</phrase>, second edition, volume 2
datenmodelle, datenbanksprachen und datenbank-<phrase>management</phrase>-systeme. 2. auflage
the <phrase>java</phrase> class <phrase>libraries</phrase>, second edition, volume 1
das <phrase>db2</phrase>-handbuch.
the <phrase>java</phrase> class <phrase>libraries</phrase>, second edition, volume 1, supplement for the <phrase>java</phrase> 2 platform standard edition, v1.2
time series prediction: forecasting the future and understanding the past.
the <phrase>relational model</phrase> for <phrase>database management</phrase>, version 2
from the preface (see front <phrase>matter</phrase> for full preface) an important adjunct to precision is a <phrase>sound</phrase> theoretical foundation. the <phrase>relational model</phrase> is solidly based on two parts of <phrase>mathematics</phrase>: firstorder <phrase>predicate logic</phrase> and the theory of relations. this <phrase>book</phrase>, however, does not dwell on the theoretical foundations, but rather on all the features of the <phrase>relational model</phrase> that i now perceive as important for <phrase>database</phrase> users, and therefore for <phrase>dbms</phrase> vendors. my perceptions result from 20 years of practical experience in <phrase>computing</phrase> and <phrase>data processing</phrase> (chiefly, but not exclusively, with <phrase>large-scale</phrase> customers of <phrase>ibm</phrase>), followed by another 20 years of <phrase>research</phrase>. i believe that this is the first <phrase>book</phrase> to deal exclusively with the <phrase>relational</phrase> approach. it does, however, include <phrase>design</phrase> principles in chapters 21 and 22. it is also the first <phrase>book</phrase> on the <phrase>relational model</phrase> by the originator of that <phrase>model</phrase>. all the ideas in the <phrase>relational model</phrase> described in this <phrase>book</phrase> are mine, except in cases where i explicitly credit someone else. in developing the <phrase>relational model</phrase>, i have tried to follow einstein's advice, "make it as simple as possible, but no simpler." i believe that in the last clause he was discouraging the pursuit of simplicity to the extent of distorting <phrase>reality</phrase>. so why does the <phrase>book</phrase> contain 30 chapters and two appendixes? to answer this question, it is necessary to look at the <phrase>history</phrase> of <phrase>research</phrase> and development of the <phrase>relational model</phrase>.
<phrase>java</phrase> platform performance - strategies and tactics
:this <phrase>book</phrase> addresses a vital issue for all those developing <phrase>software</phrase> for the <phrase>java</phrase> platform: how to achieve maximum performance and <phrase>scalability</phrase> for their applications. <phrase>drawing</phrase> on the authors' <phrase>knowledge</phrase> of the <phrase>java programming language</phrase> and their extensive experience working on performance issues, the <phrase>book</phrase> reveals common mistakes and misconceptions concerning the performance characteristics of <phrase>java</phrase> technologies. it offers overall development strategies and <phrase>concrete</phrase>, battle-tested techniques to dramatically improve the performance of applications constructed with the <phrase>java programming language</phrase>. <phrase>java</phrase> platform performance highlights the importance of integrating performance evaluation into the application development process and discusses measurement techniques. the <phrase>book</phrase> then presents practical tactics for enhancing application performance in the areas of i/o, <phrase>ram</phrase> footprint, small object <phrase>management</phrase>, <phrase>algorithms</phrase>, <phrase>data structures</phrase>, swing, and deployment. specific topics <phrase>covered</phrase> include: incorporating performance evaluation into the development process profiling and <phrase>benchmarking</phrase> building scalable, fast swing guis using <phrase>high</phrase>-speed i/o <phrase>computing</phrase> and controlling the <phrase>ram</phrase> footprint reducing the number of classes eliminating temporary objects selecting <phrase>high</phrase>-performance <phrase>algorithms</phrase> and <phrase>data structures</phrase> using <phrase>java</phrase> <phrase>native</phrase> code and <phrase>applet</phrase> packaging efficiently <phrase>garbage collection</phrase> <phrase>java</phrase> hotspot <phrase>technology</phrase> with an understanding of the performance issues and specific techniques for reducing overhead discussed in this <phrase>book</phrase>, you will have theinformation you need to enhance the efficiency, speed, and <phrase>scalability</phrase> of your <phrase>software</phrase>.
advanced <phrase>c++</phrase>: <phrase>programming</phrase> syles and <phrase>idioms</phrase>
<phrase>deductive</phrase> <phrase>databases</phrase> and <phrase>logic programming</phrase>
<phrase>high</phrase> performance compilers for <phrase>parallel computing</phrase>
an introduction to <phrase>database systems</phrase>
:the sixth edition of this well-respected text/reference--which has been almost completely rewritten--continues to be the most <phrase>comprehensive</phrase> and up-to-date treatment of <phrase>database</phrase> <phrase>technology</phrase> currently available. readers will gain a strong working <phrase>knowledge</phrase> of the overall structure, concepts, and objectives of <phrase>database systems</phrase> and will become familiar with the theoretical principles underlying the <phrase>construction</phrase> of such systems.
<phrase>tcp/ip</phrase> illustrated, volume 2: the implementation
an introduction to <phrase>database systems</phrase>, <phrase>2nd edition</phrase>
<phrase>human</phrase> behaviour and the principle of least effort: an introduction to <phrase>human ecology</phrase>
an introduction to <phrase>database systems</phrase>, <phrase>3rd edition</phrase>
:for over 25 years, c. j. date's an introduction to <phrase>database systems</phrase> has been the authoritative resource for readers interested in gaining insight into and understanding of the principles of <phrase>database systems</phrase>. this revision continues to provide a solid grounding in the foundations of <phrase>database</phrase> <phrase>technology</phrase> and to provide some ideas as to how the field is likely to develop in the future.. "readers of this <phrase>book</phrase> will gain a strong working <phrase>knowledge</phrase> of the overall structure, concepts, and objectives of <phrase>database systems</phrase> and will become familiar with the theoretical principles underlying the <phrase>construction</phrase> of such systems.
the <phrase>ingres</phrase> papers: <phrase>anatomy</phrase> of a <phrase>relational database</phrase> system
an introduction to <phrase>database systems</phrase>, volume <phrase>ii</phrase>.
a guide to <phrase>db2</phrase>, 1st edition
an introduction to <phrase>database systems</phrase>, volume i, 4th edition.
an introduction to <phrase>database systems</phrase>, volume i, 5th edition.
an introduction to <phrase>database systems</phrase>, 6th edition.
<phrase>relational database</phrase> writings 1989-1991
a guide to <phrase>sql</phrase> standard, <phrase>3rd edition</phrase>
a guide to <phrase>sql</phrase> standard, 4th edition
a guide to <phrase>db2</phrase>, <phrase>2nd edition</phrase>
a guide to <phrase>db2</phrase>, <phrase>3rd edition</phrase>
<phrase>cryptography</phrase> and <phrase>data security</phrase>
from the preface (see front <phrase>matter</phrase> for full preface) <phrase>electronic</phrase> <phrase>computers</phrase> have evolved from exiguous <phrase>experimental</phrase> enterprises in the 1940s to prolific practical <phrase>data processing</phrase> systems in the 1980s. as we have come to rely on these systems to process and store <phrase>data</phrase>, we have also come to wonder about their ability to protect valuable <phrase>data</phrase>. <phrase>data security</phrase> is the <phrase>science</phrase> and study of methods of protecting <phrase>data</phrase> in computer and <phrase>communication</phrase> systems from unauthorized disclosure and modification. the goal of this <phrase>book</phrase> is to introduce the <phrase>mathematical</phrase> principles of <phrase>data security</phrase> and to show how these principles apply to <phrase>operating systems</phrase>, <phrase>database systems</phrase>, and computer networks. the <phrase>book</phrase> is for students and professionals seeking an introduction to these principles. there are many references for those who would like to study specific topics further. <phrase>data security</phrase> has evolved rapidly since 1975. we have seen exciting developments in <phrase>cryptography</phrase>: <phrase>public</phrase>-key <phrase>encryption</phrase>, <phrase>digital signatures</phrase>, the <phrase>data</phrase> <phrase>encryption</phrase> standard (des), key safeguarding schemes, and key distribution protocols. we have developed techniques for verifying that programs do not leak confidential <phrase>data</phrase>, or transmit classified <phrase>data</phrase> to users with <phrase>lower</phrase> <phrase>security</phrase> <phrase>clearances</phrase>. we have found new controls for protecting <phrase>data</phrase> in statistical <phrase>databases</phrase>--and new methods of attacking these <phrase>databases</phrase>. we have come to a better understanding of the theoretical and practical limitations to <phrase>security</phrase>.
the annotated <phrase>c++</phrase> <phrase>reference manual</phrase>.
<phrase>functional programming</phrase>
<phrase>relational databases</phrase> and <phrase>knowledge</phrase> bases
<phrase>natural language processing</phrase> in <phrase>prolog</phrase>
<phrase>genetic algorithms</phrase> in search optimization and <phrase>machine learning</phrase>
<phrase>smalltalk</phrase>-80: the <phrase>language</phrase> and its implementation.
from the preface (see front <phrase>matter</phrase> for full preface) advances in the <phrase>design</phrase> and <phrase>production</phrase> of computer hardware have brought many more people into direct contact with <phrase>computers</phrase>. similar advances in the <phrase>design</phrase> and <phrase>production</phrase> of computer <phrase>software</phrase> are required in <phrase>order</phrase> that this increased contact be as rewarding as possible. the <phrase>smalltalk</phrase>-80 system is a result of a decade of <phrase>research</phrase> into creating computer <phrase>software</phrase> that is appropriate for producing highly functional and interactive contact with personal computer systems. this <phrase>book</phrase> is the first detailed account of the <phrase>smalltalk</phrase>-80 system. it is divided into four <phrase>major</phrase> parts: part one -- an overview of the concepts and <phrase>syntax</phrase> of the <phrase>programming language</phrase>. part two -- an annotated and illustrated specification of the system's functionality. part three -- an example of the <phrase>design</phrase> and implementation of a moderate-size application. part four -- a specification of the <phrase>smalltalk</phrase>-80 <phrase>virtual machine</phrase>.
the <phrase>java</phrase> <phrase>language</phrase> specification.
a retargetable c <phrase>compiler</phrase>: <phrase>design</phrase> and implementation
objektorientierte datenbanken: konzepte, modelle, systeme.
objektorientierte datenbanken: konzepte, modelle, systeme, 2. auflage.
introduction to <phrase>automata theory</phrase>, languages and computation.
an introduction to parallel <phrase>algorithms</phrase>
introduction to <phrase>expert systems</phrase>, 1st edition.
introduction to <phrase>expert systems</phrase>, <phrase>2nd edition</phrase>.
:the third edition of peter jackson's <phrase>book</phrase>, introduction to <phrase>expert systems</phrase>, updates the technological base of <phrase>expert systems</phrase> <phrase>research</phrase> and embeds those <phrase>results</phrase> in the context of a wide <phrase>variety</phrase> of application areas. the earlier chapters take a more practical approach to the <phrase>basic</phrase> topics than the previous editions, while the later chapters introduce new topic areas, such as <phrase>case-based reasoning</phrase>, <phrase>connectionist</phrase> systems, and <phrase>hybrid</phrase> systems. <phrase>results</phrase> in related areas, such as <phrase>machine learning</phrase> and reasoning with uncertainty are also accorded a thorough treatment.
introduction to <phrase>expert systems</phrase>, <phrase>3rd edition</phrase>.
:the third edition of peter jackson's <phrase>book</phrase>, introduction to <phrase>expert systems</phrase>, updates the technological base of <phrase>expert systems</phrase> <phrase>research</phrase> and embeds those <phrase>results</phrase> in the context of a wide <phrase>variety</phrase> of application areas. the earlier chapters take a more practical approach to the <phrase>basic</phrase> topics than the previous editions, while the later chapters introduce new topic areas, such as <phrase>case-based reasoning</phrase>, <phrase>connectionist</phrase> systems, and <phrase>hybrid</phrase> systems. <phrase>results</phrase> in related areas, such as <phrase>machine learning</phrase> and reasoning with uncertainty are also accorded a thorough treatment.
the practice of <phrase>programming</phrase>
<phrase>object-oriented</phrase> concepts, <phrase>databases</phrase>, and applications.
the <phrase>art</phrase> of computer <phrase>programming</phrase>, volume i: fundamental <phrase>algorithms</phrase>
the <phrase>art</phrase> of computer <phrase>programming</phrase>, volume <phrase>ii</phrase>: seminumerical <phrase>algorithms</phrase>
the <phrase>art</phrase> of computer <phrase>programming</phrase>, volume iii: sorting and searching
the <phrase>art</phrase> of computer <phrase>programming</phrase>, volume i: fundamental <phrase>algorithms</phrase>, <phrase>2nd edition</phrase>.
the <phrase>art</phrase> of computer <phrase>programming</phrase>, volume <phrase>ii</phrase>: seminumerical <phrase>algorithms</phrase>, <phrase>2nd edition</phrase>
inside the <phrase>c++</phrase> object <phrase>model</phrase>
inside the <phrase>c++</phrase> object <phrase>model</phrase> focuses on the underlying mechanisms that support <phrase>object-oriented programming</phrase> within <phrase>c++</phrase>: constructor <phrase>semantics</phrase>, temporary generation, support for encapsulation, <phrase>inheritance</phrase>, and "the virtuals"-virtual functions and virtual <phrase>inheritance</phrase>. this <phrase>book</phrase> shows how your understanding the underlying implementation models can help you code more efficiently and with greater confidence. lippman dispells the misinformation and myths about the overhead and complexity associated with <phrase>c++</phrase>, while pointing out areas in which costs and <phrase>trade</phrase> offs, sometimes hidden, do exist. he then explains how the various implementation models arose, points out areas in which they are likely to evolve, and why they are what they are. he covers the <phrase>semantic</phrase> implications of the <phrase>c++</phrase> object <phrase>model</phrase> and how that <phrase>model</phrase> affects your programs.highlights explores the program behavior implicit in the <phrase>c++</phrase> object model's support of <phrase>object-oriented programming</phrase>. explains the <phrase>basic</phrase> implementation of the <phrase>object-oriented</phrase> features and the <phrase>trade</phrase> offs implicit in those features. examines the impact on performance in terms of <phrase>program transformation</phrase>. provides abundant program examples, diagrams, and performance measurements to relate <phrase>object-oriented</phrase> concepts to the underlying object model.if you are a <phrase>c++</phrase> <phrase>programmer</phrase> who desires a fuller understanding of what is going on "under the hood," then inside the <phrase>c++</phrase> object <phrase>model</phrase> is for you!
introduction to linear and <phrase>nonlinear programming</phrase>.
<phrase>design</phrase> of <phrase>relational databases</phrase>
<phrase>tcl</phrase> and the tk toolkit
from the book:tcl was born of frustration. in the early 1980s my students and i developed a number of interactive tools at the <phrase>university</phrase> of <phrase>california</phrase> at <phrase>berkeley</phrase>, mostly for <phrase>integrated circuit design</phrase>, and we found ourselves spending a lot of time building bad command languages. each tool needed to have a command <phrase>language</phrase> of some sort, but our main interest was in the tool rather than its command <phrase>language</phrase>. we spent as little time as possible on the command <phrase>language</phrase> and always ended up with a <phrase>language</phrase> that was weak and quirky. furthermore, the command <phrase>language</phrase> for one tool was never quite right for the next tool, so we ended up building a new bad command <phrase>language</phrase> for each tool. this became increasingly frustrating.in the fall of 1987 it occurred to me that the <phrase>solution</phrase> was to build a reusable command <phrase>language</phrase>. if a <phrase>general</phrase>-purpose <phrase>scripting language</phrase> could be built as a c <phrase>library</phrase> package, then perhaps it could be reused for many different purposes in many different applications. of course, the <phrase>language</phrase> would need to be extensible so that each application could add its own specific features to the core provided by the <phrase>library</phrase>. in the spring of 1988 i decided to implement such a <phrase>language</phrase>, and the result was <phrase>tcl</phrase>. tk was also born of frustration. the <phrase>basic</phrase> idea for tk arose in response to apple's announcement of <phrase>hypercard</phrase> in the fall of 1987. <phrase>hypercard</phrase> generated tremendous excitement because of the power of the system and the way in which it allowed many different interactive elements to be scripted and work together. however, i was discouraged. the <phrase>hypercard</phrase> system had obviously taken a large development effort, and it seemed unlikely to me that a small group such as a <phrase>university</phrase> researchproject could ever <phrase>mount</phrase> such a massive effort. this suggested that we would not be able to participate in the development of new forms of interactive <phrase>software</phrase> in the future.i concluded that the only hope for us was a component approach. rather than building a new application as a self-contained <phrase>monolith</phrase> with hundreds of thousands of lines of code, we needed to find a way to divide applications into many smaller reusable components. ideally each component would be small enough to be implemented by a small group, and interesting applications could be created by assembling components. in this environment it should be possible to create an exciting new application by developing one new component and then combining it with existing components.the <phrase>component-based</phrase> approach requires a powerful and flexible "glue"for assembling the components, and it occurred to me that perhaps a shared <phrase>scripting language</phrase> could provide that glue. out of this thinking grew tk, an <phrase>x11</phrase> toolkit based on <phrase>tcl</phrase>. tk allows components to be either individual <phrase>user-interface</phrase> controls or entire applications; in either case components can be developed independently and <phrase>tcl</phrase> can be used to assemble the components and communicate between them.i started writing <phrase>tcl</phrase> and tk as a hobby in my spare time. as other people began to use the systems i found myself spending more and more time on them, to the point where today they occupy almost all of my waking hours and many of my sleeping ones.tcl and tk have succeeded beyond my wildest dreams. the <phrase>tcl</phrase>/tk developer <phrase>community</phrase> now numbers in the tens of thousands and there are thousands of <phrase>tcl</phrase> applications in existence or under development. the application areas for <phrase>tcl</phrase> and tk <phrase>cover</phrase> virtually the entire <phrase>spectrum</phrase> of <phrase>graphical</phrase> and <phrase>engineering</phrase> applications, including <phrase>computer-aided design</phrase>, <phrase>software development</phrase>, testing, instrument control, <phrase>scientific visualization</phrase>, and <phrase>multimedia</phrase>. <phrase>tcl</phrase> is used by itself in many applications, and <phrase>tcl</phrase> and tk are used together in many others. <phrase>tcl</phrase> and tk are being used by hundreds of companies, large and small, as well as <phrase>universities</phrase> and <phrase>research</phrase> laboratories.one benefit that came as a surprise to me is that it is possible to create interesting <phrase>graphical user interfaces</phrase> (guis) entirely as <phrase>tcl</phrase> scripts. i had always assumed that every <phrase>tcl</phrase> application would contain some new c code that implements new <phrase>tcl</phrase> commands, plus some <phrase>tcl</phrase> scripts that combine the new commands with the built-in facilities provided by <phrase>tcl</phrase>. however, once a simple <phrase>tcl</phrase>/tk application called wish became available, many people began creating <phrase>user interfaces</phrase> by writing <phrase>tcl</phrase> scripts for it, without writing any c code at all! it turned out that the <phrase>tcl</phrase> and tk commands provide a <phrase>high</phrase>-level interface to <phrase>gui</phrase> <phrase>programming</phrase> that hides many of the details faced by a c <phrase>programmer</phrase>. as a result, it is much easier to learn how to use wish than a c-based toolkit, and <phrase>user interfaces</phrase> can be written with much less code. most <phrase>tcl</phrase>/tk users never write any c code at all and most of the <phrase>tcl</phrase>/tk applications consist solely of <phrase>tcl</phrase> scripts.this <phrase>book</phrase> is intended as an introduction to <phrase>tcl</phrase> and tk for programmers who plan to write or modify <phrase>tcl</phrase>/tk applications. i assume that readers have programmed in c and have at least passing familiarity with a shell such as sh or csh or ksh. i also assume that readers have used the x window system and are familiar with <phrase>basic</phrase> ideas such as using the <phrase>mouse</phrase>, resizing <phrase>windows</phrase>, etc. no prior experience with <phrase>tcl</phrase> or tk is needed in <phrase>order</phrase> to read this <phrase>book</phrase>, and you need not have written x applications using other toolkits such as motif.the <phrase>book</phrase> is organized so that you can learn <phrase>tcl</phrase> without learning tk if you wish. also, the discussion of how to write <phrase>tcl</phrase> scripts is separate from the discussion of how to use the c <phrase>library</phrase> interfaces provided by <phrase>tcl</phrase> and tk. the first two parts of the <phrase>book</phrase> describe <phrase>tcl</phrase> and tk at the level of writing scripts, and the last two parts describe the c interfaces for <phrase>tcl</phrase> and tk; if you are like the majority of <phrase>tcl</phrase>/tk users who only write scripts, you can stop after <phrase>reading</phrase> the first two parts.in spite of my best efforts, i'm sure that there are errors in this edition of the <phrase>book</phrase>. i'm interested in hearing about any problems that you encounter, whether they are typos, formatting errors, sections or ideas that are hard to understand, or <phrase>bugs</phrase> in the examples. i'll attempt to correct the problems in future printings of the <phrase>book</phrase>. the best way to <phrase>report</phrase> problems is with <phrase>electronic</phrase> <phrase>mail</phrase> sent to tclbookbugs@cs.berkeley.edu.many people have helped in the creation of this <phrase>book</phrase>. first and foremost i would like to thank <phrase>brian kernighan</phrase>, who reviewed several drafts of the <phrase>manuscript</phrase> with almost terrifying thoroughness and uncovered numerous problems both large and small. i am also grateful for the detailed comments provided by the other <phrase>addison-wesley</phrase> technical reviewers: richard blevins, gerard holzmann, curt horkey, ron hutchins, stephen johnson, oliver jones, <phrase>david</phrase> <phrase>korn</phrase>, bill leggett, don libes, <phrase>kent</phrase> margraf, stuart mcrobert, <phrase>david</phrase> richardson, alexei <phrase>rodrigues</phrase>, gerald rosenberg, john slater, and win treese. thanks also to bob sproull, who read the next-to-last draft from <phrase>cover</phrase> to <phrase>cover</phrase> and provided countless bug fixes and suggestions.i made early drafts of the <phrase>manuscript</phrase> available to the <phrase>tcl</phrase>/tk <phrase>community</phrase> via the <phrase>internet</phrase> and received countless comments and suggestions from all over the world in return. i'm afraid that i didn't keep careful enough records to acknowledge all the people who contributed in this way, but the list of contributors includes at least the following people: <phrase>marvin</phrase> aguero, miriam amos nihart, jim anderson, frederik anheuser, jeff blaine, john boller, <phrase>david</phrase> boyce, terry brannon, richard campbell, j. cazander, wen chen, richard cheung, peter chubb, de clarke, peter collinson, peter costantinidis, alistair crooks, peter davies, tal dayan, akim demaille, mark diekhans, <phrase>matthew</phrase> dillon, tuan doan, tony duarte, paul dubois, anton eliens, marc r. ewing, luis fernandes, martin forssen, ben fried, matteo frigo, andrej gabara, steve gaede, sanjay ghemawat, <phrase>bob gibson</phrase>, michael <phrase>halle</phrase>, jun hamano, stephen hansen, brian harrison, marti <phrase>hearst</phrase>, fergus henderson, kevin <phrase>hendrix</phrase>, <phrase>david</phrase> herron, patrick hertel, carsten heyl, leszek holenderski, jamie honan, rob w.w. hooft, nick hounsome, christopher hylands, jonathan <phrase>jowett</phrase>, poul-henning kamp, karen l. karavanic, sunil <phrase>khatri</phrase>, <phrase>vivek</phrase> khera, jon <phrase>knight</phrase>, roger <phrase>knopf</phrase>, ramkumar krishnan, dave kristol, peter <phrase>labelle</phrase>, <phrase>tor</phrase>-<phrase>erik larsen</phrase>, tom legrady, will <phrase>e</phrase>. leland, kim lester, joshua levy, don libes, <phrase>oscar</phrase> linares, <phrase>david</phrase> c.p. linden, toumas j. lukka, steve <phrase>lord</phrase>, steve lumetta, earlin lutz, <phrase>david</phrase> j. mackenzie, b.g. mahesh, john maline, graham mark, stuart mcrobert, george <phrase>moon</phrase>, michael morris, russell nelson, dale k. newby, richard <phrase>newton</phrase>, peter nguyen, <phrase>david</phrase> nichols, marty olevitch, rita ousterhout, john pierce, stephen pietrowicz, anna pluzhnikov, <phrase>nico</phrase> poppelier, m.v.s. ramanath, cary d. renzema, mark roseman, samir tiongson saxena, jay schmidgall, dan m. serachitopol, <phrase>hume</phrase> smith, frank stajano, larry streepy, john <phrase>e</phrase>. stump, michael sullivan, holger teutsch, bennett <phrase>e</phrase>. todd, glenn trewitt, d.a. <phrase>vaughan</phrase>-<phrase>pope</phrase>, richard vieregge, larry w. <phrase>virden</phrase>, <phrase>david</phrase> waitzman, matt wartell, glenn waters, wally <phrase>wedel</phrase>, juergen weigert, mark weiser, brent welch, alex woo, su-lin wu, kawata yasuro, chut ngeow yee, richard yen, stephen ching-singyen, and mike young.many many people have made significant contributions to the development of <phrase>tcl</phrase> and tk. without all of their efforts there would have been nothing of interest to write about in this <phrase>book</phrase>. although i cannot hope to acknowledge all the people who helped to make <phrase>tcl</phrase> and tk what they are today, i would like to thank the following people specially: don libes, for writing the first widely used <phrase>tcl</phrase> application; mark diekhans and karl lehenbauer, for tclx; alastair fyfe, for supporting the early development of <phrase>tcl</phrase>; mary ann may-pumphrey, for developing the original <phrase>tcl</phrase> <phrase>test</phrase> suite; george howlett, michael mclennan, and sani nassif, for the <phrase>blt</phrase> extensions; kevin kenny, for showing that <phrase>tcl</phrase> can be used to communicate with almost any imaginable program; joel bartlett, for many challenging conversations and for inspiring tk's <phrase>canvas</phrase> widget with his ezd program; larry rowe, for developing <phrase>tcl</phrase>-dp and for providing <phrase>general</phrase> advice and support; sven delmas, for developing the <phrase>xf</phrase> application builder based on tk; and andrew payne, for the widget <phrase>tour</phrase> and for meritorious <phrase>tcl</phrase> evangelism.several companies have provided financial support for the development of <phrase>tcl</phrase> and tk, including <phrase>digital equipment corporation</phrase>, <phrase>hewlett-packard</phrase> <phrase>corporation</phrase>, <phrase>sun microsystems</phrase>, and computerized processes unlimited. i am particularly grateful to digital's <phrase>western</phrase> <phrase>research</phrase> <phrase>laboratory</phrase> and its <phrase>director</phrase>, richard <phrase>swan</phrase>, for providing me with a one-day-per-week hideaway where i could go to gather my thoughts and work on <phrase>tcl</phrase> and tk.terry lessard-smith and bob miller have provided fabulous administrative support for this and all my other projects. i don't know how i would get anything done without them.finally, i owe a special <phrase>debt</phrase> to my colleague and friend dave patterson, whose humor and sage advice have inspired and shaped much of my <phrase>professional</phrase> career, and to my wife rita and daughters kay and amy, who have tolerated my <phrase>workaholic</phrase> tendencies with more cheer and affection than i deserve. john ousterhout <phrase>berkeley</phrase>, <phrase>california</phrase> february, 1994
<phrase>functional programming</phrase> and parallel <phrase>graph</phrase> rewriting
web caching and replication
web caching and replication provides essential material based on the extensive <phrase>real-world</phrase> experience of two experts from <phrase>at & t labs</phrase>. this <phrase>comprehensive</phrase> examination of caching, replication, and load-balacing practices for the web brings together <phrase>information</phrase> from and for the commercial world, including real-<phrase>life</phrase> <phrase>products</phrase>; technical standards communities, such as <phrase>ietf</phrase> and <phrase>w3c</phrase>; and <phrase>academic</phrase> <phrase>research</phrase>.
mehrrechner-datenbanksysteme - grundlagen der verteilten und parallelen datenbankverarbeitung.
advanced <phrase>prolog</phrase>: techniques and examples
automatic text processing: the transformation, analysis, and retrieval of <phrase>information</phrase> by computer.
the <phrase>design</phrase> and analysis of spatial <phrase>data structures</phrase>
temporal <phrase>deductive</phrase> <phrase>databases</phrase>.
the historical <phrase>relational</phrase> <phrase>data model</phrase> (hrdm) revisited.
on the completeness of query languages for grouped and ungrouped historical <phrase>data</phrase> models.
a temporal <phrase>model</phrase> and <phrase>query language</phrase> for eer <phrase>databases</phrase>.
the time index and the monotonic <phrase>b+-tree</phrase>.
ben-zvi's pioneering work in <phrase>relational</phrase> temporal <phrase>databases</phrase>.
temporal <phrase>databases</phrase>: a prelude to parametric <phrase>data</phrase>.
object and <phrase>spreadsheet</phrase> histories.
differential <phrase>query processing</phrase> in transaction-time <phrase>databases</phrase>.
<phrase>indexing techniques</phrase> for historical <phrase>databases</phrase>.
<phrase>stream processing</phrase>: temporal <phrase>query processing</phrase> and optimization.
transaction-time <phrase>databases</phrase>.
the interval-extended <phrase>relational model</phrase> and its applications to valid-time <phrase>databases</phrase>.
temporal reasoning.
temporal extensions to the <phrase>relational model</phrase> and <phrase>sql</phrase>.
hsql: a historical <phrase>query language</phrase>.
join processing and optimization in temporal <phrase>relational databases</phrase>.
a temporal <phrase>data model</phrase> based on time sequences.
an overview of tquel.
a generalized <phrase>relational</phrase> framework for modeling temporal <phrase>data</phrase>.
applications of temporal <phrase>databases</phrase> to <phrase>knowledge</phrase>-based simulations.
integrating temporal <phrase>data</phrase> in a heterogeneous environment.
a uniform <phrase>model</phrase> for temporal and versioned <phrase>object-oriented</phrase> <phrase>databases</phrase>.
<phrase>relational database</phrase> theory
conceptual <phrase>database design</phrase>: an entity-relationship approach.
<phrase>object-oriented design</phrase> with applications
fundamentals of <phrase>database systems</phrase>
fundamentals of <phrase>database systems</phrase>, <phrase>2nd edition</phrase>.
:fundamentals of <phrase>database systems</phrase> combines clear explanations of theory and <phrase>design</phrase>, broad coverage of models and real systems, and excellent examples with up-to-date introductions to modern <phrase>database</phrase> technologies. this edition is completely revised and updated, and reflects the latest trends in technological and application development. professors elmasri and navathe focus on the <phrase>relational model</phrase> and include coverage of recent <phrase>object-oriented</phrase> developments. they also address advanced modeling and system enhancements in the areas of active <phrase>databases</phrase>, temporal and spatial <phrase>databases</phrase>, and <phrase>multimedia</phrase> <phrase>information</phrase> systems. this edition also surveys the latest application areas of <phrase>data warehousing</phrase>, <phrase>data mining</phrase>, web <phrase>databases</phrase>, <phrase>digital libraries</phrase>, <phrase>gis</phrase>, and <phrase>genome</phrase> <phrase>databases</phrase>. new to the third edition reorganized material on <phrase>data modeling</phrase> to clearly separate entity relationship modeling, extended entity relationship modeling, and <phrase>object-oriented</phrase> modeling expanded coverage of the <phrase>object-oriented</phrase> and <phrase>object/relational</phrase> approach to <phrase>data management</phrase>, including odmg and sql3 uses examples from real <phrase>database systems</phrase> including oracletm and <phrase>microsoft</phrase> accessae includes discussion of <phrase>decision support</phrase> applications of <phrase>data warehousing</phrase> and <phrase>data mining</phrase>, as well as emerging technologies of web <phrase>databases</phrase>, <phrase>multimedia</phrase>, and <phrase>mobile</phrase> <phrase>databases</phrase> covers advanced modeling in the areas of active, temporal, and spatial <phrase>databases</phrase> provides coverage of issues of physical <phrase>database</phrase> tuning discusses current <phrase>database</phrase> application areas of <phrase>gis</phrase>, <phrase>genome</phrase>, and <phrase>digital libraries</phrase>
crafting a <phrase>compiler</phrase> with c
introduction to <phrase>parallel computing</phrase>.
<phrase>computing</phrase> with <phrase>logic</phrase>: <phrase>logic programming</phrase> with <phrase>prolog</phrase>
temporal <phrase>databases</phrase>: theory, <phrase>design</phrase>, and implementation
automaten, sprachen und maschinen <phrase>fr</phrase> anwender
rechnerorganisation
rechnerorganisation, 2. auflage
computer-grafik
computer-grafik, 2. auflage
planen: einfhrung in die planerstellungsmethoden der knstlichen intelligenz
kryptologie
theoretische grundlagen relationaler datenbanksysteme
rechnernetzwerksystemarchitekturen und datenkommunikation
datenbankmaschinen: performanz durch parallelitt
parallele transaktionen in datenbanksystemen
syntaxanalyse, 3. auflage
c
algorithmen und datenstrukturen
algorithmen und datenstrukturen, 2. auflage
knstliche intelligenz: berblick und grundlagen; grundlegende konzepte und methoden zur realisierung von systemen der knstlichen intelligenz
geo-datenbanksysteme: eine speicher- und zugriffsarchitektur
logik <phrase>fr</phrase> informatiker
logik <phrase>fr</phrase> informatiker, 2. auflage
logik <phrase>fr</phrase> informatiker, 3. auflage
softwaretechnologie: <phrase>eine einfhrung</phrase>, 3. auflage
softwaretechnologie: <phrase>eine einfhrung</phrase>, 4. auflage
mikroprozessoren: vom bauteil zur anwendung, 2. auflage
objektorientierte schemaentwicklung: ein kategorialer <phrase>ansatz</phrase> <phrase>fr</phrase> datenbanken und programmierung
datenbanksysteme i
datenbanksysteme <phrase>ii</phrase>
datenbanksysteme i, 2. auflage
datenbanksysteme i, 3. auflage
compilerbau i: analyse
compilerbau <phrase>ii</phrase>: synthese und optimierung
betriebssysteme: parallele prozesse, 3. auflage
compilerbau i: analyse, 2. auflage
<phrase>information retrieval</phrase>.
building an optimizing <phrase>compiler</phrase>.
introduction to <phrase>knowledge base</phrase> systems
debuggers for <phrase>programming languages</phrase>.
<phrase>software</phrase> pipelining.
<phrase>compilation</phrase> for <phrase>distributed memory</phrase> architectures.
dynamic <phrase>compilation</phrase>.
<phrase>register allocation</phrase>.
instruction scheduling.
<phrase>data</phrase> flow testing.
profile-guided <phrase>compiler</phrase> optimizations.
<phrase>data</phrase> flow analysis.
optimizations for <phrase>object-oriented</phrase> languages.
program slicing.
automatic generation of code optimizers from formal specifications.
introduction to <phrase>operational semantics</phrase>.
<phrase>architecture</phrase> description languages for retargetable <phrase>compilation</phrase>.
retargetable very <phrase>long</phrase> instuction word <phrase>compiler</phrase> framework for <phrase>digital</phrase> signal processors.
dependence analysis and parallelizing transformations.
automatic <phrase>data</phrase> distribution.
instruction selection using <phrase>tree</phrase> <phrase>parsing</phrase>.
scalar <phrase>compiler</phrase> optimizations on the static <phrase>single</phrase> assignment form and the flow <phrase>graph</phrase>.
type systems in <phrase>programming languages</phrase>.
compiling safe <phrase>mobile</phrase> code.
shape analysis and applications.
a comparative <phrase>case study</phrase> of distributed network architectures for different automotive applications.
lonworks/eia-709 networks eia 709 protocol (lon <phrase>talk</phrase>).
distributed components in <phrase>microsoft</phrase> platforms - <phrase>technology</phrase> overview.
the quest for real-time behavior in <phrase>ethernet</phrase>.
<phrase>corba</phrase> in <phrase>manufacturing</phrase> - <phrase>technology</phrase> overview.
<phrase>ipv6</phrase>, ipsec, and vpns.
overview and classification of ip routing protocols - ip routing: interior and exterior routing protocols.
<phrase>java</phrase> <phrase>technology</phrase> and <phrase>industrial</phrase> applications.
internal <phrase>architecture</phrase> and features of real-time embedded operation systems.
<phrase>middleware</phrase>.
network on-chip <phrase>design</phrase> for gigascale systems-on-chip.
principles and features of profinet.
<phrase>knowledge</phrase> connect - an approach for an <phrase>industrial</phrase> it service tool.
intelligent space and <phrase>mobile</phrase> <phrase>robots</phrase>.
<phrase>software</phrase> for <phrase>wireless sensor networks</phrase>.
<phrase>web-based</phrase> enterprise <phrase>computing</phrase> development using j2ee.
holonic <phrase>manufacturing</phrase> systems: a technical overview.
the grafcet <phrase>specification language</phrase>.
the ida standard.
<phrase>embedded software</phrase> in the soc world. the concept of hds in view of the hw and sw <phrase>design</phrase> challenge.
platform-based and <phrase>derivative</phrase> <phrase>design</phrase>.
<phrase>internet</phrase> <phrase>programming languages</phrase>.
foundation <phrase>fieldbus</phrase>: <phrase>history</phrase> and features.
<phrase>mpls</phrase> - <phrase>multiprotocol label switching</phrase>.
operating principles and features of can networks.
<phrase>robot</phrase> tactile sensing.
hardware/<phrase>software</phrase> interfaces <phrase>design</phrase> for soc.
giving <phrase>robots</phrase> a sense of smell.
<phrase>mail</phrase> transfer and file transfer protocol.
the integrated services <phrase>architecture</phrase> and rsvp.
collaborative (<phrase>agent-based</phrase>) <phrase>factory</phrase> <phrase>automation</phrase>.
linking <phrase>factory floor</phrase> and the <phrase>internet</phrase>.
which network for which application.
<phrase>internet</phrase> firewalls.
interconnection of wireline and <phrase>wireless</phrase> fieldbusses.
<phrase>ieee 1394</phrase> for <phrase>factory</phrase> <phrase>automation</phrase>.
intelligent sensors: <phrase>analysis and design</phrase>.
securtity in <phrase>automation</phrase> networks.
integration technologies of field devices in distributed control and <phrase>engineering</phrase> systems.
the standard message specification for <phrase>industrial automation</phrase> systems -<phrase>iso</phrase> 9506 (<phrase>mms</phrase>).
<phrase>rtp</phrase>, rtcp, and rtsp - <phrase>internet</phrase> protocols for real-time <phrase>multimedia</phrase> <phrase>communication</phrase>.
smart power systems rely on standards for <phrase>information</phrase> models and <phrase>messaging</phrase> - <phrase>iec</phrase> 61850.
the fundamentals of <phrase>web services</phrase>.
open system <phrase>architecture</phrase> for controls within <phrase>automation</phrase> systems (osaca).
<phrase>programming</phrase> <phrase>web services</phrase> with .net and <phrase>java</phrase>.
a survey of congestion and <phrase>qos</phrase> control mechanisms for the <phrase>internet</phrase>.
languages for <phrase>embedded systems</phrase>.
switched <phrase>ethernet</phrase> in <phrase>automation</phrase> networking.
enterprise-<phrase>manufacturing</phrase> <phrase>data</phrase> exchange using <phrase>xml</phrase>.
<phrase>programming</phrase> with the <phrase>iec</phrase> 61131-3 languages and the matplc.
the <phrase>dynamic host configuration protocol</phrase>.
<phrase>security</phrase> in <phrase>embedded systems</phrase>.
the <phrase>internet protocol</phrase>.
profisafe - <phrase>safety</phrase> <phrase>technology</phrase> with <phrase>profibus</phrase>.
arp - <phrase>address resolution protocol</phrase>.
<phrase>xml</phrase> for the exchange of <phrase>automation</phrase> project <phrase>information</phrase>.
the worldfip <phrase>fieldbus</phrase>.
ip-mobility for cellular and <phrase>wireless networks</phrase>.
<phrase>unified modeling language</phrase>: the <phrase>industry</phrase> standard for <phrase>software development</phrase>.
a survey on self-organizing <phrase>wireless sensor networks</phrase>.
web servers, clients, and <phrase>browsers</phrase>.
the <phrase>hypertext</phrase> transfer protocol and <phrase>uniform resource identifier</phrase>.
introduction to multisensor <phrase>data fusion</phrase>.
models of computation for <phrase>embedded systems</phrase>.
<phrase>industrial</phrase> it-based <phrase>network management</phrase>.
hardware-<phrase>level design</phrase> languages.
integration between <phrase>production</phrase> and <phrase>business</phrase> systems.
from holonic control to virtual enterprises: the <phrase>multi-agent</phrase> approach.
acheiving reconfigurability of <phrase>automation</phrase> systems by using the new international standard <phrase>iec</phrase> 61499: a developer's view.
ip-<phrase>qos</phrase>: scalable and flexible quality-of-service with differentiated services.
real-time systems.
<phrase>wireless lan</phrase> <phrase>technology</phrase> for the <phrase>factory floor</phrase>.
<phrase>internet</phrase> still image and <phrase>video</phrase> formats.
principles of <phrase>lower</phrase>-layer protocols for <phrase>data</phrase> communications.
<phrase>network management</phrase>: <phrase>basic</phrase> notions and frameworks.
<phrase>multicast</phrase>.
http digest <phrase>authentication</phrase> - theory and practice.
<phrase>web services</phrase> for integrated <phrase>automation</phrase> systems - challenges, solutions and future.
verification languages.
<phrase>mobile</phrase> ip routing.
remote monitoring and control over the <phrase>internet</phrase>.
power aware embedded <phrase>computing</phrase>.
<phrase>robot</phrase> vision.
<phrase>bluetooth</phrase>.
<phrase>profibus</phrase> - open solutions for the world of <phrase>automation</phrase>.
the fundamentals of the quality of service.
<phrase>ad hoc</phrase> networks.
applications of haptics in <phrase>design</phrase> and <phrase>manufacturing</phrase>.
implementation af a virtual <phrase>factory</phrase> <phrase>communication</phrase> system using the <phrase>manufacturing</phrase> message specification standard.
<phrase>ultrasonic</phrase> sensors in <phrase>robotics</phrase>.
microsoft's .net.
multiagent-based <phrase>architecture</phrase> for <phrase>plant</phrase> <phrase>automation</phrase>.
introduction to <phrase>e</phrase>-<phrase>manufacturing</phrase>.
time-triggered <phrase>communication</phrase> networks.
<phrase>internet</phrase>-based telemanipulation.
network <phrase>security</phrase> and secure applications.
<phrase>simple network management protocol</phrase> <phrase>snmp</phrase>.
<phrase>tcp/ip</phrase> <phrase>architecture</phrase>, protocols, and services.
opc - openness, <phrase>productivity</phrase>, and connectivity.
<phrase>design</phrase> of <phrase>embedded systems</phrase>.
a smart <phrase>transducer</phrase> interface standard for sensors and actuators.
lonworkstm over ip.
the <phrase>transmission control protocol</phrase>.
<phrase>multidimensional databases</phrase>.
the <phrase>user datagram protocol</phrase>.
networked control systems overview.
system-on-chip and network-on-chip <phrase>design</phrase>.
<phrase>wireless</phrase> local <phrase>area</phrase> networks and <phrase>wireless</phrase> personal <phrase>area</phrase> networks (wlans and wpans).
open controller enabled by an advanced real-time network (<phrase>ocean</phrase>).
it <phrase>security</phrase> for <phrase>automation</phrase> systems.
the jevis service platform - distributed <phrase>energy</phrase> <phrase>data acquisition</phrase> and <phrase>management</phrase>.
an introductory survey of networked <phrase>embedded systems</phrase>.
thread <phrase>management</phrase> for <phrase>shared-memory</phrase> multiprocessors.
parallelizing compilers.
renderman: an interface for image synthesis.
<phrase>object-oriented</phrase> <phrase>databases</phrase>.
network and <phrase>internet</phrase> <phrase>security</phrase>.
concurrent/distributed <phrase>programming paradigm</phrase>.
parallel <phrase>algorithms</phrase>.
formal models and the specification process.
<phrase>user interface design</phrase> activities.
<phrase>virtual reality</phrase>.
<phrase>memory</phrase> systems.
type systems.
<phrase>computational fluid dynamics</phrase>.
rules in <phrase>data</phrase>-based systems.
<phrase>combinatorial optimization</phrase>.
distributed and <phrase>multiprocessor</phrase> scheduling.
computational reacting flow.
<phrase>logic programming</phrase> and constraint <phrase>logic programming</phrase>.
<phrase>pattern matching</phrase> and text compression <phrase>algorithms</phrase>.
process and device scheduling.
explanation-based learning.
planning and scheduling.
traditional <phrase>software design</phrase>.
<phrase>virtual memory</phrase>.
<phrase>virtual memory</phrase> is the <phrase>simulation</phrase> of a storage space so large that programmers do not need to reprogram or recompile their works when the capacity of a local <phrase>memory</phrase> or the configuration of a network changes. the name, borrowed from <phrase>optics</phrase>, recalls the virtual images formed by mirrors and lenses--images that are not there but behave as if they are. the designers of the atlas computer at the <phrase>university</phrase> of <phrase>manchester</phrase> invented <phrase>virtual memory</phrase> in the 1950s to eliminate a looming <phrase>programming</phrase> problem: planning and scheduling <phrase>data</phrase> transfers between main and <phrase>secondary</phrase> <phrase>memory</phrase> and recompiling programs for each change of size of main <phrase>memory</phrase>. <phrase>virtual memory</phrase> is even more useful in the <phrase>computers</phrase> of the 1990s, which have more things to hide-on-chip caches, separate <phrase>ram</phrase> chips, local <phrase>disk storage</phrase>, network file servers (q.v.), large numbers of separately compiled program modules, other <phrase>computers</phrase> on the local <phrase>bus</phrase> or local network, or the <phrase>internet</phrase>. the story of <phrase>virtual memory</phrase> from then to now is a story about machines helping programmers solve problems in storage allocation, protection of <phrase>information</phrase>, sharing and reuse of objects, and linking of program components. <phrase>virtual memory</phrase>, common in all <phrase>computers</phrase> and <phrase>operating systems</phrase> from the smallest <phrase>microprocessor</phrase> to the largest <phrase>supercomputer</phrase>, is now invading the <phrase>internet</phrase>.
<phrase>algebraic</phrase> <phrase>algorithms</phrase>.
distributed <phrase>file systems</phrase> and <phrase>distributed memory</phrase>.
real-time and <phrase>embedded systems</phrase>.
output devices and techniques.
interactive techniques.
advanced geometric modeling.
<phrase>digital</phrase> <phrase>logic</phrase>.
<phrase>malicious software</phrase> and hacking.
what is an operating system?
<phrase>data structures</phrase>.
mainstream rendering techniques.
qualitative reasoning.
<phrase>genetic algorithms</phrase>.
<phrase>concurrency control</phrase> and recovery.
verification and validation.
<phrase>software</phrase> qualities and principles.
<phrase>functional programming languages</phrase>.
the organizational context of development and use.
the <phrase>object-oriented language</phrase> <phrase>paradigm</phrase>.
overview of <phrase>three-dimensional</phrase> graphics.
international <phrase>user-interface</phrase> standardization.
busses.
parallel architectures.
interactive <phrase>software</phrase> <phrase>technology</phrase>.
computer vision.
<phrase>query optimization</phrase>.
input devices and techniques.
<phrase>database</phrase> <phrase>security</phrase> and <phrase>privacy</phrase>.
formal models and <phrase>computability</phrase>.
the imperative <phrase>language</phrase> <phrase>paradigm</phrase>.
<phrase>ethical</phrase> issues for computer scientists and engineers.
<phrase>neural networks</phrase>.
we present an overview of current <phrase>research</phrase> on <phrase>artificial neural networks</phrase>, emphasizing a statistical perspective. we view <phrase>neural networks</phrase> as parameterized <phrase>graphs</phrase> that make probabilistic assumptions about <phrase>data</phrase>, and view learning <phrase>algorithms</phrase> as methods for finding parameter values that look probable in the <phrase>light</phrase> of the <phrase>data</phrase>. we discuss <phrase>basic</phrase> issues in representation and learning, and treat some of the practical issues that arise in fitting networks to <phrase>data</phrase>. we also discuss links between <phrase>neural networks</phrase> and the <phrase>general</phrase> formalism of <phrase>graphical</phrase> models.
<phrase>digital</phrase> computer <phrase>architecture</phrase>.
computational <phrase>ocean</phrase> modeling.
volume visualization.
<phrase>graph</phrase> and network <phrase>algorithms</phrase>.
<phrase>task analysis</phrase> and the <phrase>design</phrase> of functionality.
<phrase>computational biology</phrase>.
search.
<phrase>case study</phrase> in <phrase>algorithms</phrase>: <phrase>vlsi</phrase> layout.
protection (<phrase>security</phrase>) models and policy.
<phrase>computational geometry</phrase>.
<phrase>robotics</phrase>.
compilers and interpreters.
<phrase>complexity theory</phrase>.
<phrase>logic</phrase>-based <phrase>deductive reasoning</phrase>.
computer <phrase>animation</phrase>.
<phrase>secondary</phrase> storage and <phrase>file systems</phrase>.
the <phrase>sql</phrase> <phrase>language</phrase>: a <phrase>case study</phrase>.
<phrase>randomized</phrase> <phrase>algorithms</phrase>.
overview of distributed <phrase>operating systems</phrase>.
interface <phrase>software</phrase> <phrase>technology</phrase>.
<phrase>security</phrase> and <phrase>privacy</phrase> issues in computer and <phrase>communication</phrase> systems.
computational <phrase>structural mechanics</phrase>.
distributed and parallel <phrase>database systems</phrase>.
routing protocols.
development strategies and <phrase>project management</phrase>.
<phrase>internetworking</phrase>.
<phrase>software</phrase> tools and environments.
geometric primitives.
access methods.
testing: principles and practice.
online <phrase>support systems</phrase>: tutorials, documentation, and help.
<phrase>computational electromagnetics</phrase>.
<phrase>scientific visualization</phrase>.
<phrase>data</phrase> models.
network <phrase>organization</phrase> and <phrase>topologies</phrase>.
computer <phrase>science</phrase> and <phrase>engineering</phrase>: the discipline and its impact.
process synchronization and interprocess communications.
<phrase>usability</phrase> <phrase>engineering</phrase>.
:written by the <phrase>author</phrase> of the best-selling <phrase>hypertext</phrase> & <phrase>hypermedia</phrase>, this <phrase>book</phrase> provides an excellent guide to the methods of <phrase>usability</phrase> <phrase>engineering</phrase>. special features: emphasizes cost-effective methods that will help developers improve their <phrase>user interfaces</phrase> immediately, shows you how to avoid the four most frequently <phrase>listed</phrase> reasons for delay in <phrase>software</phrase> projects, provides step-by-step <phrase>information</phrase> about which methods to use at various stages during the development <phrase>life</phrase> cycle, and offers <phrase>information</phrase> on the unique issues relating to informational <phrase>usability</phrase>. you do not need to have previous <phrase>knowledge</phrase> of <phrase>usability</phrase> to implement the methods provided, yet all of the latest <phrase>research</phrase> is <phrase>covered</phrase>.
<phrase>knowledge-based systems</phrase> for <phrase>natural language processing</phrase>.
run time environments and <phrase>memory management</phrase>.
<phrase>database</phrase> <phrase>performance measurement</phrase>.
<phrase>graphical</phrase> models for probabilistic and causal reasoning.
foundational calculi for <phrase>programming languages</phrase>.
decision <phrase>trees</phrase> and instance-based classifiers.
<phrase>basic</phrase> techniques for <phrase>design</phrase> and analysis of <phrase>algorithms</phrase>.
text <phrase>databases</phrase> and <phrase>information retrieval</phrase>.
the <phrase>human</phrase> factor in <phrase>programming</phrase> and <phrase>software development</phrase>.
<phrase>authentication</phrase>, access controls, and <phrase>intrusion detection</phrase>.
<phrase>programming language</phrase> <phrase>semantics</phrase>.
like <phrase>english</phrase>, <phrase>french</phrase>, and other "natural" languages, a <phrase>programming language</phrase> possesses both a <phrase>syntax</phrase> (grammatical laws that define the well-formed sentences) and a <phrase>semantics</phrase> (rules for giving meaning to <phrase>programming language</phrase> is a simple enough "<phrase>artificial</phrase>" <phrase>language</phrase> that precise definitions can be formulated for its <phrase>syntax</phrase> and <phrase>semantics</phrase>. the benefits of such precise definitions are: (1) the definitions standardize the <phrase>programming language</phrase>, so that implementors and users can agree on how the <phrase>language</phrase> bahaves; (2) the definitions can be analyzed for correctness and effciency properties; and (3) they can be used as input to automated prototyping tools like complier generators.
understanding spoken <phrase>language</phrase>.
tuning <phrase>database design</phrase> for <phrase>high</phrase> performance.
<phrase>software</phrase> support challenges for heterogeneous <phrase>computing</phrase>.
<phrase>software</phrase> process models.
<phrase>geometry</phrase>-grid generation.
<phrase>high</phrase>-speed computer <phrase>arithmetic</phrase>.
sampling, <phrase>reconstruction</phrase>, and antialiasing.
<phrase>multimedia</phrase>.
the <phrase>industrial</phrase> <phrase>information technology</phrase> handbook
<phrase>design</phrase> of compilers - techniques of <phrase>programming language</phrase> <phrase>translation</phrase>
handbook of applied <phrase>cryptography</phrase>
the computer <phrase>science</phrase> and <phrase>engineering</phrase> handbook
the <phrase>compiler</phrase> <phrase>design</phrase> handbook: optimizations and <phrase>machine code</phrase> generation
fundamentals of computer <phrase>algorithms</phrase>.
the theory of <phrase>relational databases</phrase>.
a logical <phrase>language</phrase> for <phrase>data</phrase> and <phrase>knowledge</phrase> bases.
the theory of <phrase>database</phrase> <phrase>concurrency control</phrase>
introduction to <phrase>compiler</phrase> <phrase>construction</phrase>
<phrase>data compression</phrase>: methods and theory.
principles of <phrase>database systems</phrase>, 1st edition
principles of <phrase>database systems</phrase>, <phrase>2nd edition</phrase>
principles of <phrase>database</phrase> and <phrase>knowledge-base</phrase> systems, volume i
principles of <phrase>database</phrase> and <phrase>knowledge-base</phrase> systems, volume <phrase>ii</phrase>
algorithmic studies in <phrase>mass storage</phrase> systems.
an introduction to unification-based approaches to <phrase>grammar</phrase>
<phrase>word order</phrase> and constituent structure in <phrase>german</phrase>
nonmonotonic reasoning: an overview
<phrase>specification language</phrase>.
an introduction to pascal-plus.
<phrase>algorithms</phrase> for parallel <phrase>computers</phrase>.
concurrent pascal - an appraisal.
a <phrase>model</phrase> for <phrase>communicating sequential processes</phrase>.
modules and visibility in the ada <phrase>programming language</phrase>.
<phrase>information</phrase> systems: modelling, sequencing and transformations.
specification and <phrase>design</phrase> of an <phrase>information</phrase> system conventionally starts from consideration of the system <phrase>function</phrase>. this <phrase>paper</phrase> argues that consideration may more properly be given first to the system as a <phrase>model</phrase> of the <phrase>reality</phrase> with which it is concerned, the <phrase>function</phrase> being subsequently superimposed on the <phrase>model</phrase>. the form of <phrase>model</phrase> proposed is a network of sequential processes communicating by serial <phrase>data</phrase> streams. such a <phrase>model</phrase> permits a clear representation of change or activity over time, and it also prevents over-specification of sequencing by separating problem-oriented from <phrase>solution</phrase>-oriented sequencing constraints. the <phrase>model</phrase>, however, cannot be efficiently executed on uniprocessor hardware without transformation. some relevant kinds of transformation are mentioned, and the derivation, by means of them, of conventional <phrase>information</phrase> system configurations from the proposed <phrase>model</phrase>.
a structured operating system.
languages for parallel <phrase>computers</phrase>.
<phrase>parallel processing</phrase> in ada.
a structured <phrase>compiler</phrase>.
compiling with continuations
this <phrase>book</phrase> shows how <phrase>continuation-passing style</phrase> is used as an intermediate representation to perform optimizations and program transformations. continuations can be used to compile most <phrase>programming languages</phrase>. the method is illustrated in a <phrase>compiler</phrase> for the <phrase>programming language</phrase> <phrase>standard ml</phrase>. prior <phrase>knowledge</phrase> of ml, however, is not necessary, as the <phrase>author</phrase> carefully explains each concept as it arises. this is the first <phrase>book</phrase> to show how concepts from the theory of <phrase>programming languages</phrase> can be applied to the <phrase>production</phrase> of practical optimizing compilers for modern languages like ml. all the details of compiling are <phrase>covered</phrase>, including the interface to a runtime system and garbage collector.
<phrase>modern compiler</phrase> implementation in <phrase>java</phrase>: <phrase>basic</phrase> techniques
:this <phrase>textbook</phrase> describes all phases of a <phrase>modern compiler</phrase>. it includes good coverage of current techniques in code generation and <phrase>register allocation</phrase>, as well as functional and <phrase>object-oriented</phrase> languages, that is missing from most books. in a concise way, the <phrase>author</phrase> describes the most accepted and successful techniques, rather than giving an exhaustive catalog of every possible variant. detailed descriptions of the interfaces between modules of a <phrase>compiler</phrase> are illustrated with actual <phrase>java</phrase> classes. a unique feature of the <phrase>book</phrase> is a well designed <phrase>compiler</phrase> implementation project in <phrase>java</phrase>, including front-end and <phrase>high</phrase>-tech back-end phases, giving a practical example of <phrase>java</phrase> <phrase>programming</phrase> for students which will also interest computer professionals.
<phrase>modern compiler</phrase> implementation in c: <phrase>basic</phrase> techniques
:this <phrase>textbook</phrase> describes all phases of a <phrase>modern compiler</phrase>. it includes good coverage of current techniques in code generation and <phrase>register allocation</phrase>, as well as functional and <phrase>object-oriented</phrase> languages, that is missing from most books. in a concise way, the <phrase>author</phrase> describes the most accepted and successful techniques, rather than giving an exhaustive catalog of every possible variant. detailed descriptions of the interfaces between modules of a <phrase>compiler</phrase> are illustrated with actual c header files. a unique feature of the <phrase>book</phrase> is a well designed <phrase>compiler</phrase> implementation project in c, including front-end and <phrase>high</phrase>-tech back-end phases, useful for <phrase>undergraduate</phrase> and graduate students as well as computer professionals needing a reference on <phrase>compiler</phrase> implementation.
<phrase>modern compiler</phrase> implementation in ml: <phrase>basic</phrase> techniques
:this <phrase>textbook</phrase> describes all phases of a <phrase>modern compiler</phrase>. it includes good coverage of current techniques in code generation and <phrase>register allocation</phrase>, as well as functional and <phrase>object-oriented</phrase> languages, that is missing from most books. in a concise way, the <phrase>author</phrase> describes the most accepted and successful techniques, rather than giving an exhaustive catalog of every possible variant. detailed descriptions of the interfaces between modules of a <phrase>compiler</phrase> are illustrated with actual ml signatures. a unique feature of the <phrase>book</phrase> is a well designed <phrase>compiler</phrase> implementation project in ml, including front-end and <phrase>high</phrase>-tech back-end phases, useful for <phrase>undergraduate</phrase> and graduate students as well as computer professionals needing a reference on <phrase>compiler</phrase> implementation.
<phrase>modern compiler</phrase> implementation in <phrase>java</phrase>
<phrase>modern compiler</phrase> implementation in c
<phrase>modern compiler</phrase> implementation in ml
<phrase>modern compiler</phrase> implementation in <phrase>java</phrase>, <phrase>2nd edition</phrase>.
an introduction to <phrase>mathematical</phrase> <phrase>taxonomy</phrase>
biological <phrase>sequence analysis</phrase>: probabilistic models of <phrase>proteins</phrase> and <phrase>nucleic acids</phrase>
<phrase>algorithms</phrase> on <phrase>strings</phrase>, <phrase>trees</phrase>, and sequences - computer <phrase>science</phrase> and <phrase>computational biology</phrase>
introduction to combinators and <phrase>lambda-calculus</phrase>.
on the <phrase>construction</phrase> of programs
leda: a platform for <phrase>combinatorial</phrase> and geometric <phrase>computing</phrase>.
<phrase>combinatorial</phrase> and geometric <phrase>computing</phrase> is a core <phrase>area</phrase> of computer <phrase>science</phrase> (cs). in fact, most cs curricula contain a course in <phrase>data structures</phrase> and <phrase>algorithms</phrase>. the <phrase>area</phrase> deals with objects such as <phrase>graphs</phrase>, sequences, dictionaries, <phrase>trees</phrase>, shortest paths, flows, matchings, points, segments, lines, convex hulls, and voronoi diagrams and forms the basis for application areas such as discrete optimization, scheduling, traffic control, <phrase>cad</phrase>, and graphics. there is no <phrase>standard library</phrase> of the <phrase>data structures</phrase> and <phrase>algorithms</phrase> of <phrase>combinatorial</phrase> and geometric <phrase>computing</phrase>. this is in sharp contrast to many other areas of <phrase>computing</phrase>. there are, for example, packages in <phrase>statistics</phrase> (<phrase>spss</phrase>), <phrase>numerical analysis</phrase> (<phrase>linpack</phrase>, eispack), <phrase>symbolic computation</phrase> (<phrase>maple</phrase>, <phrase>mathematica</phrase>), and <phrase>linear programming</phrase> (<phrase>cplex</phrase>).
<phrase>randomized</phrase> <phrase>algorithms</phrase>.
numerical recipes in c, <phrase>2nd edition</phrase>.
<phrase>data</phrase> refinement: <phrase>model</phrase>-oriented proof theories and their comparison
concurrency verification: introduction to compositional and noncompositional methods
<phrase>wiki</phrase> - planen, einrichten, verwalten
technologiebewusstes <phrase>design</phrase> <phrase>von web</phrase>-anwendungen.
semantisches web - das netz der bedeutungen im netz der dokumente.
sicherheit <phrase>von web</phrase>-anwendungen.
betrieb und wartung <phrase>von web</phrase>-anwendungen.
architektur <phrase>von web</phrase>-anwendungen.
entwicklungsprozess <phrase>von web</phrase>-anwendungen.
implementierungstechnologien <phrase>fr</phrase> <phrase>web-anwendungen</phrase>.
<phrase>requirements engineering</phrase> <phrase>fr</phrase> <phrase>web-anwendungen</phrase>.
<phrase>usability</phrase> <phrase>von web</phrase>-anwendungen.
web <phrase>engineering</phrase> - die disziplin zur systematischen entwicklung <phrase>von web</phrase>-anwendungen.
performanz <phrase>von web</phrase>-anwendungen.
web-projektmanagement.
modellierung <phrase>von web</phrase>-anwendungen.
testen <phrase>von web</phrase>-anwendungen.
semistrukturierte datenmodelle und <phrase>xml</phrase>.
<phrase>benchmarking</phrase> von <phrase>xml</phrase>-datenbanksystemen.
indexstrukturen <phrase>fr</phrase> <phrase>xml</phrase>.
web-basiertes lernen: eine bersicht ber stand und entwicklungen.
architektur <phrase>von web</phrase>-informationssystemen.
<phrase>web services</phrase>.
speicherung von <phrase>xml</phrase>-dokumenten.
anfragen, ndern und transformieren von <phrase>xml</phrase>.
kommerzielle systeme zur speicherung, verwaltung und anfrage von <phrase>xml</phrase>-dokumenten.
<phrase>data-warehouse</phrase>-einsatz zur web-zugriffsanalyse.
datenintegration und mediatoren.
<phrase>xml schema</phrase>.
suchmaschinen.
web caching.
<phrase>object-oriented</phrase> reengineering patterns
:the documentation is missing or obsolete, and the original developers have departed. your team has limited understanding of the system, and unit <phrase>tests</phrase> are missing for many, if not all, of the components. when you fix a bug in one place, another bug pops up somewhere else in the system. <phrase>long</phrase> rebuild times make any change difficult. all of these are signs of <phrase>software</phrase> that is close to the breaking point. many systems can be upgraded or simply thrown away if they no longer serve their purpose. legacy <phrase>software</phrase>, however, is crucial for operations and needs to be continually available and upgraded. how can you reduce the complexity of a legacy system sufficiently so that it can continue to be used and adapted at acceptable cost? based on the authors' <phrase>industrial</phrase> experiences, this <phrase>book</phrase> is a guide on how to <phrase>reverse engineer</phrase> legacy systems to understand their problems, and then reengineer those systems to meet new demands. patterns are used to clarify and explain the process of understanding large code bases, hence transforming them to meet new requirements. the key insight is that the right <phrase>design</phrase> and <phrase>organization</phrase> of your system is not something that can be evident from the initial requirements alone, but rather as a consequence of understanding how these requirements evolve.
objektorientierte datenbanksysteme. ein praktikum.
digitale bibliotheken - informatik-lsungen <phrase>fr</phrase> globale wissensmrkte
objektrelationale und objektorientierte datenbankkonzepts und -systeme
<phrase>mobile</phrase> datenbanken und informationssysteme: konzepte und techniken
web <phrase>engineering</phrase>: systematische entwicklung <phrase>von web</phrase>-anwendungen
<phrase>linux</phrase> sicherheit: <phrase>security</phrase> <phrase>mit</phrase> <phrase>open-source-software</phrase>, grundlagen und praxis
informationsintegration: architekturen und methoden zur integration verteilter und heterogener datenquellen.
systemsoftware: grundlagen moderner betriebssysteme
web & datenbanken. konzepte, architekturen, anwendungen
datenbanken & <phrase>java</phrase> - <phrase>jdbc</phrase>, sqlj und odmg
<phrase>html</phrase> & <phrase>xhtml</phrase> - die sprachen des web, lehrgang und referenz, 5. auflage
<phrase>sql</phrase>:1999 & <phrase>sql</phrase>:2003 - objektrelationales <phrase>sql</phrase>, sqlj & <phrase>sql</phrase>/<phrase>xml</phrase>
wissensbasierte textverarbeitung: schriftsatz und typographie - mglichkeiten einer intelligenteren textverarbeitung
anfrageverarbeitung in komplexobjekt-datenbanksystemen
a formal <phrase>model</phrase> for lazy implementations of a <phrase>prolog</phrase>-compatible <phrase>functional language</phrase>.
<phrase>garbage collection</phrase> in <phrase>prolog</phrase> interpreters.
deduction revision by intelligent <phrase>backtracking</phrase>.
should <phrase>prolog</phrase> be list or record oriented.
finding backtrack points for intelligent <phrase>backtracking</phrase>.
poler - implementation of a <phrase>pop</phrase>-2-based planner.
a note on micro-planer.
an interpreting <phrase>algorithm</phrase> for <phrase>prolog</phrase> programs.
what the naive user wants from <phrase>prolog</phrase>.
a <phrase>prolog</phrase> interpreter working with infinite terms.
<phrase>exeter</phrase> <phrase>prolog</phrase> - some thoughts on <phrase>prolog</phrase> <phrase>design</phrase> by a <phrase>lisp</phrase> user.
the control of searching and <phrase>backtracking</phrase> in string <phrase>pattern matching</phrase>.
efficient implementation of unification of cyclic structures.
how to implement <phrase>prolog</phrase> on a <phrase>lisp machine</phrase>.
the '<phrase>marseille</phrase> interpreter' - a personal perspective.
<phrase>prolog</phrase> - a <phrase>panacea</phrase>?
integrating <phrase>prolog</phrase> in the poplog environment.
although <phrase>prolog</phrase> undoubtedly has its good points, there are some tasks (such as writing a screen <phrase>editor</phrase> or <phrase>network interface controller</phrase>) for which it is not the <phrase>language</phrase> of choice. the most natural computational concepts [2] for these tasks are hard to reconcile with prolog's declarative <phrase>nature</phrase>. just as there is a need for even the most committed <phrase>prolog</phrase> <phrase>programmer</phrase> to use "conventional" languages for some tasks, so too is there a need for "<phrase>logic</phrase>" oriented components in conventional applications programs, such as <phrase>cad</phrase> systems [7] and <phrase>relational databases</phrase> [5]. at <phrase>sussex</phrase>, the problems of integrating <phrase>logic</phrase> with <phrase>procedural programming</phrase> are being addressed by two projects. one of these [4] involves a distributed ring of processors communicating by <phrase>message passing</phrase>. the other project is the poplog system, a <phrase>mixed language</phrase> <phrase>ai</phrase> <phrase>programming</phrase> environment which <phrase>runs</phrase> on conventional hardware. this <phrase>paper</phrase> describes the way in which we have integrated <phrase>prolog</phrase> into poplog.
a proposal for distributed <phrase>programming</phrase> in <phrase>logic</phrase>.
associative evaluation of <phrase>prolog</phrase> programs.
the world's shortest <phrase>prolog</phrase> interpreter?
formal <phrase>vienna</phrase>-definition-method models of <phrase>prolog</phrase>.
<phrase>logic</phrase> control with <phrase>logic</phrase>.
epilog: a <phrase>language</phrase> for extended <phrase>programming</phrase> in <phrase>logic</phrase>.
w-grammars for <phrase>logic programming</phrase>.
the taming of the sleuth.
epilog: re-interpreting and extending <phrase>prolog</phrase> for a <phrase>multiprocessor</phrase> environment.
system <phrase>simulation</phrase> and <phrase>cooperative</phrase> <phrase>problem-solving</phrase> on a <phrase>prolog</phrase> basis.
on <phrase>prolog</phrase> <phrase>dbms</phrase> connections: a step <phrase>forward</phrase> from educe.
grid files for efficient <phrase>prolog</phrase> clause access.
a source-to-source meta-<phrase>translation</phrase> system for <phrase>database</phrase> query languages - implementation in <phrase>prolog</phrase>.
a flexible <phrase>prolog</phrase>-based lexical <phrase>database</phrase> system.
a generalized interface between <phrase>prolog</phrase> and <phrase>relational databases</phrase>.
a <phrase>prolog</phrase>-<phrase>relational database</phrase> interface.
treql (thornton <phrase>research</phrase> easy <phrase>query language</phrase>): an intelligent front-end to a <phrase>relational database</phrase>.
modular commitment in persistent <phrase>prolog</phrase>.
implementing query languages in <phrase>prolog</phrase>.
on <phrase>transaction processing</phrase>, <phrase>knowledge-based systems</phrase> and <phrase>databases</phrase>.
<phrase>an object-oriented database</phrase> for storage and analysis of <phrase>protein structure</phrase> <phrase>data</phrase>.
the nu-<phrase>prolog</phrase> <phrase>deductive</phrase> <phrase>database</phrase> system.
<phrase>software configuration management</phrase> using <phrase>prolog</phrase>.
guarded default <phrase>databases</phrase>: a <phrase>prototype</phrase> implementation.
an interface from <phrase>prolog</phrase> to a <phrase>binary</phrase> <phrase>relational database</phrase>.
a <phrase>data</phrase>-driven execution mechanism for transaction-oriented <phrase>information</phrase> systems.
the interaction between bim-<phrase>prolog</phrase> and <phrase>relational databases</phrase>.
recursive <phrase>query processing</phrase>: fundamental <phrase>algorithms</phrase> and the dedgin system.
the trace club expert system and <phrase>databases</phrase>.
<phrase>benchmarking</phrase> <phrase>prolog</phrase> for <phrase>database</phrase> applications.
<phrase>abstract interpretation</phrase> of declarative languages
statistical and scientific <phrase>databases</phrase>
<phrase>machine learning</phrase>, neural and <phrase>statistical classification</phrase>
pascal implementation: the p4 <phrase>compiler</phrase> and interpreter
<phrase>classical</phrase> <phrase>type theory</phrase>.
unification theory.
normal form transformations.
automated deduction for many-valued logics.
resolution <phrase>theorem proving</phrase>.
proof-assistants using dependent type systems.
solving numerical constraints.
the <phrase>automation</phrase> of proof by <phrase>mathematical induction</phrase>.
reasoning in expressive description logics.
<phrase>automated reasoning</phrase> in <phrase>geometry</phrase>.
<phrase>model checking</phrase>.
inductionless induction.
the early <phrase>history</phrase> of automated deduction.
the inverse method.
equality reasoning in <phrase>sequent</phrase>-based calculi.
rewriting.
nonmonotonic reasoning: towards efficient calculi and implementations.
higher-<phrase>order</phrase> unification and matching.
resolution decision procedures.
tableaux and related methods.
<phrase>model</phrase> elimination and connection tableau procedures.
paramodulation-based <phrase>theorem proving</phrase>.
<phrase>computing</phrase> small clause normal forms.
encoding two-valued nonclassical logics in <phrase>classical logic</phrase>.
logical frameworks.
term indexing.
preface.
connections in nonclassical logics.
combining <phrase>superposition</phrase>, sorts and splitting.
computer-aided <phrase>database design</phrase>: the dataid approach.
a <phrase>software engineering</phrase> approach to <phrase>database design</phrase>: the <phrase>galileo</phrase> project.
a tool for modeling dynamics in conceptual <phrase>design</phrase>.
gincod: a <phrase>graphical</phrase> tool for conceptual <phrase>design</phrase> of <phrase>data</phrase> base applications.
the logical <phrase>design</phrase> in the dataid project: the easymap system.
a <phrase>workbench</phrase> for conceptual <phrase>design</phrase> in <phrase>galileo</phrase>.
dataid-d: methodology for <phrase>distributed database</phrase> <phrase>design</phrase>.
dynamics in logical <phrase>database design</phrase>.
<phrase>architecture</phrase> of a physical <phrase>design</phrase> tool for <phrase>relational</phrase> dbmss.
important issues in <phrase>database design</phrase> methodologies and tools.
integrated tools for physical <phrase>database design</phrase> in codasyl environment.
concepts, implementation, and applications of a typed <phrase>logic</phrase> <phrase>programming language</phrase>.
<phrase>combinatorial</phrase> <phrase>problem solving</phrase> in constraint <phrase>logic programming</phrase> with cooperating solvers.
the wam - definition and <phrase>compiler</phrase> correctness.
using constraint <phrase>logic programming</phrase> for <phrase>industrial</phrase> scheduling problems.
<phrase>temporal logic</phrase> <phrase>programming</phrase> applied to image <phrase>sequence</phrase> evaluation.
goal-<phrase>directed</phrase> <phrase>forward chaining</phrase>: <phrase>tuple</phrase>-oriented bottom-up approach.
<phrase>logic</phrase> program modules for interoperable <phrase>information</phrase> systems.
<phrase>robot</phrase> control systems as contextual <phrase>logic</phrase> programs.
<phrase>logic programming</phrase> - past or future?
polymorphic feature types.
automatic verification of parallel <phrase>logic</phrase> programs: termination.
scheduling and meta-scheduling.
efficient <phrase>object-oriented programming</phrase> in <phrase>prolog</phrase>.
a generic scheduling framework developed in <phrase>prolog</phrase>.
dialogo: an interactive environment for conceptual <phrase>design</phrase> in <phrase>galileo</phrase>.
requirements collection and analysis.
incod-dte: a system for interative conceptual <phrase>design</phrase> of <phrase>data</phrase>, transactions and events.
views integration.
logical <phrase>design</phrase> in codasyl and <phrase>relational</phrase> environment.
a separability-based method for <phrase>secondary</phrase> index selection in physical <phrase>database design</phrase>.
a set of integrated tools for the conceptual <phrase>design</phrase> of <phrase>database</phrase> schemas and transactions.
views conceptual <phrase>design</phrase>.
methodology and tools for <phrase>data</phrase> base <phrase>design</phrase> in the dataid project.
nlda: a <phrase>natural language</phrase> reasoning system for the analysis of <phrase>data</phrase> base requirements.
physical <phrase>data</phrase> base <phrase>design</phrase> for codasyl <phrase>dbms</phrase>.
<phrase>algorithms</phrase> for finding patterns in <phrase>strings</phrase>.
<phrase>logic programming</phrase>.
<phrase>functional programming</phrase> and <phrase>lambda calculus</phrase>.
<phrase>context-free</phrase> languages.
machine models and <phrase>simulation</phrase>.
the complexity of finite functions.
<phrase>graph</phrase> rewriting: an <phrase>algebraic</phrase> and <phrase>logic</phrase> approach.
recursive applicative program schemes.
methods and logics for proving programs.
rewrite systems.
temporal and <phrase>modal logic</phrase>.
<phrase>semantic</phrase> domains.
a catalog of complexity classes.
elements of <phrase>relational database</phrase> theory.
the goal of this <phrase>paper</phrase> is to provide a systematic and unifying introduction to <phrase>relational database</phrase> theory, including some of the recent developments in <phrase>database</phrase> <phrase>logic programming</phrase>. the first part of the presentation covers the two <phrase>basic</phrase> components of the <phrase>relational</phrase> <phrase>data model</phrase>: its specification component, that is the <phrase>database</phrase> scheme with dependencies, and its operational component, that is the <phrase>relational algebra</phrase> <phrase>query language</phrase>. the choice of <phrase>basic</phrase> constructs for specifying the semantically meaningful <phrase>databases</phrase> and for querying them is justified through an in-depth investigation of their properties. some important <phrase>research</phrase> themes are reviewed in this context: the analysis of the <phrase>hypergraph</phrase> <phrase>syntax</phrase> of a <phrase>database</phrase> scheme and the extensions of the <phrase>query language</phrase> using deduction or <phrase>universal</phrase> relation assumptions. the subsequent parts of the presentation are structured around the two fundamental concepts illustrated in the first part, dependencies and queries. the main themes of <phrase>dependency theory</phrase> are implication problems and applications to <phrase>database</phrase> scheme <phrase>design</phrase>. queries are classified in a <phrase>variety</phrase> of ways, with emphasis on the connections between the expressibility of query languages, finite <phrase>model theory</phrase> and <phrase>logic programming</phrase>. the theory of queries is very much related to <phrase>research</phrase> on <phrase>database</phrase> <phrase>logic</phrase> programs, which are an elegant formalism for the study of the principles of <phrase>knowledge base</phrase> systems. the optimization of such programs involves both techniques developed for the <phrase>relational</phrase> <phrase>data model</phrase> and new methods for analyzing recursive definitions. the exposition closes with a discussion of how <phrase>relational database</phrase> theory deals with the problems of complex objects, <phrase>incomplete information</phrase> and <phrase>database</phrase> updates.
parallel <phrase>algorithms</phrase> for <phrase>shared-memory</phrase> machines.
logics of programs.
none available
<phrase>distributed computing</phrase>: models and methods.
<phrase>graph</phrase> <phrase>algorithms</phrase>.
<phrase>vlsi</phrase> theory.
<phrase>algorithms</phrase> in <phrase>number theory</phrase>.
<phrase>kolmogorov complexity</phrase> and its applications.
<phrase>data structures</phrase>.
operational and <phrase>algebraic</phrase> <phrase>semantics</phrase> of concurrent processes.
type systems for <phrase>programming languages</phrase>.
<phrase>denotational semantics</phrase>.
finite <phrase>automata</phrase>.
<phrase>communication</phrase> networks.
<phrase>cryptography</phrase>.
<phrase>formal language</phrase> and <phrase>power series</phrase>.
algorithmic <phrase>motion planning</phrase> in <phrase>robotics</phrase>.
machine-<phrase>independent</phrase> <phrase>complexity theory</phrase>.
<phrase>algebraic</phrase> <phrase>complexity theory</phrase>.
<phrase>automata</phrase> on infinite objects.
<phrase>general</phrase> purpose parallel architectures.
<phrase>average</phrase>-case analysis of <phrase>algorithms</phrase> and <phrase>data structures</phrase>.
<phrase>algebraic</phrase> specification.
<phrase>computational geometry</phrase>.
<phrase>suffix</phrase>, prefix and maximal <phrase>tree</phrase> codes.
fixed point characterization of weak monadic <phrase>logic</phrase> definable sets of <phrase>trees</phrase>.
recognizing sets of labelled <phrase>acyclic</phrase> <phrase>graphs</phrase>.
recognizable sets of unrooted <phrase>trees</phrase>.
structural complexity of classes of <phrase>tree</phrase> languages.
decidability of the inclusion in monoids generated by <phrase>tree</phrase> transformation classes.
unification procedures in automated deduction methods based on matings: a survey.
<phrase>algebraic</phrase> specification of <phrase>action</phrase> <phrase>trees</phrase> and recursive processes.
<phrase>trees</phrase> and <phrase>algebraic</phrase> <phrase>semantics</phrase>.
<phrase>tree</phrase>-adjoining grammars and lexicalized grammars.
<phrase>computing</phrase> <phrase>trees</phrase> with <phrase>graph</phrase> rewriting systems with priorities.
<phrase>binary tree</phrase> codes.
a <phrase>monoid</phrase> approach to <phrase>tree</phrase> <phrase>automata</phrase>.
a survey of <phrase>tree</phrase> transductions.
rational and recognizable infinite <phrase>tree</phrase> sets.
<phrase>automata</phrase> on infinite <phrase>trees</phrase> and rational control.
interpretability and <phrase>tree</phrase> <phrase>automata</phrase>: a simple way to solve algorithmic problems on <phrase>graphs</phrase> closely related to <phrase>trees</phrase>.
ambiguity and valuedness.
a <phrase>short</phrase> proof of the factorization <phrase>forest</phrase> theorem.
a theory of <phrase>tree</phrase> <phrase>language</phrase> varieties.
computer-aided <phrase>database design</phrase>: the dataid approach.
<phrase>computability</phrase>, complexity, <phrase>logic</phrase> (<phrase>english</phrase> <phrase>translation</phrase> of "berechenbarkeit, komplexitt, logik" from 1985)
methodology and tools for <phrase>data</phrase> base <phrase>design</phrase>.
<phrase>data</phrase> and <phrase>reality</phrase>, 1st edition.
handbook of theoretical computer <phrase>science</phrase>, volume a: <phrase>algorithms</phrase> and complexity
handbook of theoretical computer <phrase>science</phrase>, volume b: formal models and <phrase>semantics</phrase>
<phrase>tree</phrase> <phrase>automata</phrase> and languages.
handbook of <phrase>automated reasoning</phrase> (in 2 volumes)
classification, estimation and <phrase>pattern recognition</phrase>.
mthodes de programmation, 1st edition
mthodes de programmation, <phrase>3rd edition</phrase>
logout - warum computer nichts im klassenzimmer zu suchen haben und andere <phrase>high</phrase>-tech-ketzereien
<phrase>fractal geometry</phrase> of <phrase>nature</phrase>.
fractals, chaos, power laws: minutes from an infinite <phrase>paradise</phrase>.
<phrase>computers</phrase> and intractability: a guide to the theory of <phrase>np-completeness</phrase>.
spatial analysis and <phrase>gis</phrase>.
deterministic <phrase>translation</phrase> grammars
the analysis of a practical and nearly optimal priorty queue
queueing network models of multiprogramming
a practical formal <phrase>semantic</phrase> definition and verification system for typed <phrase>lisp</phrase>
linear lists and prorty queues as balanced <phrase>binary</phrase> <phrase>trees</phrase>
computer display of curved surfaces
the application of <phrase>theorem proving</phrase> to <phrase>question-answering</phrase> systems
decidability questions for <phrase>petri nets</phrase>
an understanding of the <phrase>mathematical</phrase> properties of <phrase>petri nets</phrase> is essential when one wishes to use <phrase>petri nets</phrase> as an abstract <phrase>model</phrase> for concurrent systems. the decidability of various problems which arise in this context is an important <phrase>aspect</phrase> of this question. the fact that these problems also arise in the context of other <phrase>mathematical</phrase> theories, such as <phrase>commutative</phrase> semigroups, closure under linear relations, matrix <phrase>context-free</phrase> grammars, or weak counter <phrase>automata</phrase>, provides further <phrase>motivation</phrase>. the <phrase>reachability</phrase> problem for <phrase>vector</phrase> addition systems - whose decidability is still an open question - is of central importance. we show that a number of <phrase>petri net</phrase> problems are recursively equivalent to this problem. these include the liveness problem (e.g. can a system reach a deadlocked <phrase>state</phrase>?), the persistence problem (can a given transition ever be disabled by the firing of another transition?), and the membership and emptiness problems for certain classes of languages generated by <phrase>petri nets</phrase>. the power of the unrestricted <phrase>petri net</phrase> <phrase>model</phrase> is illustrated by various <phrase>undecidable</phrase> equivalence <phrase>results</phrase>. in particular, we show that the equality of <phrase>reachability</phrase> sets and the equivalence of two <phrase>petri nets</phrase> in terms of their <phrase>language</phrase>-generating capability are recursive <phrase>undecidable</phrase>. it is hoped that the constructions used to prove our <phrase>results</phrase> will shed some <phrase>light</phrase> on the source of the complexities of the unrestricted <phrase>petri net</phrase> <phrase>model</phrase>, and may eventually <phrase>permit</phrase> us to achieve an optimal balance between representational transparency and analytical power of the <phrase>petri net</phrase> <phrase>model</phrase>.
control-theoretic formulation of <phrase>operating systems</phrase> resource <phrase>management</phrase> policies
the metanovel: writing stories by computer
a processor <phrase>design</phrase> for the efficient implementation of apl
the <phrase>design</phrase> and <phrase>construction</phrase> of flexible and efficient interactive <phrase>programming</phrase> systems
reasoning from incomplete <phrase>knowledge</phrase> in a procedural deduction system
axiomatic proof techniques for parallel programs
this <phrase>thesis</phrase> presents an axiomatic method for proving certain correctness properties of parallel programs. <phrase>axioms</phrase> and inference rules for partial correctness are given for two <phrase>parallel programming</phrase> languages: the <phrase>general</phrase> parallel <phrase>language</phrase> and the restricted parallel <phrase>language</phrase>. the <phrase>general</phrase> <phrase>language</phrase> is flexible enough to represent most standard synchronizers (e.g. semaphores, events), so that programs using these synchronizers may be verified using the <phrase>gpl</phrase> <phrase>deductive</phrase> system. however, proofs for <phrase>gpl</phrase> programs are in <phrase>general</phrase> quite complex. the restricted <phrase>language</phrase> reduces this complexity by requiring shared variables to be <phrase>protected</phrase> by critical sections, so that only one process at a time has access to them. this discipline does not significantly reduce the power of the <phrase>language</phrase>, and it greatly simplifies the process of program verification. although the <phrase>axioms</phrase> and inference rules are primarily intended for proofs of partial correctness, there are a number of other important properties of parallel programs. we give some practical techniques which use <phrase>information</phrase> obtained from a partial correctness proof to derive other correctness properties, including program termination, <phrase>mutual exclusion</phrase>, and freedom from <phrase>deadlock</phrase>. a number of examples of such proofs are given. finally, the <phrase>axioms</phrase> and inference rules are shown to be consistent and complete (in a special sense) with respect to an interpretive <phrase>model</phrase> of parallel execution. thus the <phrase>deductive</phrase> system gives an accurate description of program execution and is powerful enough to yield a proof of any true partial correctness formula.
shellsort and sorting networks
machine <phrase>perception</phrase> of <phrase>three-dimensional</phrase> solids
source <phrase>language</phrase> <phrase>debugging</phrase> tools
<phrase>quicksort</phrase>
queueing models for computer systems with <phrase>general</phrase> service time <phrase>distributions</phrase>
a <phrase>data</phrase> definition facility for <phrase>programming languages</phrase>
<phrase>sketchpad</phrase>, a man-machine <phrase>graphical</phrase> <phrase>communication</phrase> system
the <phrase>sketchpad</phrase> system makes it possible for a man and a computer to converse rapidly through the medium of line drawings. heretofore, most interaction between man and <phrase>computers</phrase> has been slowed down by the need to reduce all <phrase>communication</phrase> to written statements that can be typed; in the past, we have been writing letters to rather than conferring with our <phrase>computers</phrase>. for many types of <phrase>communication</phrase>, such as describing the shape of a mechanical part or the connections of an <phrase>electrical circuit</phrase>, typed statements can prove cumbersome. the <phrase>sketchpad</phrase> system, by eliminating typed statements (except for legends) in favor of line drawings, opens up a new <phrase>area</phrase> of man-machine <phrase>communication</phrase>.
automatic verification of programs with complex <phrase>data structures</phrase>
studies in extensible <phrase>programming languages</phrase>
automatic generation of assemblers
understanding goal-based stories
predicate-<phrase>oriented database</phrase> search <phrase>algorithms</phrase>
<phrase>semantics</phrase> for a <phrase>question-answering</phrase> system
the hensel lemma in <phrase>algebraic</phrase> manipulation
<phrase>cluster analysis</phrase>.
objektorientierte softwareentwicklung
erfolgsschlssel objekttechnologie -- managerfhrer zur neuorganisation des softwareprozesses
<phrase>logic</phrase> for computer <phrase>science</phrase>: foundations of automatic <phrase>theorem proving</phrase>
an introduction to <phrase>compiler</phrase> contruction
<phrase>artificial intelligence</phrase> architectures for composition and performance environment.
<phrase>dynamic programming</phrase> for interactive <phrase>music</phrase> systems.
the <phrase>mechanization</phrase> of <phrase>intelligence</phrase> and the <phrase>human</phrase> aspects of <phrase>music</phrase>.
<phrase>artificial intelligence</phrase> in <phrase>music education</phrase>: a critical review.
<phrase>music</phrase>, <phrase>intelligence</phrase> and artificiality.
regarding <phrase>music</phrase>, machines, <phrase>intelligence</phrase> and the <phrase>brain</phrase>: an introduction to <phrase>music</phrase> and <phrase>ai</phrase>.
computer analysis of <phrase>jazz</phrase> chord <phrase>sequence</phrase>: is <phrase>solar</phrase> a <phrase>blues</phrase>?
<phrase>musical</phrase> pattern extraction and similarity assessment.
interactive <phrase>music</phrase> systems in ensemble performance.
symbolic <phrase>ai</phrase> versus <phrase>connectionism</phrase> in <phrase>music</phrase> <phrase>research</phrase>.
on the potential of <phrase>machine learning</phrase> for <phrase>music</phrase> <phrase>research</phrase>.
<phrase>musical</phrase> <phrase>knowledge</phrase>: what can <phrase>artificial intelligence</phrase> bring to the <phrase>musician</phrase>?
<phrase>artificial intelligence</phrase> and <phrase>music education</phrase>.
readings in <phrase>music</phrase> and <phrase>artificial intelligence</phrase>
<phrase>information extraction</phrase> from <phrase>free</phrase>-text <phrase>business</phrase> documents.
the objective of this chapter is an investigation of the applicability of <phrase>information extraction</phrase> techniques in <phrase>real-world</phrase> <phrase>business</phrase> applications dealing with textual <phrase>data</phrase> since <phrase>business</phrase> relevant <phrase>data</phrase> is mainly transmitted through <phrase>free</phrase>-text documents. in particular, we give an overview of the <phrase>information extraction</phrase> task, designing <phrase>information extraction</phrase> systems and some examples of existing <phrase>information extraction</phrase> systems applied in the financial, <phrase>insurance</phrase> and legal domains. furthermore, we demonstrate the enormous indexing potential of <phrase>lightweight</phrase> <phrase>linguistic</phrase> text processing techniques applied in <phrase>information extraction</phrase> systems and other closely related fields of <phrase>information technology</phrase> which concern processing vast amounts of textual <phrase>data</phrase>.
active rules and active <phrase>databases</phrase>: concepts and application.
this chapter surveys the topic of active rules and active <phrase>databases</phrase>. we analyze the <phrase>state</phrase> of the <phrase>art</phrase> of active <phrase>databases</phrase> and active rules, their properties and applications. in particular, we describe the case of triggers following the <phrase>sql</phrase>-1999 standard committee point of view. then, we consider the case of dynamic constraints for which we use a <phrase>temporal logic</phrase> formalism. finally, we discuss the applicability, limitations and partial solutions found when attempting to ensure the satisfaction of dynamic constraints.
applying <phrase>java</phrase>-triggers for x-link <phrase>management</phrase> in the <phrase>industrial</phrase> framework.
this chapter focuses on referential link integrity problems. in the <phrase>industrial</phrase> context, the <phrase>life</phrase> cycle of a document plays a central role in describing the "steps out" of a product. users realize some manipulations like creation, edition, suppression and querying under a <phrase>multi-user</phrase> environment, risking possible destruction or the alteration of the document's integrity. a <phrase>classical</phrase> impact is the infamous "error 404: file not found." however, the user needs a notification alert mechanism to prevent and warrant the coherence of manipulations over all the <phrase>life</phrase> cycle processes of a product. the main objective of this chapter is to provide a generic relationship validation mechanism to remedy this shortcoming. we believe in the combination of some standard features of <phrase>xml</phrase>, specifically xll specification as a support for integrity <phrase>management</phrase> and <phrase>java</phrase>-triggers approach as an alert method. this study, compared with actual approaches, proposes a <phrase>solution</phrase> based on active functionalities.
novel indexing method of relations between salient objects.
since the last decade, images have been integrated into several application domains such as <phrase>gis</phrase>, <phrase>medicine</phrase>, etc. this integration necessitates new managing methods particularly in image retrieval. queries should be formulated using different types of features such as low-level features of images (histograms, color distribution, etc.), spatial and temporal relations between salient objects, <phrase>semantic</phrase> features, etc. in this chapter, we propose a novel method for identifying and indexing several types of relations between salient objects. spatial relations are used here to show how our method can provide <phrase>high</phrase> expressive power to relations in comparison to the traditional methods.
on the computation of <phrase>recursion</phrase> in <phrase>relational databases</phrase>.
a composite object represented as a <phrase>directed graph</phrase> is an important <phrase>data structure</phrase> which requires efficient support in <phrase>cad</phrase>/cam, case, office systems, <phrase>software</phrase> <phrase>management</phrase>, web <phrase>databases</phrase> and document <phrase>databases</phrase>. it is cumbersome to handle such an object in <phrase>relational database</phrase> systems when it involves recursive relationships. in this chapter, we present a new encoding method to support the efficient computation of <phrase>recursion</phrase>. in addition, we devise a linear time <phrase>algorithm</phrase> to identify a <phrase>sequence</phrase> of reachable <phrase>trees</phrase> (w.r.t.) a <phrase>directed acyclic graph</phrase> (dag), which covers all the edges of the <phrase>graph</phrase>. together with the new encoding method, this <phrase>algorithm</phrase> enables us to compute <phrase>recursion</phrase> w.r.t, a dag in time o(<phrase>e</phrase>), where <phrase>e</phrase> represents the number of edges of the dag. more importantly, this method is especially suitable for a <phrase>relational</phrase> environment.
building signature-<phrase>trees</phrase> on path signatures in document <phrase>databases</phrase>.
<phrase>java</phrase> is a prevailing implementation platform for <phrase>xml</phrase>-based systems. several <phrase>high</phrase>-quality in-<phrase>memory</phrase> implementations for the standardized <phrase>xml</phrase>-dom <phrase>api</phrase> are available. however, persistency support has not been addressed. in this chapter, we discuss this problem and introduce pdom (persistent dom) to accommodate documents as permanent object sets. in addition, we propose a new indexing technique: path signatures to speed up the evaluation of path-oriented queries against document object sets, which is further enhanced by combining the technique of signature-<phrase>trees</phrase> with it to expedite scanning of signatures stored in a physical file.
dealing with relationship <phrase>cardinality</phrase> constraints in <phrase>relational</phrase> <phrase>database design</phrase>.
conceptual models are well-known tools to achieve a good <phrase>design</phrase> of <phrase>information</phrase> systems. nevertheless, the understanding and use of all the constructs and constraints which are presented in such models are not an easy task and sometimes it is cause of loss of interest.in this chapter we have tried to study in depth and clarify the meaning of the features of conceptual models. the disagreements between main conceptual models, the confusion in the use of some of their constructs and some open problems in these models are shown. another important topic treated in this chapter is the conceptual-to-<phrase>logic</phrase> schemata transformation process.some solutions are presented in <phrase>order</phrase> to clarify the relationship construct and to extend the <phrase>cardinality</phrase> constraint concept in <phrase>ternary</phrase> relationships. how to preserve the <phrase>cardinality</phrase> constraint <phrase>semantics</phrase> in <phrase>binary</phrase> and <phrase>ternary</phrase> relationships for their implementation in a <phrase>dbms</phrase> with active capabilities has also been developed.
repairing and querying inconsistent <phrase>databases</phrase>.
the integration of <phrase>knowledge</phrase> from multiple sources is an important <phrase>aspect</phrase> in several areas such as <phrase>data warehousing</phrase>, <phrase>database</phrase> integration, <phrase>automated reasoning</phrase> systems, active reactive <phrase>databases</phrase> and others. thus a central topic in <phrase>databases</phrase> is the <phrase>construction</phrase> of integration systems, designed for retrieving and querying uniform <phrase>data</phrase> stored in multiple <phrase>information</phrase> sources. this chapter illustrates recent techniques for <phrase>computing</phrase> repairs as well as consistent answers over inconsistent <phrase>databases</phrase>. often <phrase>databases</phrase> may be inconsistent with respect to a set of <phrase>integrity constraints</phrase>, that is, one or more <phrase>integrity constraints</phrase> are not satisfied. most of the techniques for <phrase>computing</phrase> repairs and queries over inconsistent <phrase>databases</phrase> work for restricted cases and only recently there have been proposals to consider more <phrase>general</phrase> constraints. in this chapter we give an informal description of the main techniques proposed in the <phrase>literature</phrase>.
unifying access to heterogeneous document <phrase>databases</phrase> through contextual <phrase>metadata</phrase>.
document <phrase>databases</phrase> available on the <phrase>internet</phrase> carry massive <phrase>information</phrase> resources. to a person needing a piece of <phrase>information</phrase> on a specific domain, finding the piece, however, is often quite problematic even though there were a representative collection of <phrase>databases</phrase> available on the domain. the languages used in the content, the names of document types, their structures, the ways documents are organized and their retrieval techniques often vary in the <phrase>databases</phrase>. the <phrase>databases</phrase> containing legal <phrase>information</phrase> on the <phrase>internet</phrase> offer a typical example. for finding relevant documents and for being able to interpret the content of the documents correctly, the user may need <phrase>information</phrase> about the context where the documents have been created. in this chapter we introduce a method for collecting contextual <phrase>metadata</phrase> and for representing the <phrase>metadata</phrase> to the users by <phrase>graphical</phrase> models. the <phrase>solution</phrase> is demonstrated by a case of retrieving <phrase>information</phrase> from distributed <phrase>european</phrase> legal <phrase>databases</phrase>.
managing document taxonomies in <phrase>relational databases</phrase>.
this chapter addresses the challenge of applying <phrase>relational database</phrase> technologies to manage taxonomies, which are commonly used to classify documents, <phrase>knowledge</phrase> and <phrase>websites</phrase> into a hierarchy of topics. it first describes how denormalizing the <phrase>data model</phrase> can facilitate <phrase>data</phrase> retrieval from such topic hierarchies. it then shows how the typical <phrase>data</phrase> maintenance difficulties associated with denormalized <phrase>data</phrase> models can be solved using <phrase>database</phrase> triggers.
<phrase>database management</phrase> issues in the web environment.
the focus of this chapter is on the progressive adaptation of <phrase>database</phrase> techniques to web usage in a way quite similar to the <phrase>evolution</phrase> from integrated file <phrase>management</phrase> systems to <phrase>database management systems</phrase>. we review related and open issues, such as the semi-structured <phrase>data</phrase> and <phrase>xml</phrase>, integrity problem, <phrase>query optimization</phrase> problem, and integration issues in both the web and <phrase>semantic web</phrase> environments. the representation of meta-<phrase>information</phrase> along with <phrase>data</phrase> opens up a new way to automatically process web <phrase>information</phrase> due to the use of explicit <phrase>semantic</phrase> <phrase>information</phrase>. we hope that researchers will take into account traditional <phrase>database</phrase> techniques and how they can assist new web technologies. in this sense, the amalgamation of web and <phrase>database</phrase> <phrase>technology</phrase> appears to be very promising.
interactive indexing of documents with a multilingual <phrase>thesaurus</phrase>.
with the growing significance of <phrase>digital libraries</phrase> and the <phrase>internet</phrase>, more and more <phrase>electronic</phrase> texts become accessible to a wide and geographically disperse <phrase>public</phrase>. this requires adequate tools to facilitate indexing, storage and retrieval of documents written in different languages. we present a method for <phrase>semi-automatic</phrase> indexing of <phrase>electronic</phrase> documents and <phrase>construction</phrase> of a multilingual <phrase>thesaurus</phrase>, which can be used for query formulation and <phrase>information retrieval</phrase>. we use special dictionaries and user interaction in <phrase>order</phrase> to solve ambiguities and find adequate canonical terms in the <phrase>language</phrase> and an adequate abstract <phrase>language</phrase>-<phrase>independent</phrase> term. the abstract <phrase>thesaurus</phrase> is updated incrementally by new indexed documents and is used to search for documents using adequate terms.
understanding functional dependency.
in explaining functional dependency to students, i have noticed in texts a mixture of two types of elements: intensional (or <phrase>psychological</phrase> or meaning) and <phrase>extensional</phrase> (patterns of repetition in the <phrase>data</phrase>). in this chapter i examine whether it is possible to consider functional dependency, in particular, in second and third normal forms, solely on an <phrase>extensional</phrase> basis. the <phrase>microsoft access</phrase> analyzer utility seems to do so. i illustrate the mix of intensional and <phrase>extensional</phrase> elements in <phrase>textbook</phrase> definitions of functional dependency. i conclude that although in principle first, second and third normal form can be done solely by <phrase>extensional</phrase> means, in practice intensional considerations are indispensable. finally, i discuss these questions with respect to the "higher <phrase>order</phrase>" normal forms, namely boyce-codd, fourth, fifth and domain/key normal form.
keyword-based queries over web <phrase>databases</phrase>.
in this chapter, we propose an approach to using keywords (as in a <phrase>web search engine</phrase>) for querying <phrase>databases</phrase> over the web. the approach is based on a <phrase>bayesian network</phrase> <phrase>model</phrase> and provides a suitable <phrase>alternative</phrase> to the use of interfaces based on multiple forms with several fields. two <phrase>major</phrase> steps are involved when querying a web <phrase>database</phrase> using this approach. first, structured (<phrase>database</phrase>-like) queries are derived from a query composed only of the keywords specified by the user. next, the structured queries are submitted to a web <phrase>database</phrase>, and the retrieved <phrase>results</phrase> are presented to the user as ranked answers. to demonstrate the feasibility of the approach, a simple <phrase>prototype</phrase> <phrase>web search</phrase> system based on the approach is presented. <phrase>experimental</phrase> <phrase>results</phrase> obtained with this system indicate that the approach allows for accurately structuring the user queries and retrieving appropriate answers with minimum intervention from the user.
system of <phrase>information retrieval</phrase> in <phrase>xml</phrase> documents.
this chapter introduces the process to retrieve units (or subdocuments) of relevant <phrase>information</phrase> from <phrase>xml</phrase> documents. for this, we use the <phrase>extensible markup language</phrase> (<phrase>xml</phrase>) which is considered as a new standard for <phrase>data</phrase> representation and exchange on the web. <phrase>xml</phrase> opens opportunities to develop a new generation of <phrase>information retrieval</phrase> system (<phrase>irs</phrase>) to improve the interrogation process of document bases on the web.our work focuses instead on end-users who do not have expertise in the domain (like a majority of the end-users). this approach supports keyword-based searching like <phrase>classical</phrase> <phrase>irs</phrase> and integrates structured searching with the search attributes notion. it is based on an indexing method of document <phrase>tree</phrase> <phrase>leafs</phrase> which authorize a content-oriented retrieval. the retrieval subdocuments are ranked according to their similarity with the user's query. we use a similarity measure which is a compromise between two measures: exhaustiveness and specificity.
a <phrase>taxonomy</phrase> for <phrase>object-relational</phrase> queries.
a <phrase>comprehensive</phrase> study of <phrase>object-relational</phrase> queries gives not only an understanding of full capability of <phrase>object-relational</phrase> <phrase>query language</phrase> but also a direction for <phrase>query processing</phrase> and optimization. this chapter classifies <phrase>object-relational</phrase> queries into ref queries, aggregate queries and <phrase>inheritance</phrase> queries. ref queries are queries involving ref pointers, whereas aggregation queries use either nested table structures or index on clusters. finally, <phrase>inheritance</phrase> queries are queries on <phrase>inheritance</phrase> hierarchies.
re-<phrase>engineering</phrase> and <phrase>automation</phrase> of <phrase>business processes</phrase>: criteria for selecting supporting tools.
re-<phrase>engineering</phrase> of <phrase>business processes</phrase> and their <phrase>automation</phrase> is an activity very common in most organizations in <phrase>order</phrase> to keep or create a competitive <phrase>business</phrase> advantage in the changing <phrase>business</phrase> environment. <phrase>business process modeling</phrase> tools (bpmts) and <phrase>workflow</phrase> <phrase>management</phrase> systems (wfmss) are the most popular tools used for <phrase>business process</phrase> transformation and <phrase>automation</phrase> of the redesigned <phrase>business processes</phrase> within and outside organization's boundaries. this chapter describes a set of criteria for selecting appropriate bpmts and wfmss among the diversity of the tools offered in the market in <phrase>order</phrase> to assist the interested <phrase>manager</phrase> or <phrase>business process</phrase> <phrase>engineer</phrase> to more successfully manage the <phrase>business process</phrase> transformation. while establishing the proposed criteria, we considered currently available <phrase>technology</phrase> and standards for visual enterprise support and inter-organizational <phrase>business process modeling</phrase> and <phrase>automation</phrase>.
metrics for <phrase>data warehouse</phrase> quality.
this chapter proposes a set of metrics to assess <phrase>data warehouse</phrase> quality. a set of <phrase>data warehouse</phrase> metrics is presented, and the formal and empirical validations that have been done with them. as we consider that <phrase>information</phrase> is the main organizational asset, one of our primary duties should be assuring its quality. although some interesting guidelines have been proposed for designing "good" <phrase>data</phrase> models for <phrase>data</phrase> warehouses, more objective indicators are needed. metrics are a useful objective mechanism for improving the quality of <phrase>software</phrase> <phrase>products</phrase> and also for determining the best ways to help professionals and researchers. in this way, our goal is to elaborate a set of metrics for measuring <phrase>data warehouse</phrase> quality which can help designers in choosing the best option among more than one <phrase>alternative</phrase> <phrase>design</phrase>.
<phrase>integrity constraints</phrase> in an active <phrase>database</phrase> environment.
this chapter surveys the interaction between active rules and <phrase>integrity constraints</phrase>. first, we analyze the static case following the <phrase>sql</phrase>-1999 standard committee point of view which, up to date, represents the <phrase>state</phrase> of the <phrase>art</phrase>. then, we consider the case of dynamic constraints for which we use a <phrase>temporal logic</phrase> formalism. finally, we discuss the applicabilty, limitations and partial solutions found when attempting to ensure the satisfaction of dynamic constraints.
<phrase>integrity constraints</phrase> in spatial <phrase>databases</phrase>.
preserving relationship <phrase>cardinality</phrase> constraints in <phrase>relational</phrase> schemata.
translating advanced integrity checking <phrase>technology</phrase> to <phrase>sql</phrase>.
introduction.
consistent queries over <phrase>databases</phrase> with <phrase>integrity constraints</phrase>.
integrity issues in the web: beyond distributed <phrase>databases</phrase>.
functional dependencies for value based identification in <phrase>object-oriented</phrase> <phrase>databases</phrase>.
<phrase>database</phrase> integrity: fundamental and current implementations.
in part i, this chapter survyes the <phrase>state</phrase> of the <phrase>art</phrase> of the <phrase>semantic</phrase> <phrase>integrity constraints</phrase> in some <phrase>relational</phrase> and <phrase>object relational</phrase> available <phrase>database systems</phrase>. in part <phrase>ii</phrase>, it also provides an overview of the <phrase>sql</phrase> standard integrity issues and describes <phrase>semantic</phrase> integrity support in the following dbmss: <phrase>oracle</phrase>, <phrase>ibm db2</phrase>, <phrase>informix</phrase>, <phrase>sybase</phrase> and postgresql.the <phrase>major</phrase> differences and similarities among these systems are analyzed in relation to the definition, <phrase>semantics</phrase> and fidelity to the <phrase>sql</phrase> standard prescriptions.
integrity maintenance in extensible <phrase>databases</phrase>.
<phrase>project management</phrase> in graduate <phrase>education</phrase>.
alignment of <phrase>business</phrase> and <phrase>knowledge management</phrase> strategy.
<phrase>project management</phrase> best practices to increase success.
autopoietic approach for <phrase>information</phrase> system development.
sponsorship in it <phrase>project management</phrase>.
a systematic approach for <phrase>information</phrase> systems evaluation.
organizing <phrase>multimedia</phrase> objects by using class <phrase>algebra</phrase>.
ict-supported gaming for <phrase>competitive intelligence</phrase>.
virtual communities in practice.
concepts and dynamics of the <phrase>application service provider</phrase> <phrase>industry</phrase>.
it supporting strategy formulation.
<phrase>credit card</phrase> users' <phrase>data mining</phrase>.
better executive <phrase>information</phrase> with the <phrase>dashboard</phrase> approach.
<phrase>project management</phrase> models in it.
<phrase>human</phrase> motion tracking and recognition.
expanding <phrase>data mining</phrase> power with system dynamics.
enhancing competitiveness of <phrase>b2b</phrase> and b2c <phrase>e-commerce</phrase>.
natural <phrase>computing</phrase>.
<phrase>information technology</phrase> usage in <phrase>nigeria</phrase>.
<phrase>distance education</phrase> success factors.
impacts of the <phrase>intranet</phrase> on pdo.
online learning as a form of accomodation.
smes amidst global technological changes.
legal issues of virtual organizations.
<phrase>telemedicine</phrase> in <phrase>healthcare</phrase> organisations.
<phrase>multicast</phrase> routing protocols, <phrase>algorithms</phrase> and its <phrase>qos</phrase> extensions.
triggers, rules and constraints in <phrase>databases</phrase>.
<phrase>data mining</phrase> for combining forecasts in inventory <phrase>management</phrase>.
spatial modeling of <phrase>risk</phrase> factors for <phrase>gender</phrase>-specific <phrase>child mortality</phrase>.
<phrase>content-based</phrase> retrieval concept.
<phrase>knowledge management</phrase> and <phrase>social learning</phrase>.
<phrase>interface design</phrase> issues for <phrase>mobile commerce</phrase>.
perturbations, accuracy and robustness in <phrase>neural networks</phrase>.
a duplicate <phrase>chinese</phrase> document image retrieval system.
<phrase>multimedia</phrase> evaluations based on <phrase>cognitive science</phrase> findings.
one-to-one <phrase>video-conferencing</phrase> <phrase>education</phrase>.
international <phrase>digital</phrase> studies approach for examining international online interactions.
structure- and <phrase>content-based</phrase> retrieval for <phrase>xml</phrase> documents.
strategic utilization of <phrase>data mining</phrase>.
incorporating <phrase>data stream</phrase> analysis into <phrase>decision support systems</phrase>.
<phrase>semantic web</phrase> fundamentals.
novel indexing method of relations between salient objects.
<phrase>democratic</phrase> <phrase>e-governance</phrase>.
<phrase>graph</phrase> encoding and <phrase>recursion</phrase> computation.
strategic <phrase>knowledge management</phrase> in <phrase>public</phrase> organizations.
<phrase>organization</phrase> and <phrase>management</phrase> issues of end user <phrase>computing</phrase>.
<phrase>management</phrase> considerations for <phrase>b2b</phrase> online exchanges.
behavioral perspective of <phrase>groupware</phrase> <phrase>adoption</phrase>.
addressing the central problem in cyber <phrase>ethics</phrase> through stories.
<phrase>data mining</phrase> in practice.
web technologies and <phrase>data warehousing</phrase> strategies.
similarity web pages retrieval technologies on the <phrase>internet</phrase>.
content description for face <phrase>animation</phrase>.
<phrase>data mining</phrase> in franchise organizations.
<phrase>diffusion</phrase> of <phrase>e</phrase>-learning as an educational <phrase>innovation</phrase>.
<phrase>franchising</phrase> and <phrase>information technology</phrase>.
<phrase>diffusion</phrase> of innovations in organisations.
<phrase>neural networks</phrase> for <phrase>automobile</phrase> <phrase>insurance</phrase> customers.
<phrase>project management</phrase> for it projects.
incremental expansion of a <phrase>distributed database</phrase> systems.
integrative document and <phrase>content management</phrase> solutions.
managing strategic it <phrase>investment</phrase> decisions.
innovative thinking in <phrase>software development</phrase>.
effective learning through optimum distance among team members.
user <phrase>spreadsheet</phrase> systems development.
<phrase>history</phrase> and future development of group <phrase>support systems</phrase>.
migrating legacy systems to the web.
evaluation of an open learning environment.
reasoning about user preferences.
portable portals for m-<phrase>commerce</phrase>.
collective intentional <phrase>action</phrase> in virtual communities.
<phrase>communication</phrase> <phrase>management</phrase> for large modules.
building <phrase>police</phrase>/<phrase>community</phrase> relations through virtual communities.
critical success factors of <phrase>erp</phrase> implementation.
executive judgement in <phrase>e</phrase>-<phrase>business</phrase> strategy.
<phrase>mobile</phrase> transaction models framework.
audience response systems and face-to-face learning.
isoluminance contours for <phrase>animated</phrase> visualization.
<phrase>data warehouse</phrase> development.
web initiatives and <phrase>e-commerce</phrase> strategy.
is implementation in the <phrase>uk</phrase> <phrase>health</phrase> sector.
<phrase>geographic information systems</phrase> as decision tools.
<phrase>macromedia flash</phrase> on the client and the server.
relationship <phrase>cardinality</phrase> constraints in <phrase>relational</phrase> <phrase>database design</phrase>.
<phrase>outsourcing</phrase> <phrase>information technology</phrase> in <phrase>australia</phrase>.
virtual teams as sociotechnical systems.
web access by older adult users.
market of resources for agile/virtual <phrase>enterprise integration</phrase>.
web <phrase>usability</phrase>.
challenges in quality of service for tomorrow's networks.
<phrase>artificial intelligence</phrase> techniques in <phrase>medicine</phrase> and <phrase>healthcare</phrase>.
methods for understanding is failures.
women in the it profession.
exception rules in <phrase>data mining</phrase>.
virtual work <phrase>research</phrase> agenda.
web caching.
governance in it <phrase>outsourcing</phrase> partnerships.
context-aware framework for <phrase>erp</phrase>.
<phrase>usability</phrase> evaluation of online learning programs.
hypothetical reasoning over <phrase>databases</phrase>.
computer attitude and <phrase>anxiety</phrase>.
<phrase>tertiary education</phrase> and the <phrase>internet</phrase>.
trust in <phrase>technology</phrase> partnerships.
<phrase>object database</phrase> benchmarks.
hyper <phrase>video</phrase> for <phrase>distance learning</phrase>.
rotating banner advertisements on the <phrase>world wide web</phrase>.
new perspectives on rewards and <phrase>knowledge</phrase> sharing.
<phrase>formal methods</phrase> in <phrase>software engineering</phrase>.
<phrase>design</phrase> levels for distance and online learning.
concepts of emergence index in image <phrase>databases</phrase>.
<phrase>interactivity</phrase> and <phrase>amusement</phrase> in <phrase>electronic commerce</phrase>.
<phrase>media</phrase> and personal involvement in the perceptions of <phrase>data quality</phrase>.
principles of advanced <phrase>database</phrase> integrity checking.
ecrm in a <phrase>manufacturing</phrase> environment.
alignment of <phrase>information technology</phrase> and <phrase>human resources</phrase> strategies.
integrating <phrase>security</phrase> in the development process with <phrase>uml</phrase>.
principles to guide the integration and implementation of <phrase>educational technology</phrase>.
strategic vision for <phrase>information technology</phrase>.
the <phrase>crm</phrase>-kdd nexus.
balancing risks and rewards of <phrase>erp</phrase>.
from <phrase>digital divide</phrase> to <phrase>digital</phrase> <phrase>dividend</phrase>.
<phrase>business model</phrase> application of <phrase>uml</phrase> <phrase>stereotypes</phrase>.
<phrase>motivation</phrase> for using microcomputers.
<phrase>globalization</phrase> of <phrase>consumer</phrase> <phrase>e-commerce</phrase>.
<phrase>journalism</phrase> online in <phrase>peru</phrase>.
<phrase>object-oriented</phrase> <phrase>software</phrase> reuse in <phrase>business</phrase> systems.
global implications of <phrase>e-commerce</phrase> tool and artifact creation.
impediments for <phrase>knowledge</phrase> sharing in <phrase>professional</phrase> service firms.
issues and challenges in it in small <phrase>business</phrase>.
audio analysis applications for <phrase>music</phrase>.
<phrase>web-based</phrase> <phrase>distance learning</phrase> and the second <phrase>digital divide</phrase>.
<phrase>web-based</phrase> <phrase>distance learning</phrase> programs promise learning options anywhere, anytime, to anyone. however, some individuals with disabilities are locked out of these opportunities when courses are designed in such a way that they are inaccessible to individuals using <phrase>assistive technology</phrase>. this chapter provides an overview of access challenges for people with disabilities; suggestions for course developers on creating accessible courses; and suggestions for administrators on developing accessiblity policies, guidelines, and procedures.
<phrase>data</phrase> communications and <phrase>e</phrase>-learning.
online <phrase>academic</phrase> <phrase>libraries</phrase> and <phrase>distance learning</phrase>.
use cases and the <phrase>uml</phrase>.
<phrase>u.s</phrase>. disabilities <phrase>legislation</phrase> affecting <phrase>information technology</phrase>.
critical realism as an underlying <phrase>philosophy</phrase> for is <phrase>research</phrase>.
<phrase>engineering</phrase> emotionally intelligent agents.
managing the organisational impacts of <phrase>information</phrase> systems.
learning portals as new <phrase>academic</phrase> spaces.
<phrase>database</phrase> integrity.
critical realism in is <phrase>research</phrase>.
informationbase - a new <phrase>information</phrase> system layer.
building <phrase>educational technology</phrase> partnerships through <phrase>participatory design</phrase>.
self-organizing networked learning environments.
recursive <phrase>nature</phrase> of the market for enterprise applications.
<phrase>e-commerce</phrase> training for smes.
<phrase>decision support systems</phrase> in small businesses.
learnability.
contingency theory, <phrase>agent-based</phrase> systems and a virtual advisor.
process-aware <phrase>information</phrase> systems for virtual teamwork.
assessing the value of <phrase>information</phrase> systems investments.
<phrase>business processes</phrase> and <phrase>knowledge management</phrase>.
moderator in governmen t-initiated online discussions.
<phrase>wireless</phrase> <phrase>ad hoc</phrase> networking.
<phrase>knowledge management</phrase> systems acceptance.
this chapter introduces a framework of <phrase>knowledge management</phrase> systems acceptance labeled requirements of acceptance <phrase>model</phrase> (<phrase>ram</phrase>). it argues that acceptance of <phrase>knowledge management</phrase> systems is dependent on perceived relevance, systems accessibility, and <phrase>management</phrase> support. together these components constitute the <phrase>ram</phrase>. further, it argues that implementation of systems is <phrase>at large</phrase> a process of acceptance where the requirements of acceptance are attained. finally, it argues that to achieve the requirements of acceptance, implementation should be iterative and <phrase>cooperative</phrase> between users and developers by continually developing, implementing, and testing <phrase>prototypes</phrase>.
an <phrase>erp</phrase> <phrase>life</phrase>-cycle costs <phrase>model</phrase>.
socio-<phrase>cognitive</phrase> <phrase>model</phrase> of trust.
<phrase>video</phrase> <phrase>content-based</phrase> retrieval techniques.
<phrase>forward</phrase> <phrase>engineering</phrase> of <phrase>uml</phrase> static models.
reuse of formal specifications.
trust placement process in metateam projects.
issues of <phrase>e</phrase>-learning in third world countries.
teams and <phrase>electronic</phrase> technologies.
<phrase>unified modeling language</phrase>.
functional integration of decision making support.
stickiness and <phrase>web-based</phrase> customer loyalty.
decision-making <phrase>support systems</phrase>.
<phrase>collaborative learning</phrase> on-demand.
<phrase>enterprise resource planning</phrase> for intelligent enterprises.
<phrase>e</phrase>-<phrase>mail</phrase> usage in <phrase>south</phrase> <phrase>pacific</phrase> <phrase>distance education</phrase>.
<phrase>privacy</phrase>-dangers and protections.
<phrase>usability</phrase> of online learning systems and course materials.
<phrase>software</phrase> contracts for <phrase>component-based</phrase> web <phrase>engineering</phrase>.
<phrase>translation</phrase> of <phrase>natural language</phrase> patterns to object and process modeling.
client/server and the <phrase>knowledge</phrase> directory.
multilingual <phrase>electronic commerce</phrase> in a global <phrase>economy</phrase>.
<phrase>census</phrase> <phrase>data</phrase> for <phrase>health</phrase> preparedness and response.
using <phrase>geographic information systems</phrase> to solve <phrase>community</phrase> problems.
agent- and <phrase>web-based</phrase> <phrase>employment</phrase> marketspaces in the <phrase>u.s</phrase>. <phrase>department</phrase> of defense.
parallel and distributed <phrase>multimedia</phrase> <phrase>databases</phrase>.
<phrase>knowledge</phrase>, it, and the <phrase>firm</phrase>.
<phrase>information</phrase> systems and <phrase>systems theory</phrase>.
heuristics in <phrase>medical</phrase> <phrase>data mining</phrase>.
optical <phrase>music</phrase> recognition with wavelets.
marketplace <phrase>architecture</phrase> for <phrase>enterprise integration</phrase>.
<phrase>mobile</phrase> location services.
<phrase>software</phrase> and <phrase>systems engineering</phrase> integration.
<phrase>ethics</phrase> of new technologies.
supporting assurnace and compliance monitoring.
infocratic perspective on the delivery of personal <phrase>financial services</phrase>.
benefits and challenges of <phrase>blended learning</phrase> environments.
computer-mediated learning groups.
<phrase>quality assurance</phrase> issues for online <phrase>universities</phrase>.
consistent queries over <phrase>databases</phrase> with <phrase>integrity constraints</phrase>.
user perceptions and <phrase>groupware</phrase> use.
the changing <phrase>library</phrase> <phrase>education</phrase> <phrase>curriculum</phrase>.
<phrase>diffusion</phrase> patterns of the <phrase>internet</phrase> <phrase>technology</phrase> cluster in <phrase>irish</phrase> smes.
<phrase>internet</phrase> abuse and addiction in the workplace.
application service provision for intelligent enterprises.
agents and payment systems in <phrase>e-commerce</phrase>.
<phrase>mobile</phrase> agent <phrase>authentication</phrase> and authorization in <phrase>e-commerce</phrase>.
<phrase>ontology</phrase>-based query formation and <phrase>information retrieval</phrase>.
theoretical framework for <phrase>crm</phrase> <phrase>outsourcing</phrase>.
packet inter-arrival <phrase>distributions</phrase> in computer network workloads.
<phrase>information</phrase> modeling in <phrase>uml</phrase> and <phrase>orm</phrase>.
<phrase>web-based</phrase> <phrase>supply chain</phrase> strategy.
<phrase>metadata</phrase> for <phrase>electronic</phrase> documents using the <phrase>dublin core</phrase>.
overcoming barriers in the planning of a virtual <phrase>library</phrase>.
<phrase>library</phrase> <phrase>management</phrase> and organizational change.
<phrase>technology</phrase> of formal <phrase>education</phrase>.
<phrase>security</phrase> issues in distributed <phrase>transaction processing</phrase> systems.
offshore <phrase>software development</phrase> <phrase>outsourcing</phrase>.
successful <phrase>health</phrase> <phrase>information</phrase> system implementation.
<phrase>space opera</phrase>- <phrase>gis</phrase> basics.
chief <phrase>knowledge</phrase> officers.
intelligent <phrase>software</phrase> agents.
<phrase>vrml</phrase>-based system for a 3d virtual <phrase>museum</phrase>.
dot-comming smes in <phrase>singapore</phrase> for a new <phrase>economy</phrase>.
<phrase>electronic</phrase> <phrase>government</phrase> strategies and <phrase>research</phrase> in the <phrase>u.s</phrase>..
process-based <phrase>data mining</phrase>.
<phrase>automation</phrase> of <phrase>american</phrase> <phrase>criminal justice</phrase>.
measurement issues in <phrase>decision support systems</phrase>.
after a brief discussion on the <phrase>history</phrase> of decision-making, this chapter focuses on metrics for justifying <phrase>investment</phrase> in <phrase>information</phrase> systems and <phrase>technology</phrase> and for measuring <phrase>business</phrase> and <phrase>management</phrase> performance. the discussion of metrics is linked to current practices in <phrase>decision support systems</phrase> and focuses on the needs for <phrase>future systems</phrase>. with several examples drawn from contemporary practice, we introduce implementation guidelines for dss development incorporating new metrics that go beyond roi and <phrase>balanced scorecard</phrase>-like measures. suggested guidelines include simplicity, selectivity, a focus on <phrase>research</phrase> and learning, and <phrase>benchmarking</phrase>. these guidelines suggest that future metrics to support <phrase>decision support systems</phrase> should be grouped into meaningful categories and tied more closely to system <phrase>architecture</phrase>.
icts as participatory vehicles.
<phrase>content management</phrase> in organizations.
current <phrase>multicast</phrase> <phrase>technology</phrase>.
technology's role in <phrase>distance education</phrase>.
dimensions of <phrase>database</phrase> quality.
<phrase>world wide web</phrase> search technologies.
<phrase>information</phrase> systems and small <phrase>business</phrase>.
trust in <phrase>knowledge</phrase>-based organizations.
<phrase>enterprise resource planning</phrase> and systems integration.
<phrase>instant messaging</phrase> moves from the home to the office.
<phrase>electronic commerce</phrase> policies for <phrase>dutch</phrase> smes.
measuring collaboration in online communications.
<phrase>electronic</phrase>/<phrase>digital</phrase> <phrase>government</phrase> <phrase>innovation</phrase>, and <phrase>publishing</phrase> trends with it.
<phrase>e-government</phrase>, <phrase>e-democracy</phrase> and the politicians.
<phrase>simulation</phrase> for <phrase>business</phrase> <phrase>engineering</phrase> of <phrase>electronic</phrase> markets.
the past, present, and future of end-user performance.
<phrase>adoption</phrase> of <phrase>e-commerce</phrase> in the value chain by smes.
<phrase>internet</phrase> support for <phrase>knowledge management</phrase> systems.
case-based learning in computer <phrase>information</phrase> systems.
a <phrase>language</phrase>/<phrase>action</phrase> <phrase>based approach</phrase> to <phrase>information</phrase> modeling.
harmonizing it and <phrase>business</phrase> strategies.
this chapter proposes that all <phrase>business</phrase> strategies should be harmonized into a <phrase>single</phrase> strategy, rather than attempt to align it strategy with <phrase>business</phrase> strategy. it focuses on two hypotheses: firstly, that it strategy is not widely aligned with <phrase>business</phrase> strategy; and secondly, that it is still thought of as "something different" in businesses. the chapter proposes that rather than attempt to align it strategy with <phrase>business</phrase> strategy, the strategies should be harmonized into a <phrase>single</phrase> strategy. the chapter attempts to use lessons from geese to outline the process of strategic development.
intelligent metabusiness.
small busines transformation through <phrase>knowledge management</phrase>.
fundamentals of multirate systems.
simple methods for <phrase>design</phrase> of <phrase>narrowband</phrase> highpass <phrase>fir</phrase> filters.
one method for <phrase>design</phrase> of <phrase>narrowband</phrase> lowpass filters.
this chapter describes a <phrase>design</phrase> of a <phrase>narrowband</phrase> lowpass <phrase>finite impulse response</phrase> (<phrase>fir</phrase>) filter using a small number of multipliers per output sample (mps). the <phrase>method is based</phrase> on the use of a <phrase>frequency</phrase>-improved recursive running sum (rrs), called the sharpening rrs filter, and the interpolated <phrase>finite impulse response</phrase> (ifir) structure. the filter sharpening technique uses multiple copies of the same filter according to an <phrase>amplitude</phrase> change <phrase>function</phrase> (acf), which maps a <phrase>transfer function</phrase> before sharpening to a desired form after sharpening. three acfs are used in the <phrase>design</phrase>, as illustrated in the accompanying examples.
flexible job-shop scheduling problems.
the <phrase>software industry</phrase> in <phrase>egypt</phrase>.
on the <phrase>relativity</phrase> of <phrase>ontological</phrase> domains and their specifications.
<phrase>e</phrase>-collaboration <phrase>support systems</phrase> issues to be addressed.
cross-<phrase>cultural</phrase> <phrase>research</phrase> in mis.
standards for <phrase>web-based</phrase> integration adapters.
<phrase>interoperability</phrase> in geospatial <phrase>information</phrase> systems.
<phrase>usability</phrase> and learnability evaluation of <phrase>web-based</phrase> odl programs.
facial and body <phrase>feature extraction</phrase> for emotionally-rich hci.
<phrase>gender</phrase> and computer <phrase>anxiety</phrase>.
cache <phrase>management</phrase> for web-powered <phrase>databases</phrase>.
<phrase>e</phrase>-learning environment.
formal development of reactive <phrase>agent-based</phrase> systems.
contracting mechanisms in is/it <phrase>outsourcing</phrase> phenomenon.
archival issues related to <phrase>digital</phrase> creations.
<phrase>curriculum</phrase> development in <phrase>web-based</phrase> <phrase>education</phrase>.
online communities and <phrase>community</phrase> building.
<phrase>information</phrase> resources development challenges in a cross-<phrase>cultural</phrase> environment.
organizations in the <phrase>western world</phrase> devote much attention to the development of <phrase>information</phrase> systems as strategic resources. the transfer of this <phrase>management</phrase> practice to a different <phrase>cultural</phrase> environment generates new challenges as reported in the experience of a <phrase>pacific basin</phrase> <phrase>public</phrase> institution with <phrase>western</phrase> affiliation, while under strong influence of local cultures and practices. the value of the <phrase>investment</phrase> in <phrase>information</phrase> resources is judged not only on technical merits, but also on the ability of the <phrase>organization</phrase> to properly address the <phrase>cultural</phrase> and organizational issues related to <phrase>information</phrase> usage.
real options analysis in strategic <phrase>information technology</phrase> <phrase>adoption</phrase>.
<phrase>virtualization</phrase> and its role in <phrase>business</phrase>.
this chapter presents some of the aspects of virtualisation and its role in modern <phrase>society</phrase>. in today's world, a controlled virtualisation process creates enormous opportunity for <phrase>economic growth</phrase> of those countries and organisations, which, so far, due to various restrictions, have had no chance to become competitive in the global market. those people and organisations that know how to make use of the opportunities presented by virtualisation may become more effective in <phrase>business</phrase>. moreover, virtualisation creates the best options for intellectual enterprise development. virtualisation is a very complex process. the <phrase>author</phrase> would like to discusses both the positive impact virtualisation can have on <phrase>society</phrase> and also some dangers or problems.
<phrase>information technology</phrase> strategic alignment.
promotion of <phrase>e-government</phrase> in <phrase>japan</phrase> and its operation.
<phrase>knowledge</phrase> discovery using heuristics.
<phrase>artificial neural networks</phrase> used in <phrase>automobile</phrase> <phrase>insurance</phrase> <phrase>underwriting</phrase>.
modeling <phrase>erp</phrase> acedemic deployment via ast.
scenarios for web-enhanced learning.
<phrase>business model</phrase> <phrase>innovation</phrase> in the <phrase>digital economy</phrase>.
<phrase>enterprise resource planning</phrase> maintenance concepts.
<phrase>multimedia</phrase> content adaption.
triangular strategic analysis for <phrase>hybrid</phrase> <phrase>e</phrase>-retailers.
<phrase>e-commerce</phrase> <phrase>curriculum</phrase>.
virtual <phrase>organization</phrase> in the <phrase>human</phrase> mind.
experiential perspective on <phrase>knowledge management</phrase>.
<phrase>mobile commerce</phrase> <phrase>technology</phrase>.
bridging the <phrase>industry</phrase>-<phrase>university</phrase> gap through <phrase>action research</phrase>.
challenges in m-<phrase>commerce</phrase>.
contemporary it-assisted <phrase>retail</phrase> <phrase>management</phrase>.
<phrase>security</phrase> and trust of <phrase>online auction</phrase> systems.
quality of <phrase>uml</phrase>.
mesh <phrase>object-oriented</phrase> <phrase>hypermedia</phrase> framework.
usable m-<phrase>commerce</phrase> systems.
application of <phrase>fuzzy logic</phrase> <phrase>fraud</phrase> detection.
structural <phrase>text mining</phrase>.
the impact of <phrase>sound</phrase> relationships on achieving alignment.
<phrase>internet</phrase> <phrase>data mining</phrase> using statistical techniques.
<phrase>business</phrase> modelling with client-oriented requirements strategy.
adaptive <phrase>mobile</phrase> applications.
<phrase>technology</phrase> and work in the virtual <phrase>organization</phrase>.
<phrase>enterprise resource planning</phrase> and integration.
<phrase>database</phrase> support for m-<phrase>commerce</phrase>.
minorities and the <phrase>digital divide</phrase>.
eight key elements of successful self-funding <phrase>e</phrase>-learning programs.
<phrase>drm</phrase> <phrase>technology</phrase> for <phrase>mobile</phrase> <phrase>multimedia</phrase>.
empirical study of <phrase>e-commerce</phrase> <phrase>adoption</phrase> smes in <phrase>thailand</phrase>.
<phrase>information</phrase> resources development in <phrase>china</phrase>.
certifying <phrase>software</phrase> product and processes.
comparing conventional and non-parametric option pricing.
contextual <phrase>metadata</phrase> for document <phrase>databases</phrase>.
<phrase>bayesian</phrase> <phrase>machine learning</phrase>.
intelligent <phrase>business</phrase> portals.
implementation <phrase>management</phrase> of an <phrase>e-commerce</phrase>-enabled enterprise <phrase>information</phrase> system.
<phrase>culture</phrase> and anonymity in gss meetings.
<phrase>public sector</phrase> <phrase>case study</phrase> on the benefits of is/it.
faculty perceptions and participation in <phrase>distance education</phrase>.
spatial analysis in a <phrase>public health</phrase> setting.
user experiences of the <phrase>e-commerce</phrase> site with the standard <phrase>user interface</phrase>.
newcomer assimilation in virtual team <phrase>socialization</phrase>.
softening the mis <phrase>curriculum</phrase> for a <phrase>technology</phrase>-based profession.
strategic alignment of organizational strategies.
introducing <phrase>java</phrase> to the it master's <phrase>curriculum</phrase>.
inexperienced and global <phrase>software</phrase> teams.
<phrase>simulation</phrase> and gaming in it <phrase>education</phrase>.
<phrase>systems thinking</phrase> and the <phrase>internet</phrase>.
speech and <phrase>audio signal</phrase> applications.
<phrase>wireless</phrase> <phrase>middleware</phrase>.
trust in b2c <phrase>e-commerce</phrase> for the new <phrase>zealand</phrase> maori.
<phrase>technology</phrase> planning in schools.
<phrase>multimedia</phrase> <phrase>information</phrase> filtering.
designing omis-based collaboration for learning organizations.
personal <phrase>internet</phrase> usage and quality of work <phrase>life</phrase>.
trends in <phrase>information technology</phrase> governance.
bridging the <phrase>digital divide</phrase> in <phrase>scotland</phrase>.
critical strategies for is projects.
behavioral factors in strategic alliances.
multiple <phrase>internet</phrase> technologies in in-class <phrase>education</phrase>.
<phrase>business process</phrase> and <phrase>workflow</phrase> modeling in <phrase>web services</phrase>.
signature-based <phrase>indexing techniques</phrase> for web access logs.
transfering <phrase>technology</phrase> to the developing world.
<phrase>qos</phrase>-oriented mac protocols for future <phrase>mobile</phrase> applications.
learning <phrase>systems engineering</phrase>.
delivering <phrase>web-based</phrase> <phrase>education</phrase>.
<phrase>object-oriented</phrase> <phrase>software</phrase> metrics.
critical success factors for <phrase>distance education</phrase> programs.
designing agents with negotiation capabilities.
infosec policy - the foundation for an effective <phrase>security</phrase> program.
decision-making <phrase>support systems</phrase> and representation levels.
integrating <phrase>requirements engineering</phrase> techniques and <phrase>formal methods</phrase>.
future of small <phrase>business</phrase> <phrase>e-commerce</phrase>.
staying up-to-date with changes in it.
complex adaptive enterprises.
success <phrase>surrogates</phrase> in representational <phrase>decision support systems</phrase>.
cooperation of geographic and <phrase>multidimensional databases</phrase>.
managing value-creation in the <phrase>digital economy</phrase>.
hierarchies in <phrase>multidimensional databases</phrase>.
<phrase>e</phrase>-<phrase>mail</phrase> as a strategic tool in organizations.
<phrase>decision support systems</phrase> concept.
is <phrase>project management</phrase> contemporary <phrase>research</phrase> challenges.
classification-rule discovery with an <phrase>ant colony</phrase> <phrase>algorithm</phrase>.
<phrase>internet</phrase> <phrase>diffusion</phrase> in the <phrase>hospitality industry</phrase>.
a web-geographical <phrase>information</phrase> system to support territorial <phrase>data integration</phrase>.
educating the <phrase>business</phrase> <phrase>information</phrase> technologist.
<phrase>text mining</phrase> in the context of <phrase>business intelligence</phrase>.
<phrase>web search</phrase> via learning from relevance <phrase>feedback</phrase>.
<phrase>animated</phrase> characters within the <phrase>mpeg-4</phrase> standard.
<phrase>bayesian</phrase> modelling for <phrase>machine learning</phrase>.
current network <phrase>security</phrase> <phrase>software</phrase>.
building and <phrase>management</phrase> of trust in <phrase>information</phrase> systems.
sharing organizational <phrase>knowledge</phrase> through <phrase>knowledge</phrase> repositories.
efficient multirate filtering.
<phrase>basic</phrase> notions on multidimensional <phrase>aggregate data</phrase>.
managing hierarchies and taxonomies in <phrase>relational databases</phrase>.
intelligent <phrase>software</phrase> agents in <phrase>e-commerce</phrase>.
<phrase>innovation</phrase> link between <phrase>organization</phrase> <phrase>knowledge</phrase> and customer <phrase>knowledge</phrase>.
intelligent agents for <phrase>competitive advantage</phrase>.
<phrase>technology</phrase> and <phrase>knowledge management</phrase>.
<phrase>e-commerce</phrase> <phrase>taxation</phrase> issues.
policy frameworks for secure <phrase>electronic</phrase> <phrase>business</phrase>.
a socio-technical <phrase>case study</phrase> of <phrase>bangladesh</phrase>.
strategically-focused enterprise <phrase>knowledge management</phrase>.
<phrase>data mining</phrase> for <phrase>supply chain management</phrase> complex networks.
functionalities and position of <phrase>manufacturing</phrase> execution systems.
constructionist perspective of organizational <phrase>data mining</phrase>.
next-generation <phrase>erp</phrase>.
<phrase>evolution</phrase> of <phrase>erp</phrase> systems.
<phrase>database</phrase> technologies on the web.
modeling for <phrase>e</phrase>-learning systems.
limited-perspective bias in <phrase>technology</phrase> projects.
<phrase>xml schema</phrase> integration and <phrase>e-commerce</phrase>.
<phrase>web accessibility</phrase> and the <phrase>law</phrase>.
geospatial <phrase>information</phrase> systems and enterprise collaboration.
discovery of classification rules from <phrase>databases</phrase>.
technological collaboration and trust in virtual teams.
change process drivers for <phrase>e</phrase>-<phrase>business</phrase>.
relating <phrase>cognitive</phrase> <phrase>problem-solving</phrase> style to user resistance.
antecedents of trust in online communities.
<phrase>digital</phrase> <phrase>government</phrase> and individual <phrase>privacy</phrase>.
the growth of the <phrase>internet</phrase> and <phrase>digital</phrase> <phrase>government</phrase> has dramatically increased the <phrase>federal</phrase> government's ability to collect, analyze, and disclose personal <phrase>information</phrase> about many <phrase>private</phrase> aspects of citizens' lives. personal <phrase>information</phrase> once available only on <phrase>paper</phrase> to a limited number of people is now instantly retrievable anywhere in the world by anyone with a computer and an <phrase>internet</phrase> connection. over time, there has also been a declining level of trust by <phrase>americans</phrase> in <phrase>government</phrase>, and currently, many perceive the <phrase>government</phrase> as a potential threat to their <phrase>privacy</phrase>. given these forces at work in our <phrase>society</phrase>, one should not be surprised to read the <phrase>results</phrase> of surveys that show <phrase>privacy</phrase> as a top concern of citizens in the 21st century. if citizens do not believe that the <phrase>government</phrase> is adequately protecting the <phrase>privacy</phrase> of their individual <phrase>information</phrase>, they may be less willing to provide this <phrase>information</phrase>. such reluctance could compromise the ability of <phrase>government</phrase> to collect important <phrase>information</phrase> necessary to develop, administer and evaluate the impact of various policies and programs. <phrase>privacy</phrase> issues discussed in this chapter include challenges regarding (1) protecting personal <phrase>privacy</phrase>; (2) ensuring confidentiality of <phrase>data</phrase> collected; and (3) implementing appropriate <phrase>security</phrase> controls. perspectives on <phrase>privacy</phrase> and stewardship responsibilities of agencies are also discussed.
dynamic multidimensional <phrase>data</phrase> cubes for interactive analysis of massive datasets.
virtual work, trust and <phrase>rationality</phrase>.
extensions to <phrase>uml</phrase> using <phrase>stereotypes</phrase>.
<phrase>agent-based</phrase> negotiation in <phrase>e</phrase>-<phrase>marketing</phrase>.
<phrase>risk management</phrase> in the <phrase>digital economy</phrase>.
<phrase>contract</phrase>-based <phrase>workflow</phrase> <phrase>design patterns</phrase> in m-<phrase>commerce</phrase>.
modeling <phrase>information</phrase> systems in <phrase>uml</phrase>.
<phrase>e</phrase>-<phrase>business</phrase> transaction in web integrated network environment.
inclusion dependencies.
<phrase>multimedia</phrase> <phrase>computing</phrase> environment for telemedical applications.
<phrase>conducting</phrase> <phrase>ethical</phrase> <phrase>research</phrase> in <phrase>virtual environments</phrase>.
<phrase>technology</phrase>-mediated progressive inquiry in <phrase>higher education</phrase>.
adaptive <phrase>playout</phrase> control schemes for speech over the <phrase>internet</phrase>.
issues in delivering course material via the web.
how teachers use <phrase>instructional design</phrase> in real classrooms.
strategic experimentation and <phrase>knowledge management</phrase>.
trends and perspectives in online <phrase>education</phrase>.
evaluating computer-supported learning initiatives.
designing <phrase>web applications</phrase>.
innovations for online <phrase>collaborative learning</phrase> in <phrase>mathematics</phrase>.
shaping the <phrase>evolution</phrase> of <phrase>mobile commerce</phrase>.
basics of the <phrase>triune</phrase> continuum <phrase>paradigm</phrase>.
a <phrase>university</phrase>/<phrase>community</phrase> partnership to <phrase>bridge</phrase> the <phrase>digital divide</phrase>.
<phrase>data mining</phrase> and <phrase>mobile</phrase> <phrase>business</phrase> <phrase>data</phrase>.
virtual schools.
<phrase>wireless</phrase> technologies to enable <phrase>electronic</phrase> <phrase>business</phrase>.
developing dynamic balanced scorecards.
<phrase>music</phrase> score watermarking.
non-functional requirements and <phrase>uml</phrase> <phrase>stereotypes</phrase>.
<phrase>knowledge</phrase>-based support environment.
combining local and global expertise in services.
bibliomining for <phrase>library</phrase> decision-making.
defining and understanding <phrase>erp</phrase> systems.
<phrase>interoperability</phrase> of <phrase>information</phrase> systems.
survey of 3d <phrase>human</phrase> body representations.
classroom <phrase>communication</phrase> on a different <phrase>blackboard</phrase>.
implementing an online <phrase>academic</phrase> evaluation system.
delineating <phrase>knowledge</phrase> flows for enterprise agility.
the organizational context in the use of a <phrase>workflow</phrase> system.
critical trends in <phrase>telecommunications</phrase>.
social responsibility and the <phrase>technology</phrase> <phrase>paradigm</phrase> in <phrase>latin america</phrase>.
distributed recommender systems for <phrase>internet</phrase> <phrase>commerce</phrase>.
<phrase>model</phrase>-supported alignment of is <phrase>architecture</phrase>.
a primer on <phrase>e-government</phrase>.
enhancing workplaces with constructine online recreation.
digitization of <phrase>library</phrase> <phrase>information</phrase> and its accessibilty for people with disabilities.
progammed instruction, programmed branching, and learning outcomes.
<phrase>software</phrase> requirements <phrase>risk</phrase> and <phrase>maintainability</phrase>.
<phrase>human</phrase> body part classification and activity recognition for real-time systems.
functional dependency and other related dependancies.
<phrase>software</phrase> reuse in <phrase>hypermedia</phrase> applications.
<phrase>computing</phrase> <phrase>curriculum</phrase> analysis and development.
storage and <phrase>access control</phrase> issues for <phrase>xml</phrase> documents.
strategies of <phrase>e-commerce</phrase> <phrase>business</phrase> value optimization.
developing trust in virtual teams.
<phrase>software</phrase> agents in <phrase>e-commerce</phrase> systems.
new <phrase>sql</phrase> standard in <phrase>database</phrase> modeling.
leader-facilitated relationship building in virtual teams.
program execution and visualization on the web.
<phrase>programming</phrase> is a demanding task with an <phrase>education</phrase> program that requires the assistance of complex tools such as <phrase>programming</phrase> environments, <phrase>algorithm</phrase> animators, problem graders, etc. in this chapter, we give a <phrase>comprehensive</phrase> presentation of tools for program execution and visualization on the web. we summarize the technical <phrase>evolution</phrase> of these tools, describe educational uses, <phrase>report</phrase> lessons learned, and look at formal evaluations of their educational effectiveness. we also deal with a closely related <phrase>matter</phrase>, namely, collections of web documents containing <phrase>programming</phrase> exercises. finally, we outline our view of future trends in the use of the web for <phrase>programming</phrase> <phrase>education</phrase>, and we give our personal conclusions. this chapter is of interest to educators and researchers, because it gives a <phrase>comprehensive</phrase> presentation of the main issues and <phrase>results</phrase> of a field where most of the contributions are sparse in the <phrase>literature</phrase>.
metrics for <phrase>data warehouse</phrase> quality.
institutional dimensions of <phrase>information</phrase> systems evaluation.
it <phrase>productivity</phrase> impacts in <phrase>manufacturing</phrase> contexts.
obstacles for smes for <phrase>e</phrase>-<phrase>adoption</phrase> in the <phrase>asia pacific region</phrase>.
smes in <phrase>knowledge</phrase>-based economies.
end-user <phrase>computing</phrase> success measurement.
essentials of functional and <phrase>object-oriented</phrase> methodology.
students' perceptions of online courses.
<phrase>information</phrase> systems and <phrase>technology</phrase> in <phrase>south africa</phrase>.
departure of the <phrase>expert systems</phrase> project champion.
the <phrase>social contract</phrase> revised.
observations on implementing specializations within an it program.
<phrase>citizenship</phrase> and new technologies.
changing trends in the preparation of print <phrase>media</phrase>.
generic framework for defining <phrase>domain-specific</phrase> models.
<phrase>management</phrase> of <phrase>cognitive</phrase> and affective trust to support collaboration.
<phrase>life</phrase> cycle of <phrase>erp</phrase> systems.
liability for system and <phrase>data quality</phrase>.
uses and gratifications for the <phrase>world wide web</phrase>.
tailorable <phrase>information</phrase> systems.
best practices for effective virtual teams.
system development for <phrase>e</phrase>-<phrase>business</phrase>.
<phrase>public-key cryptography</phrase>.
component-oriented approach for designing <phrase>enterprise architecture</phrase>.
<phrase>digital asset management</phrase> concepts.
organizational <phrase>hypermedia</phrase> <phrase>document management</phrase> through <phrase>metadata</phrase>.
governance structures for it in the <phrase>health care</phrase> <phrase>industry</phrase>.
<phrase>histogram</phrase> generation from the <phrase>hsv</phrase> <phrase>color space</phrase>.
legal <phrase>expert systems</phrase> in administrative organizations.
<phrase>technology</phrase> in the foreign <phrase>language</phrase> classroom.
integration framework for <phrase>complex systems</phrase>.
personal <phrase>information privacy</phrase> and <phrase>internet</phrase> <phrase>technology</phrase>.
<phrase>cognitive</phrase> <phrase>research</phrase> in <phrase>information</phrase> systems.
<phrase>mobile</phrase> <phrase>telecommunications</phrase> and m-<phrase>commerce</phrase> applications.
<phrase>agent-based</phrase> <phrase>intelligence</phrase> <phrase>infrastructure</phrase>.
<phrase>erp</phrase> <phrase>adoption</phrase> by <phrase>indian</phrase> organizations.
<phrase>information</phrase> laws.
cross-<phrase>culture</phrase> <phrase>communication</phrase>.
bridging the growing <phrase>digital divide</phrase>.
<phrase>actor</phrase>-<phrase>network theory</phrase> in <phrase>information</phrase> systems <phrase>research</phrase>.
modelling <phrase>technological change</phrase> in small <phrase>business</phrase>.
<phrase>actor</phrase>-<phrase>network theory</phrase> and <phrase>adoption</phrase> of <phrase>e-commerce</phrase> in smes.
<phrase>ecological</phrase> models and <phrase>information</phrase> systems <phrase>curriculum</phrase>.
<phrase>knowledge</phrase> exchange in <phrase>electronic</phrase> networks of practice.
designing <phrase>hypertext</phrase> and the web.
<phrase>business</phrase> strategy, structure and it alignment.
building local capacity via scaleable <phrase>web-based</phrase> services.
querying multidimensional <phrase>data</phrase>.
a powerful and easy-to-use querying environment is certainly one of the most important components in a multidimensional <phrase>database</phrase>, and its effectiveness is influenced by many other aspects, both logical (<phrase>data model</phrase>, integration, policy of view materialization, etc.) and physical (multidimensional or <phrase>relational</phrase> storage, indexes, etc.). as is evident, multidimensional querying is often based on the <phrase>metaphor</phrase> of the <phrase>data</phrase> <phrase>cube</phrase> and on the concepts of facts, measures, and dimensions. in contrast to conventional transactional environments, multidimensional querying is often an exploratory process, performed by navigating along the dimensions and measures, increasing/decreasing the level of detail and focusing on specific subparts of the <phrase>cube</phrase> that appear to be "promising" for the required information.in this chapter we focus on the main languages proposed in the <phrase>literature</phrase> to express multidimensional queries, particularly those based on: (i) an <phrase>algebraic</phrase> approach, (<phrase>ii</phrase>) a declarative <phrase>paradigm</phrase> (<phrase>calculus</phrase>), and (iii) visual constructs and <phrase>syntax</phrase>. we analyze the problem of evaluation, i.e., the issues related to the efficient <phrase>data</phrase> retrieval and calculation, possibly (often necessarily) using some pre-computed <phrase>data</phrase>, a problem known in the <phrase>literature</phrase> as the problem of rewriting a query using views. we also illustrate the use of particular <phrase>index structures</phrase> to speed up the query evaluation process.
improving virtual teams through <phrase>creativity</phrase>.
leapfrogging an it sector.
qualitative methods in is <phrase>research</phrase>.
<phrase>e</phrase>-<phrase>business</phrase> systems <phrase>security</phrase> for intelligent enterprise.
implementing the shared event <phrase>paradigm</phrase>.
audience-driven web site <phrase>design</phrase>.
web tools for molecular biological <phrase>data analysis</phrase>.
monitoring strategies for <phrase>internet</phrase> technologies.
<phrase>digital literacy</phrase> and the position of the end-user.
<phrase>intranet</phrase> use and emergence of networks of practice.
metrics for the evaluation of <phrase>test</phrase> delivery systems.
face expression and motion analysis over <phrase>monocular</phrase> images.
<phrase>artificial neural networks</phrase> in financial trading.
<phrase>surveying</phrase> <phrase>mobile commerce</phrase> environments.
<phrase>content-based image retrieval</phrase> query paradigms.
analyzing the quality of virtual teams.
<phrase>bayesian analysis</phrase> of geographical variation in <phrase>disease</phrase> <phrase>risk</phrase>.
<phrase>simulation</phrase> in <phrase>information</phrase> systems <phrase>research</phrase>.
<phrase>cluster analysis</phrase> using rough clustering and <phrase>k-means clustering</phrase>.
networks and <phrase>electronic commerce</phrase> <phrase>adoption</phrase> in small businesses.
<phrase>information</phrase> and <phrase>communication</phrase> <phrase>technology</phrase> tools for <phrase>competitive intelligence</phrase>.
supporting the evaluation of intelligent sources.
<phrase>data</phrase> dissemination in <phrase>mobile</phrase> <phrase>databases</phrase>.
trust in b2c <phrase>e-commerce</phrase> interface.
<phrase>gis</phrase>-based accessibility measures and application.
it <phrase>industry</phrase> success in <phrase>finland</phrase> and new <phrase>zealand</phrase>.
enhanced <phrase>knowledge</phrase> <phrase>warehouse</phrase>.
learning 3d face <phrase>animation</phrase> <phrase>model</phrase>.
<phrase>e</phrase>-governement <phrase>interoperability</phrase>.
<phrase>data</phrase> collection methodologies for <phrase>web-based</phrase> experiments.
<phrase>ethical</phrase> implications of investigating <phrase>internet</phrase> relationships.
"anytime, anywhere" in the context of <phrase>mobile</phrase> work.
<phrase>knowledge</phrase> discovery solutions for intelligent enterprises.
evaluating is quality as a measure of is effectiveness.
<phrase>e</phrase>-<phrase>mail</phrase> and <phrase>communication</phrase>.
it implementation in small <phrase>business</phrase>.
trust in virtual enterprises.
designing <phrase>high</phrase> performance virtual teams.
improving <phrase>public sector</phrase> service delivery through <phrase>knowledge</phrase> sharing.
traversal pattern <phrase>mining</phrase> in web usage <phrase>data</phrase>.
applying a <phrase>metadata</phrase> framework to improve <phrase>data quality</phrase>.
<phrase>java</phrase> 2 micro edition for <phrase>wireless</phrase> enterprose.
distributed <phrase>construction</phrase> through <phrase>participatory design</phrase>.
tactic <phrase>knowledge</phrase> and <phrase>discourse analysis</phrase>.
advanced techniques for object-based image retrieval.
<phrase>knowledge management</phrase> on the web.
<phrase>neural networks</phrase> for <phrase>retail</phrase> sales forecasting.
new advancements in <phrase>image segmentation</phrase> for cbir.
the impact of it on <phrase>business</phrase> partnerships and organizational structures.
<phrase>open source software</phrase> development <phrase>model</phrase>.
<phrase>fault tolerance</phrase> for distributed and networked systems.
isochronus distributed <phrase>multimedia</phrase> synchronization.
integrated-services <phrase>architecture</phrase> for building <phrase>internet</phrase> <phrase>multimedia</phrase> applications.
<phrase>knowledge</phrase> discovery from <phrase>databases</phrase>.
discovering association rules in temporal <phrase>databases</phrase>.
kernelized <phrase>database systems</phrase> <phrase>security</phrase>.
collective <phrase>knowledge</phrase> composition in a <phrase>p2p</phrase> network.
<phrase>e-government</phrase> <phrase>databases</phrase>.
converting a legacy <phrase>database</phrase> to <phrase>object-oriented</phrase> <phrase>database</phrase>.
<phrase>e</phrase>-<phrase>mail</phrase> <phrase>data</phrase> stores.
proper placement of derived classes in the class hierarchy.
replication mechanisms over a set of distributed uddi registries.
<phrase>logic</phrase> <phrase>databases</phrase> and inconsistency handling.
<phrase>high</phrase> quality conceptual schemes.
a <phrase>rhetorical</phrase> perspective on localization and international <phrase>outsourcing</phrase>.
hierarchical <phrase>architecture</phrase> of <phrase>expert systems</phrase> for <phrase>database management</phrase>.
<phrase>ontological</phrase> assumptions in <phrase>information</phrase> modeling.
<phrase>intension</phrase> <phrase>mining</phrase>.
integrative document and <phrase>content management</phrase> <phrase>systems architecture</phrase>.
dataveillance and panoptic marketspaces.
advanced <phrase>query optimization</phrase>.
main <phrase>memory</phrase> <phrase>databases</phrase>.
<phrase>business</phrase> rules in <phrase>databases</phrase>.
<phrase>database</phrase> replication protocols.
<phrase>data</phrase> warehouses.
text <phrase>databases</phrase>.
<phrase>relational</phrase>, <phrase>object-oriented</phrase> and <phrase>object-relational</phrase> <phrase>data</phrase> models.
online <phrase>data mining</phrase>.
extended entity relationship modeling.
symbolic objects and symbolic <phrase>data analysis</phrase>.
integration of <phrase>data</phrase> <phrase>semantics</phrase> in heterogeneous <phrase>database</phrase> federations.
query operators in temporal <phrase>xml</phrase> <phrase>databases</phrase>.
<phrase>raster</phrase> <phrase>databases</phrase>.
since the launch of <phrase>google earth</phrase> at the latest it is clear that online services for multi-<phrase>terabyte</phrase> <phrase>satellite imagery</phrase> are becoming <phrase>integral</phrase> part of our <phrase>internet</phrase> experience. actually, 2-d imagery is but the tip of the <phrase>iceberg</phrase> - the <phrase>general</phrase> concept of multi-dimensional spatio-temporal <phrase>raster</phrase> <phrase>data</phrase> covers 1-d <phrase>sensor</phrase> time series, 2-d imagery, 3-d image time series (x/y/t) and exploration <phrase>data</phrase> (x/y/z), 4-d <phrase>climate</phrase> models (x/y/z/t), and many more.
ensuring <phrase>serializability</phrase> for <phrase>mobile</phrase>-client <phrase>data</phrase> caching.
<phrase>databases</phrase> for <phrase>mobile</phrase> applications.
rough sets.
<phrase>data model</phrase> versioning and <phrase>database</phrase> <phrase>evolution</phrase>.
vertical <phrase>database design</phrase> for scalable <phrase>data mining</phrase>.
<phrase>ontology</phrase>-based <phrase>data integration</phrase>.
<phrase>database</phrase> support for <phrase>workflow</phrase> <phrase>management</phrase> systems.
real-time <phrase>databases</phrase>.
object modeling of <phrase>rdbms</phrase> based applications.
<phrase>business-to-business</phrase> integration.
<phrase>knowledge</phrase> <phrase>mining</phrase>.
<phrase>enterprise application integration</phrase>.
<phrase>moving objects</phrase> <phrase>databases</phrase>.
<phrase>database</phrase> <phrase>engineering</phrase> focusing on modern dynamism crises.
<phrase>multimedia</phrase> <phrase>databases</phrase>.
using views to query <phrase>xml</phrase> documents.
<phrase>open source software</phrase> and <phrase>information</phrase> systems on the web.
set valued attributes.
normalizing <phrase>multimedia</phrase> <phrase>databases</phrase>.
consistency in spatial <phrase>databases</phrase>.
metric <phrase>databases</phrase>.
storing <phrase>xml</phrase> documents in <phrase>databases</phrase>.
path-oriented queries and <phrase>tree</phrase> inclusion problems.
<phrase>ubiquitous computing</phrase> and <phrase>databases</phrase>.
signature files and signature file <phrase>construction</phrase>.
<phrase>semantic</phrase> <phrase>information management</phrase>.
sensors, uncertainty models, and probabilistic queries.
<phrase>data warehousing</phrase> and <phrase>olap</phrase>.
the <phrase>information</phrase> quality of <phrase>databases</phrase>.
text categorization.
active <phrase>database management systems</phrase>.
querical <phrase>data</phrase> networks.
<phrase>query processing</phrase> in spatial <phrase>databases</phrase>.
extraction-transformation-loading processes.
<phrase>free software</phrase> and <phrase>open source</phrase> <phrase>databases</phrase>.
<phrase>open source</phrase> <phrase>database management systems</phrase>.
set comparison in <phrase>relational</phrase> query languages.
biological <phrase>data mining</phrase>.
deriving spatial <phrase>integrity constraints</phrase> from geographic application schemas.
<phrase>query processing</phrase> for <phrase>rdf</phrase> <phrase>data</phrase>.
<phrase>data warehousing</phrase>, multi-dimensional <phrase>data</phrase> models, and <phrase>olap</phrase>.
modeling and querying temporal <phrase>data</phrase>.
temporal <phrase>databases</phrase>.
<phrase>benchmarking</phrase> and <phrase>data</phrase> generation in <phrase>moving objects</phrase> <phrase>databases</phrase>.
<phrase>mathematics</phrase> of generic specifications for <phrase>model</phrase> <phrase>management</phrase>, i.
spatio-temporal <phrase>indexing techniques</phrase>.
<phrase>mathematics</phrase> of generic specifications for <phrase>model</phrase> <phrase>management</phrase>, <phrase>ii</phrase>.
active federated <phrase>database systems</phrase>.
generic <phrase>model</phrase> <phrase>management</phrase>.
<phrase>digital media</phrase> warehouses.
semantically modeled enterprise <phrase>databases</phrase>.
biometric <phrase>databases</phrase>.
<phrase>knowledge</phrase> discovery and geographical <phrase>databases</phrase>.
multiparticipant decision making and <phrase>balanced scorecard</phrase> collaborative.
<phrase>semantic</phrase> enrichment of geographical <phrase>databases</phrase>.
<phrase>knowledge management</phrase> in <phrase>tourism</phrase>.
<phrase>syntactical</phrase> and semantical correctness of pictorial queries for <phrase>gis</phrase>.
<phrase>ontologies</phrase> and their practical implementation.
<phrase>data</phrase> dissemination.
using <phrase>semantic web</phrase> tools for <phrase>ontologies</phrase> <phrase>construction</phrase>.
repairing inconsistent <phrase>xml</phrase> <phrase>data</phrase> with functional dependencies.
geometric quality in geographic <phrase>information</phrase>.
managing inconsistent <phrase>databases</phrase> using active <phrase>integrity constraints</phrase>.
<phrase>object-relational</phrase> modeling in the <phrase>uml</phrase>.
replication methods and their properties.
service mechanism quality for enhanced <phrase>mobile</phrase> <phrase>multimedia</phrase> <phrase>database</phrase> <phrase>query processing</phrase>.
transaction concurrency methods.
common <phrase>information</phrase> <phrase>model</phrase>.
document versioning in <phrase>digital libraries</phrase>.
multilevel <phrase>databases</phrase>.
<phrase>component-based</phrase> generalized <phrase>database</phrase> index <phrase>model</phrase>.
an <phrase>xml</phrase> multi-tier pattern dissemination system.
<phrase>bioinformatics</phrase> <phrase>data management</phrase> and <phrase>data mining</phrase>.
<phrase>natural language</phrase> front-end for a <phrase>database</phrase>.
applying <phrase>database</phrase> techniques to the <phrase>semantic web</phrase>.
preferred repairs for inconsistent <phrase>databases</phrase>.
rewriting and efficient computation of bound disjunctive <phrase>datalog</phrase> queries.
rewriting and efficient computation of bound disjunctive <phrase>datalog</phrase> queries.
transformation-based <phrase>database</phrase> <phrase>engineering</phrase>.
case tools for <phrase>database</phrase> <phrase>engineering</phrase>.
checking <phrase>integrity constraints</phrase> in a <phrase>distributed database</phrase>.
optimization of continual queries.
<phrase>security</phrase> controls for <phrase>database</phrase> <phrase>technology</phrase> and applications.
similarity search in time series <phrase>databases</phrase>.
<phrase>database</phrase> query <phrase>personalization</phrase>.
<phrase>data quality</phrase> assessment.
a development environment for customer-oriented web <phrase>business</phrase>.
transactional support for <phrase>mobile</phrase> <phrase>databases</phrase>.
<phrase>engineering</phrase> <phrase>information</phrase> modeling in <phrase>databases</phrase>.
fuzzy <phrase>database</phrase> modeling.
semistructured <phrase>data</phrase> and its conceptual models.
<phrase>management</phrase> of large <phrase>moving objects</phrase> datasets: indexing, <phrase>benchmarking</phrase> and uncertainty in movement representation.
approximate computation of distance-based queries.
spatiotemporal prediction using <phrase>data mining</phrase> tools.
<phrase>object-relational</phrase> spatial indexing.
integrating web <phrase>data</phrase> and geographic <phrase>knowledge</phrase> into spatial <phrase>databases</phrase>.
spatial joins: <phrase>algorithms</phrase>, cost models and optimization techniques.
<phrase>quadtree</phrase>-based image representation and retrieval.
similarity learning in <phrase>gis</phrase>: an overview of definitions, prerequisites and challenges.
survey on spatial <phrase>data</phrase> modelling approaches.
indexing multi-dimensional trajectories for similarity queries.
<phrase>mining</phrase> in spatiotemporal <phrase>databases</phrase>.
applications of <phrase>moving objects</phrase> <phrase>databases</phrase>.
simple and incremental <phrase>nearest-neighbor search</phrase> for spatiotemporal <phrase>databases</phrase>.
source integration for <phrase>data warehousing</phrase>.
while the main goal of a <phrase>data warehouse</phrase> is to provide support for <phrase>data analysis</phrase> and management's decisions, a fundamental <phrase>aspect</phrase> in <phrase>design</phrase> of a <phrase>data warehouse</phrase> system is the process of acquiring the raw <phrase>data</phrase> from a set of relevant <phrase>information</phrase> sources. we will call source integration system the component of a <phrase>data warehouse</phrase> system dealing with this process. the main goal of a source integration system is to deal with the transfer of <phrase>data</phrase> from the set of sources constituting the application-oriented operational environment, to the <phrase>data warehouse</phrase>. since sources are typically autonomous, distributed, and heterogeneous, this task has to deal with the problem of cleaning, reconciling, and integrating <phrase>data</phrase> coming from the sources. the <phrase>design</phrase> of a source integration system is a very complex task, which comprises several different issues. the purpose of this chapter is to discuss the most important problems arising in the <phrase>design</phrase> of a source integration system, with special emphasis on schema integration, processing queries for <phrase>data integration</phrase>, and <phrase>data</phrase> cleaning and reconciliation.
<phrase>incomplete information</phrase> in <phrase>multidimensional databases</phrase>.
while <phrase>incomplete information</phrase> is <phrase>endemic</phrase> to <phrase>real-world</phrase> <phrase>data</phrase>, current multidimensional <phrase>data</phrase> models are not engineered to manage <phrase>incomplete information</phrase> in base <phrase>data</phrase>, derived <phrase>data</phrase>, and dimensions. this chapter presents several strategies for managing <phrase>incomplete information</phrase> in <phrase>multidimensional databases</phrase>. which strategy to use is dependent on the kind of <phrase>incomplete information</phrase> present, and also on where it occurs in the multidimensional <phrase>database</phrase>. a relatively simple strategy is to replace <phrase>incomplete information</phrase> with appropriate, complete <phrase>information</phrase>. the advantage of this strategy is that all <phrase>multidimensional databases</phrase> can manage complete <phrase>information</phrase>. other strategies require more substantial changes to the multidimensional <phrase>database</phrase>. one strategy is to reflect the incompleteness in computed aggregates, which is possible only if the multidimensional <phrase>database</phrase> allows incomplete values in its hierarchies. another strategy is to measure the amount of incompleteness in aggregated values by tallying how much uncertain <phrase>information</phrase> went into their <phrase>production</phrase>.
<phrase>privacy</phrase> in <phrase>multidimensional databases</phrase>.
when answering queries that ask for, summary <phrase>statistics</phrase>, the query-system of a multidimensional <phrase>database</phrase> should <phrase>guard</phrase> confidential <phrase>data</phrase>, that is, it should avoid revealing (directly or indirectly) individual <phrase>data</phrase>, which could be exactly calculated or accurately estimated from the values of answered queries. in <phrase>order</phrase> to prevent the disclosure of confidential <phrase>data</phrase>, the query-system should be provided with an auditing procedure which, each time a new query is processed, checks that its answer does not allow a (knowledgeable) user to disclose any sensitive <phrase>data</phrase>. a promising approach consists in keeping <phrase>track</phrase> of (or auditing) answered queries by means a dynamic <phrase>graphical</phrase> <phrase>data structure</phrase>, here called the answer map, whose size increases with the number of answered queries and with the number of dimensions of the <phrase>database</phrase>, so that the problem of the existence of an efficient auditing procedure naturally arises. this chapter reviews recent <phrase>results</phrase> on this problem for "additive" queries (such as <phrase>count</phrase> and sum queries) by listing some polynomially solvable problems as well as some hard problems, and suggests directions for future work.
time in <phrase>multidimensional databases</phrase>.
in spite of the obvious importance of time in <phrase>data warehousing</phrase> and <phrase>olap</phrase>, current commercial systems do not support tracking the <phrase>history</phrase> of a <phrase>data warehouse</phrase>, either at the schema or instance level. in this chapter we address this issue, introducing the temporal multidimensional <phrase>model</phrase> and a <phrase>query language</phrase>, denoted tolap, allowing expressing temporal <phrase>olap</phrase> queries at a <phrase>high</phrase> level of abstraction. further, we show that previous work in temporal <phrase>databases</phrase> needs to be extended in <phrase>order</phrase> to handle <phrase>evolution</phrase> and versioning in <phrase>olap</phrase>. finally, we present an implementation, along with preliminary <phrase>experimental</phrase> <phrase>results</phrase>.
materialized viewsin <phrase>multidimensional databases</phrase>.
cooperation with geographic <phrase>databases</phrase>.
the purpose of this chapter is to create cooperation between geographic <phrase>databases</phrase> (gdbs) and <phrase>multidimensional databases</phrase> (mddbs), which are considered as the most promising and efficient <phrase>information</phrase> technologies for supporting decision making. we focus on the common key elements between geographic and multidimensional <phrase>data</phrase> which allow effective support in <phrase>data</phrase> cooperating. these elements are basically time and space, which are present implicitly or explicitly in mddb and are modeled on the dimensions, time and location. thus, because gdbs are primarily concerned with geographic <phrase>data</phrase>, we will focus on space as a <phrase>bridge</phrase> element for cooperating mddbs and gdbs. we propose an approach that extends the geographic <phrase>data structure</phrase> through special attributes, called binding attributes, in <phrase>order</phrase> to describe all phenomena represented by mddbs. this extension will make it possible to answer more specific "<phrase>olap</phrase>-based" queries within gdbs without modifying the physical <phrase>organization</phrase> of <phrase>data</phrase> in both environments.
hierarchies.
in this chapter we will focus on the rules of aggregation hierarchies in analysis dimensions of a <phrase>cube</phrase>. we give an overview of the related works on the <phrase>basic</phrase> concepts of the different types of aggregation hierarchies. we then discuss the hierarchies from two different points of view: mapping between domain values and hierarchical structures. in relation to them, we introduce the characterization of some <phrase>olap</phrase> operators on hierarchies and give a set of operators that concern the change in the hierarchy structure. finally, we propose an enlargement of the operator set concerning hierarchies.
<phrase>basic</phrase> notions.
this chapter presents the <phrase>basic</phrase> notions regarding multidimensional (aggregate) <phrase>databases</phrase> by referring to different definitions given for them in the <phrase>literature</phrase>. it illustrates the important concepts of micro, macro, and <phrase>metadata</phrase>; presents a formal definition of the aggregation process, discussing the concepts of <phrase>dimension</phrase> and <phrase>dimension</phrase> hierarchies; describes the multidimensional <phrase>aggregate data</phrase> structure, distinguishing between simple, complex, and composite structure; illustrates the different types of null values; and discusses differences and similarities which exist between multidimensional <phrase>aggregate data</phrase> (generally called statistical <phrase>data</phrase> because they are used mainly by statisticians) and the on-line-analytic processing (<phrase>olap</phrase>) of multidimensional <phrase>data</phrase> represented by different <phrase>data</phrase> cubes, also discussing the different (symmetric and nonsymmetric) treatment of dimensions and measures required by <phrase>olap</phrase> and aggregate <phrase>multidimensional databases</phrase>. finally it discusses a <phrase>graph</phrase> <phrase>model</phrase> and a tabular <phrase>model</phrase> for this kind of <phrase>data</phrase>, and gives a set of definitions regarding the <phrase>olap</phrase> terminology.
operators for multidimensional <phrase>aggregate data</phrase>.
in this chapter the <phrase>author</phrase> proposes the different approaches for defining operators able to manipulate this multidimensional structure. in particular, he initially considers operators for multidimensional <phrase>aggregate data</phrase> which extend <phrase>relational algebra</phrase> and <phrase>relational</phrase> <phrase>calculus</phrase> (the so-called enlarged <phrase>relational model</phrase>). then he discusses operators for multidimensional <phrase>aggregate data</phrase> defined in a tabular environment. in both the cases the <phrase>author</phrase> defines such <phrase>data</phrase> as statistical (aggregate) <phrase>data</phrase>. subsequently he introduces the operators for <phrase>olap</phrase> applications, giving a terminology correspondence between the multidimensional aggregate (statistical) <phrase>databases</phrase> and <phrase>olap</phrase> areas. then he defines the fundamental operators deduced from the previous ones, which form the <phrase>basic</phrase> <phrase>algebra</phrase> for the manipulation of multidimensional <phrase>aggregate data</phrase>, giving their formal definitions and some explanatory examples.
dynamic multidimensional <phrase>data</phrase> cubes.
<phrase>data</phrase> cubes are ubiquitous tools in <phrase>data warehousing</phrase>, <phrase>online analytical processing</phrase>, and <phrase>decision support</phrase> applications. based on a selection of pre-computed and materialized aggregate values, they can dramatically speed up aggregation and summarization over large <phrase>data</phrase> collections. traditionally, the emphasis has been on lowering query costs with little regard to maintenance, i.e., update cost issues. we argue that current trends require <phrase>data</phrase> cubes to be not only query-efficient, but also dynamic at the same time, and we also show how this can be achieved. several array-based techniques with different tradeoffs between query and update cost are discussed in detail. we also survey selected approaches for sparse <phrase>data</phrase> and the popular <phrase>data</phrase> <phrase>cube</phrase> operator, <phrase>cube</phrase>. moreover, this work includes an overview of future trends and their impact on <phrase>data</phrase> cubes.
multidimensionality in statistical, <phrase>olap</phrase>, and scientific <phrase>databases</phrase>.
the term "multidimensional databses" refers to <phrase>data</phrase> that can be viewed conceptually in a multidimensional space, where each <phrase>dimension</phrase> represents some attributes of the <phrase>data</phrase>. viewing <phrase>data</phrase> in this form is natural for many applications, yet the concepts are not treated in a uniform way in the <phrase>database</phrase> <phrase>literature</phrase>. in this chapter, we show the commonality of concepts between three <phrase>database</phrase> areas: statistical, <phrase>olap</phrase>, and scientific <phrase>databases</phrase>. we show that these domains have two main structural concepts: the <phrase>cross-product</phrase> space of the dimensions, and the classification hierarchy structure associated with each <phrase>dimension</phrase>. in the first part of this chapter we describe how these structures are <phrase>sed</phrase> to represent <phrase>data</phrase> in statistical and <phrase>olap</phrase> <phrase>databases</phrase> and how summarization operators can be applied to them. further, we discuss how these structures can be extended to represent related <phrase>information</phrase> using federated <phrase>database</phrase> concepts. in the second part of the chapter we show that these concepts are common to many scientific <phrase>database</phrase> application. in particular, we discuss the importance of supporting classification structures and the difficulty in representing them as tables in <phrase>relational databases</phrase>. we also discuss <phrase>data structures</phrase> to support <phrase>multidimensional databases</phrase>, emphasizing space-time representation, clustering in multidimensional space, indexing in multidimensional space, and supporting classification structures. we conclude by arguing that the concepts of multidimensionality and classification structures as well as the operation over them should be elevated to "first class" object types. these object types should be visible by the application user explicitly in the conceptual schemas as well as exposing them in the <phrase>user interfaces</phrase>.
querying multidimensional <phrase>data</phrase>.
a powerful and easy-to-use querying environment is certainly one of the most important components in a multidimensional <phrase>database</phrase>, and its effectiveness is influenced by many other aspects, both logical (<phrase>data model</phrase>, integration, policy of view materialization, etc.) and physical (multidimensional or <phrase>relational</phrase> storage, indexes, etc.). as is evident, multidimensional querying is often based on the <phrase>metaphor</phrase> of the <phrase>data</phrase> <phrase>cube</phrase> and on the concepts of facts, measures, and dimensions. in contrast to conventional transactional environments, multidimensional querying is often an exploratory process, performed by navigating along the dimensions and measures, increasing/decreasing the level of detail and focusing on specific subparts of the <phrase>cube</phrase> that appear to be "promising" for the required information.in this chapter we focus on the main languages proposed in the <phrase>literature</phrase> to express multidimensional queries, particularly those based on: (i) an <phrase>algebraic</phrase> approach, (<phrase>ii</phrase>) a declarative <phrase>paradigm</phrase> (<phrase>calculus</phrase>), and (iii) visual constructs and <phrase>syntax</phrase>. we analyze the problem of evaluation, i.e., the issues related to the efficient <phrase>data</phrase> retrieval and calculation, possibly (often necessarily) using some pre-computed <phrase>data</phrase>, a problem known in the <phrase>literature</phrase> as the problem of rewriting a query using views. we also illustrate the use of particular <phrase>index structures</phrase> to speed up the query evaluation process.
conceptual multidimensional models.
a <phrase>variety</phrase> of multidimensional <phrase>data</phrase> models have recently been proposed by both <phrase>academic</phrase> and <phrase>industry</phrase> communities. but consensus on formalism or even a common terminology has not yet emerged. in this chapter, we first discuss the requirements that an ideal conceptual multidimensional <phrase>model</phrase> should fulfill. these requirements are suggested by <phrase>general</phrase> <phrase>information</phrase> system modeling principles and the specific characteristics of <phrase>olap</phrase> applications. building on these requirements, we then present a <phrase>general</phrase> conceptual multidimensional <phrase>data model</phrase> and show how it can be used to describe the <phrase>basic</phrase> aspects of a <phrase>business</phrase> application in a way that is easy to understand and <phrase>independent</phrase> of the criteria for actual <phrase>data</phrase> <phrase>organization</phrase> in the various systems. starting from the characteristics of the <phrase>model</phrase> proposed, we summarize the <phrase>general</phrase> features that a multidimensional <phrase>conceptual model</phrase> should support. we then survey various multidimensional models proposed and relate their characteristics to these <phrase>general</phrase> features. finally, we discuss the main points raised in the chapter and some problems that remain to be solved in this context.
towards an autopoietic approach for <phrase>information</phrase> systems development.
<phrase>business</phrase> <phrase>action</phrase> and <phrase>information</phrase> modeling - the task of the next <phrase>millennium</phrase>.
event modeling.
<phrase>information</phrase> system <phrase>design</phrase> based on reuse of conceptual components.
an environment for managing enterprise domain <phrase>ontology</phrase>.
spatial and <phrase>topological</phrase> <phrase>data</phrase> models.
coherent, consistent, and <phrase>comprehensive</phrase> modeling of <phrase>communication</phrase>, <phrase>information</phrase>, <phrase>action</phrase>, and <phrase>organization</phrase>.
a unifying <phrase>translation</phrase> of <phrase>natural language</phrase> patterns to object and process modeling.
an <phrase>information management</phrase> environment based on the <phrase>model</phrase> of object primitives.
designing <phrase>model</phrase>-based intelligent dialogue systems.
conceptual modeling process and the notion of a concept.
integrating fact-oriented modeling with <phrase>object-oriented</phrase> modeling.
a <phrase>language</phrase>/<phrase>action</phrase> <phrase>based approach</phrase> to <phrase>information</phrase> modeling.
from <phrase>information</phrase> <phrase>model</phrase> to controllable implementation.
modeling of customers' interactive control of service processes.
on the convergence of <phrase>analysis and design</phrase> methods for <phrase>multi-agent</phrase>, <phrase>component-based</phrase> and <phrase>object-oriented</phrase> systems.
requirements for web <phrase>engineering</phrase> methodologies.
a <phrase>genre</phrase>-based method for <phrase>information</phrase> systems planning.
metrics for managing quality in <phrase>information</phrase> modeling.
preface.
<phrase>object-oriented</phrase> <phrase>web applications</phrase> modeling.
<phrase>information</phrase> modeling in the <phrase>internet</phrase> age - challenges, issues, and <phrase>research</phrase> directions.
conceptual web site modeling.
<phrase>information</phrase> models for document <phrase>engineering</phrase>.
audience-driven <phrase>web design</phrase>.
a systematic relationship analysis for modeling <phrase>information</phrase> domains.
hmt: modeling interactive and adaptive <phrase>hypermedia</phrase> applications.
mapping <phrase>uml</phrase> techniques to <phrase>design</phrase> activities.
seamless formalizing the <phrase>uml</phrase> <phrase>semantics</phrase> through metamodels.
the whole-part relationship in the <phrase>unified modeling language</phrase>: a new approach.
temporal ocl meeting specification demands for <phrase>business</phrase> components.
a systematic approach to transform <phrase>uml</phrase> static models to <phrase>object-oriented</phrase> code.
an interactive viewpoint on the role of <phrase>uml</phrase>.
supplementing <phrase>uml</phrase> with concepts from <phrase>orm</phrase>.
systematic <phrase>design</phrase> of <phrase>web applications</phrase> with <phrase>uml</phrase>.
rup - a process <phrase>model</phrase> for working with <phrase>uml</phrase>.
extension of the <phrase>unified modeling language</phrase> for <phrase>mobile</phrase> agents.
using a <phrase>semiotic</phrase> framework to evaluate <phrase>uml</phrase> for the development of models of <phrase>high</phrase> quality.
rendering <phrase>distributed systems</phrase> in <phrase>uml</phrase>.
linking <phrase>uml</phrase> with integrated formal techniques.
<phrase>data modeling</phrase> and <phrase>uml</phrase>.
<phrase>rational unified process</phrase> and <phrase>unified modeling language</phrase> - a goms analysis.
preface.
<phrase>uml</phrase> modeling support for early reuse decisions in <phrase>component-based</phrase> development.
modeling of <phrase>business</phrase> rules for active <phrase>database</phrase> application specification.
active <phrase>database</phrase> applications require the classic cycle of analysis, <phrase>design</phrase>, prototyping and implementation. during <phrase>analysis and design</phrase> steps of the <phrase>information</phrase> system <phrase>engineering</phrase> process, modeling behavior is an important task. this task is both essential and crucial when <phrase>information</phrase> system is centered on active <phrase>databases</phrase>, which allow the replacement of parts of application programs with active rules. for that reason, the specification of <phrase>business</phrase> rules during <phrase>analysis and design</phrase> steps becomes an actual requirement. <phrase>business</phrase> rules ensure the well-functioning of <phrase>information</phrase> system. they are descriptive (<phrase>integrity constraints</phrase>) or functional (derivation rules and active rules). to relieve programmers from using either traditional or <phrase>ad hoc</phrase> techniques to <phrase>design</phrase> active <phrase>databases</phrase>, it is necessary to develop new techniques to <phrase>model</phrase> <phrase>business</phrase> rules. these techniques have to enhance the specification of dynamic <phrase>aspect</phrase> through a <phrase>high</phrase>-level description <phrase>language</phrase> able to express precisely and completely rule <phrase>semantic</phrase>. in this chapter, we propose a uniform approach to <phrase>model</phrase> <phrase>business</phrase> rules (active rules, <phrase>integrity constraints</phrase>, etc.). to improve the behavior specification we extend the <phrase>state</phrase> diagrams that are widely used for dynamic modeling. this extension is a transformation of <phrase>state</phrase> transitions according to rule <phrase>semantics</phrase>. in addition, we outline new functionalities of computer-aided system <phrase>engineering</phrase> (case) to take into consideration the active <phrase>database</phrase> specificities. in this way, the designer can be assisted to control, maintain and reuse a set of rules.
<phrase>cmu</phrase>-web: a <phrase>conceptual model</phrase> with metrics for testing and designing <phrase>usability</phrase> in <phrase>web applications</phrase>.
with the ubiquitous availability of <phrase>browsers</phrase> and <phrase>internet access</phrase>, the last few years have seen a tremendous growth in the number of applications being developed on the <phrase>world wide web</phrase> (www). models for analyzing and designing these applications are only just beginning to emerge. in this work, we propose a <phrase>three-dimensional</phrase> classification space for www applications, consisting of a <phrase>degree</phrase> of structure of pages <phrase>dimension</phrase>, a <phrase>degree</phrase> of support for interrelated events <phrase>dimension</phrase> and a location of processing <phrase>dimension</phrase>. next, we propose <phrase>usability</phrase> <phrase>design</phrase> metrics for www applications along the structure of pages <phrase>dimension</phrase>. to measure these, we propose <phrase>cmu</phrase>-web-a <phrase>conceptual model</phrase> that can be used to <phrase>design</phrase> www applications, such that its schema provide values for the <phrase>design</phrase> metrics. this work represents the first effort, to the best of our <phrase>knowledge</phrase>, to provide a <phrase>conceptual model</phrase> that measures quantifiable metrics that can be used for the <phrase>design</phrase> of more usable <phrase>web applications</phrase>, and that can also be used to compare the <phrase>usability</phrase> of existing <phrase>web applications</phrase>, without empirical testing.
enforcing <phrase>cardinality</phrase> constraints in the <phrase>er</phrase> <phrase>model</phrase> with integrity methods.
entity-relationship (<phrase>er</phrase>) schemas include <phrase>cardinality</phrase> constraints that restrict the dependencies among entities within a relationship type. the <phrase>cardinality</phrase> constraints have direct impact on application transactions, since insertions or deletions of entities or relationships might affect related entities. application transactions can be strengthened to preserve the consistency of a <phrase>database</phrase> with respect to the <phrase>cardinality</phrase> constraints in a schema. yet, once an <phrase>er</phrase> schema is translated into a logical <phrase>database schema</phrase>, the direct correlation between the <phrase>cardinality</phrase> constraints and application transaction is <phrase>lost</phrase>, since the components of the <phrase>er</phrase> schema might be decomposed among those of the logical <phrase>database</phrase> schema.we suggest extending the enhanced-<phrase>er</phrase> (eer) <phrase>data model</phrase> with integrity methods that can enforce the <phrase>cardinality</phrase> constraints. the integrity methods can be fully defined by the <phrase>cardinality</phrase> constraints, using a small number of primitive update methods, and are automatically created for a given eer diagram. a <phrase>translation</phrase> of an eer schema into a logical <phrase>database schema</phrase> can create integrity routines by translating the primitive update methods alone. these integrity routines may be implemented as <phrase>database</phrase> procedures, if a <phrase>relational</phrase> <phrase>dbms</phrase> is utilized, or as class methods, if an <phrase>object-oriented</phrase> <phrase>dbms</phrase> is utilized.
<phrase>algorithm</phrase> development, <phrase>simulation</phrase> analysis, and parametric studies for <phrase>data</phrase> allocation in <phrase>distributed database</phrase> systems.
in a <phrase>distributed database</phrase> system, an increase in workload typically necessitates the installation of additional <phrase>database</phrase> servers followed by the implementation of expensive <phrase>data</phrase> reorganization strategies. we present the partial reallocate and full reallocate heuristics for efficient <phrase>data</phrase> reallocation. complexity is controlled and cost minimized by allowing only incremental introduction of servers into the <phrase>distributed database</phrase> system. using first simple examples and then, a simulator, our framework for incremental growth and <phrase>data</phrase> reallocation in <phrase>distributed database</phrase> systems is shown to produce near optimal solutions when compared with exhaustive methods.
<phrase>object-oriented</phrase> <phrase>database</phrase> benchmarks.
the role of use cases in the <phrase>uml</phrase>: a review and <phrase>research</phrase> agenda.
a use case is a description of a <phrase>sequence</phrase> of actions constituting a complete task or transaction in an application. use cases were first proposed by jacobson (1987) and have since been incorporated as one of the key modeling constructs in the <phrase>uml</phrase>(booch, jacobson, & rumbaugh, 1999) and the unified <phrase>software development process</phrase>(jacobson, booch, & rumbaugh, 1999). this chapter traces the development of use cases, and identifies a number of problems with both their application and theoretical underpinnings. from an application perspective, the use-case concept is marked by a <phrase>high</phrase> <phrase>degree</phrase> of <phrase>variety</phrase> in the level of abstraction versus implementation detail advocated by various authors. in addition, use cases are promoted as a primary mechanism for identifying objects in an application, even though they focus on processes rather than objects. moreover, there is an apparent inconsistency between the so-called naturalness of object models and the commonly held view that use cases should be the primary means of communicating and verifying requirements with users. from a theoretical standpoint, the introduction of implementation issues in use cases can be seen as prematurely anchoring the analysis to particular implementation decisions. in addition, the fragmentation of objects across use cases creates conceptual difficulties in developing a <phrase>comprehensive</phrase> <phrase>class diagram</phrase> from a set of use cases. moreover, the role of categorization in <phrase>human</phrase> thinking suggests that class diagrams may serve directly as a good mechanism for communicating and verifying application requirements with users. we conclude by outlining a framework for further <phrase>empirical research</phrase> to resolve issues raised in our analysis.
object-process methodology applied to modeling <phrase>credit card</phrase> transactions.
object-process methodology (opm) is a system development and specification approach that combines the <phrase>major</phrase> system aspects-<phrase>function</phrase>, structure and behavior-within a <phrase>single</phrase> <phrase>graphic</phrase> and textual <phrase>model</phrase>. having applied opm in a <phrase>variety</phrase> of domains, this chapter specifies an <phrase>electronic commerce</phrase> system in a hierarchical manner, at the top of which are the processes of managing a generic product <phrase>supply chain</phrase> before and after the product is manufactured. focusing on the post-product <phrase>supply chain management</phrase>, we gradually refine the details of the fundamental, almost "<phrase>classical</phrase>" <phrase>electronic commerce</phrase> interaction between the retailer and the end-customer, namely payment over the <phrase>internet</phrase> using the customer's <phrase>credit card</phrase>. the specification <phrase>results</phrase> in a set of object-process diagrams and a corresponding equivalent set of object-process <phrase>language</phrase> sentences. the <phrase>synergy</phrase> of combining structure and behavior within a <phrase>single</phrase> formal <phrase>model</phrase>, expressed both graphically and textually, yields a highly expressive system modeling and specification tool. the <phrase>comprehensive</phrase>, unambiguous treatment of this <phrase>basic</phrase> <phrase>electronic commerce</phrase> process is formal, yet intuitive and clear, suggesting that opm is a prime candidate for becoming a common standard vehicle for defining, specifying, and analyzing <phrase>electronic commerce</phrase> and <phrase>supply chain management</phrase> systems.
<phrase>information</phrase> analysis in <phrase>uml</phrase> and <phrase>orm</phrase>: a comparison.
since its <phrase>adoption</phrase> by the <phrase>object management group</phrase> as a <phrase>language</phrase> for <phrase>object-oriented analysis and design</phrase>, the <phrase>unified modeling language</phrase> (<phrase>uml</phrase>) has become widely used for designing <phrase>object-oriented</phrase> code. however, <phrase>uml</phrase> has had only minimal <phrase>adoption</phrase> among practitioners for the purposes of <phrase>information</phrase> analysis and <phrase>database design</phrase>. one main reason for this is that the class diagrams used in <phrase>uml</phrase> for <phrase>data modeling</phrase> provide only weak, and awkward, support for the kinds of <phrase>business</phrase> rules found in <phrase>data</phrase>-intensive applications. moreover, uml's <phrase>graphical</phrase> <phrase>language</phrase> does not lend itself readily to verbalization and multiple instantiation for validating <phrase>data</phrase> models with domain experts. these defects can be remedied by using a fact-oriented approach for <phrase>information</phrase> analysis, from which <phrase>uml</phrase> class diagrams may be derived. <phrase>object-role modeling</phrase> (<phrase>orm</phrase>) is currently the most popular fact-oriented modeling approach. this chapter examines the relative strengths and weaknesses of <phrase>uml</phrase> and <phrase>orm</phrase> for conceptual <phrase>data modeling</phrase>, and indicates how models in one notation can be translated into the other.
<phrase>cooperative</phrase> <phrase>query processing</phrase> via <phrase>knowledge</phrase> abstraction and query relaxation.
as <phrase>database</phrase> users adopt a <phrase>query language</phrase> to obtain <phrase>information</phrase> from a <phrase>database</phrase>, a more intelligent query answering system is increasingly needed that cooperates with the users to provide informative responses by understanding the intent behind a query. the effectiveness of <phrase>decision support</phrase> would improve significantly if the query answering system returned approximate answers rather than a null <phrase>information</phrase> response when there is no matching <phrase>data</phrase> available. even when exact answers are found, neighboring <phrase>information</phrase> is still useful to users if the query is intended to explore some hypothetical <phrase>information</phrase> or abstract <phrase>general</phrase> fact. this chapter proposes an abstraction hierarchy as a framework to practically derive such approximate answers from ordinary everyday <phrase>databases</phrase>. it provides a <phrase>knowledge</phrase> abstraction <phrase>database</phrase> to facilitate the approximate query answering. the <phrase>knowledge</phrase> abstraction <phrase>database</phrase> specifically adopts an abstraction approach to extract <phrase>semantic</phrase> <phrase>data</phrase> relationships from the underlying <phrase>database</phrase>, and uses a multi-level hierarchy for coupling multiple levels of abstraction <phrase>knowledge</phrase> and <phrase>data</phrase> values. in cooperation with the underlying <phrase>database</phrase>, the <phrase>knowledge</phrase> abstraction <phrase>database</phrase> allows the relaxation of query conditions so that the original query scope can be broadened and thus <phrase>information</phrase> approximate to exact answers can be obtained. conceptually abstract queries can also be posed to provide a less rigid query interface. a <phrase>prototype</phrase> system has been implemented at <phrase>kaist</phrase> and is being tested with a personnel <phrase>database</phrase> system to demonstrate the usefulness and practicality of the <phrase>knowledge</phrase> abstraction <phrase>database</phrase> in ordinary <phrase>database systems</phrase>.
<phrase>ternary</phrase> relationships: <phrase>semantic</phrase> requirements and logically correct alternatives.
a <phrase>case study</phrase> of the use of the viable system <phrase>model</phrase> in the <phrase>organization</phrase> of <phrase>software development</phrase>.
this chapter considers the usefulness of the viable system <phrase>model</phrase> (vsm) in the study of organizational adaptation. the vsm is a rigorous organizational <phrase>model</phrase> that was developed from the study of <phrase>cybernetics</phrase> and has been given considerable attention by <phrase>management science</phrase> <phrase>research</phrase>. the chapter presents a longitudinal <phrase>case study</phrase> that focuses upon a <phrase>software development</phrase> team. the vsm was useful in diagnosing the likely consequences of different organizational designs and in prescribing an <phrase>alternative</phrase> <phrase>solution</phrase>.
using weakly structured documents at the <phrase>user-interface</phrase> level to fill in a <phrase>classical</phrase> <phrase>database</phrase>.
<phrase>electronic</phrase> documents have become a <phrase>universal</phrase> way of <phrase>communication</phrase> due to web expansion. but using structured <phrase>information</phrase> stored in <phrase>databases</phrase> is still essential for <phrase>data</phrase> coherence <phrase>management</phrase>, querying facilities, etc. we thus face a <phrase>classical</phrase> problem-known as "<phrase>impedance</phrase> mismatch" in the <phrase>database</phrase> world; two <phrase>antagonist</phrase> approaches have to collaborate. using documents at the end-<phrase>user interface</phrase> level provides simplicity and flexibility. but it is possible to take documents as <phrase>data</phrase> sources only if helped by a <phrase>human</phrase> being; automatic document analysis systems have a significant error rate. <phrase>databases</phrase> are an <phrase>alternative</phrase> as <phrase>semantics</phrase> and format of <phrase>information</phrase> are strict; queries via <phrase>sql</phrase> provide 100% correct responses. the aim of this work is to provide a system that associates document capture freedom with <phrase>database</phrase> storage structure.the system we propose does not intend to be <phrase>universal</phrase>. it can be used in specific cases where people usually work with technical documents dedicated to a particular domain. our examples concern <phrase>medicine</phrase> and more explicitly <phrase>medical</phrase> records. computerization has very often been rejected by <phrase>physicians</phrase> because it necessitates too much standardization and form-based <phrase>user interfaces</phrase> are not easily adapted to their <phrase>daily</phrase> practice. in this domain, we think that this study provides a viable <phrase>alternative</phrase> approach. this system offers freedom to <phrase>doctors</phrase>; they would fill in documents with the <phrase>information</phrase> they want to store, in a convenient <phrase>order</phrase> and in a freer way. we have developed a system that allows a <phrase>database</phrase> to fill in quasi-automatically from documents paragraphs.the <phrase>database</phrase> used is an already existing <phrase>database</phrase> that can be queried in a <phrase>classical</phrase> way for statistical studies or <phrase>epidemiological</phrase> purposes. in this system, the document fund and the <phrase>database</phrase> containing extractions from dccuments coexist. queries are sent to the <phrase>database</phrase>, answers include <phrase>data</phrase> from the <phrase>database</phrase> and references to source documents.
extending <phrase>uml</phrase> for space- and time-dependent applications.
changing the face of <phrase>war</phrase> through <phrase>telemedicine</phrase> and <phrase>mobile</phrase> <phrase>e-commerce</phrase>.
foom - functional and <phrase>object-oriented</phrase> methodology for <phrase>analysis and design</phrase> of <phrase>information</phrase> systems.
foom is an integrated methodology for <phrase>analysis and design</phrase> of <phrase>information</phrase> systems, which combines the two essential <phrase>software-engineering</phrase> paradigms: the functional- (or process-) oriented approach and the <phrase>object-oriented</phrase> (oo) approach. in foom, system analysis includes both functional and <phrase>data modeling</phrase> activities, thereby producing both a functional <phrase>model</phrase> and a <phrase>data model</phrase>. these activities can be performed either by starting with <phrase>functional analysis</phrase> and continuing with <phrase>data modeling</phrase>, or vice versa. foom <phrase>products</phrase> of the analysis phase include:a)a hierarchy of oo-<phrase>dfds</phrase> (<phrase>object-oriented</phrase> <phrase>data</phrase> flow diagrams), and b) an initial object schema, which can be created directly from the user requirements specification or from an entity-relationship diagram (erd) that is mapped to that object schema. system <phrase>design</phrase> is performed according to the oo approach. the <phrase>products</phrase> of the <phrase>design</phrase> phase include: a) a complete object schema, consisting of the classes and their relationships, attributes, and method interfaces; b) object classes for the menus, forms and reports; and c) a behavior schema, which consists of detailed descriptions of the methods and the application transactions, expressed in pseudo-code and message diagrams. the seamless transition from analysis to <phrase>design</phrase> is attributed to adissa methodology, which facilitates the <phrase>design</phrase> of the menus, forms and reports classes, and the system behavior schema, from <phrase>dfds</phrase> and the application transactions.
the <phrase>psychology</phrase> of <phrase>information</phrase> modeling.
<phrase>information</phrase> modeling is the <phrase>cornerstone</phrase> of <phrase>information</phrase> <phrase>systems analysis</phrase> and <phrase>design</phrase>. <phrase>information</phrase> models, the <phrase>products</phrase> of <phrase>information</phrase> modeling, not only provide the abstractions required to facilitate <phrase>communication</phrase> between the analysts and end-users, but they also provide a formal basis for developing tools and techniques used in <phrase>information</phrase> systems development. the process of designing, constructing, and adapting <phrase>information</phrase> modeling methods for <phrase>information</phrase> systems development is known as method <phrase>engineering</phrase>. despite the pivotal role of modeling methods in successful <phrase>information</phrase> systems development, most modeling methods are designed based on common sense and intuition of the method designers with little or no theoretical foundation or <phrase>empirical evidence</phrase>. systematic scientific approach is missing! this chapter proposes the use of <phrase>cognitive psychology</phrase> as a reference discipline for <phrase>information</phrase> modeling and method <phrase>engineering</phrase>. theories in <phrase>cognitive psychology</phrase> are reviewed in this chapter and their application to <phrase>information</phrase> modeling and method <phrase>engineering</phrase> is discussed.
how complex is the <phrase>unified modeling language</phrase>?
<phrase>unified modeling language</phrase> (<phrase>uml</phrase>)has emerged as the <phrase>software</phrase> industry's dominant <phrase>modeling language</phrase>. it is the de facto <phrase>modeling language</phrase> standard for specifying, visualizing, constructing, and documenting the components of <phrase>software</phrase> systems. despite its prominence and status as the standard <phrase>modeling language</phrase>, <phrase>uml</phrase> has its critics. opponents argue that it is complex and difficult to learn. some question the rationale of having nine diagramming techniques in <phrase>uml</phrase> and the raison d'tre of those nine techniques in <phrase>uml</phrase>. others point out that <phrase>uml</phrase> lacks a <phrase>comprehensive</phrase> methodology to guide its users, which makes the <phrase>language</phrase> even more convoluted. a few studies on <phrase>uml</phrase> can be found in the <phrase>literature</phrase>. however, no study exists to provide a quantitative measure of <phrase>uml</phrase> complexity or to compare <phrase>uml</phrase> with other <phrase>object-oriented</phrase> techniques. in this <phrase>research</phrase>, we evaluate the complexity of <phrase>uml</phrase> using complexity metrics. the objective is to provide a reliable and accurate quantitative measure of <phrase>uml</phrase> complexity. a comparison of the complexity metrical values of <phrase>uml</phrase> with other <phrase>object-oriented</phrase> techniques was also carried out. our findings suggest that each diagram in <phrase>uml</phrase> is not distinctly more complex than techniques in other modeling methods. but as a whole, <phrase>uml</phrase> is very complex-2-11 times more complex than other modeling methods.
managing organizational <phrase>hypermedia</phrase> documents: a meta-<phrase>information</phrase> system.
recently, many organizations have attempted to build <phrase>hypermedia</phrase> systems to expand their working areas into <phrase>internet</phrase>-based virtual work places. increasingly, it becomes more important than ever to manage organizational <phrase>hypermedia</phrase> documents (ohds); <phrase>metadata</phrase> plays a critical role for managing these documents. this chapter redefines <phrase>metadata</phrase> roles and proposes a <phrase>metadata</phrase> classification and the corresponding <phrase>metadata</phrase> schema for ohds. furthermore, a meta-<phrase>information</phrase> system, hydomis (hyperdocument meta-<phrase>information</phrase> system) built on the basis of this schema is proposed. hydomis performs three functions: <phrase>metadata</phrase> <phrase>management</phrase>, search, and reporting. the <phrase>metadata</phrase> <phrase>management</phrase> <phrase>function</phrase> is concerned with <phrase>workflow</phrase>, documents, and <phrase>databases</phrase>. the system is more likely to help implement and maintain <phrase>hypermedia</phrase> <phrase>information</phrase> systems effectively.
formal approaches to <phrase>systems analysis</phrase> using <phrase>uml</phrase>: an overview.
implementation techniques for extensible object storage systems.
a review of experiments on <phrase>natural language</phrase> interfaces.
a framework for analyzing <phrase>mobile</phrase> transaction models.
methodology evaluation framework for <phrase>component-based</phrase> system development.
considering mobility in <phrase>query processing</phrase> for <phrase>mobile commerce</phrase> systems.
a run-time based technique to optimize queries in distributed <phrase>internet</phrase> <phrase>databases</phrase>.
towards flexible specification, composition, and coordination of <phrase>workflow</phrase> activities.
on the representation of temporal dynamics.
<phrase>software</phrase> agents for <phrase>mobile commerce</phrase> services support.
the development of ordered <phrase>sql</phrase> packages in <phrase>peer-to-peer</phrase> warehousing environment.
performance implication of <phrase>knowledge</phrase> discovery techniques in <phrase>databases</phrase>.
applying <phrase>uml</phrase> for designing <phrase>multidimensional databases</phrase> and <phrase>olap</phrase> applications.
meta-<phrase>model</phrase> based <phrase>information</phrase> mediation.
federated process framework for transparent process monitoring in <phrase>business process outsourcing</phrase>.
fuzzy aggregations and fuzzy specializations in <phrase>eindhoven</phrase> fuzzy eer <phrase>model</phrase>.
using <phrase>demo</phrase> and <phrase>orm</phrase> in <phrase>concert</phrase>: a <phrase>case study</phrase>.
improving the understandability of dynamic <phrase>semantics</phrase>: an enhanced metamodel for <phrase>uml</phrase> <phrase>state</phrase> machines.
online analytic <phrase>mining</phrase> for web access patterns.
modeling motion: building blocks of a motion <phrase>database</phrase>.
comparing metamodels for <phrase>er</phrase>, <phrase>orm</phrase> and <phrase>uml</phrase> <phrase>data</phrase> models.
<phrase>regression</phrase> <phrase>test</phrase> selection for <phrase>database</phrase> applications.
framework for the rapid development of modeling environments.
revisiting <phrase>workflow</phrase> modeling with statecharts.
normalization of relations with nulls in candidate keys: traditional and domain key normal forms.
metrics for <phrase>workflow</phrase> <phrase>design</phrase>: how an <phrase>information processing</phrase> view on <phrase>business processes</phrase> helps to make good designs.
preface.
agile development methods and component-orientation: a review and analysis.
an evaluation framework for <phrase>component-based</phrase> and <phrase>service-oriented</phrase> system development methodologies.
toward an extended framework for <phrase>human factors</phrase> <phrase>research</phrase> on <phrase>data modeling</phrase>.
an attempt to establish a correspondence between development methods and problem domains.
evaluation of <phrase>component-based</phrase> development methods.
two meta-models for <phrase>object-role modeling</phrase>.
comprehension of hierarchical <phrase>er</phrase> diagrams compared to flat <phrase>er</phrase> diagrams.
analyzing and comparing <phrase>ontologies</phrase> with meta-methods.
participatory development of enterprise process models.
constraints on conceptual join paths.
evaluating conceptual coherence in multi-modeling techniques.
a comparison of the foom and opm methodologies for user comprehension of analysis specifications.
an empirical investigation of requirements specification languages: detecting defects while formalizing requirements.
goal modeling in <phrase>requirements engineering</phrase>: analysis and critique of current methods.
assessing enterprise modeling languages using a generic quality framework.
preface.
validating an evaluation framework for <phrase>requirements engineering</phrase> tools.
an approach for <phrase>evolution</phrase>-driven method <phrase>engineering</phrase>.
a <phrase>taxonomic</phrase> class modeling methodology for <phrase>object-oriented</phrase> analysis.
a <phrase>service-oriented</phrase> component modeling approach.
using a <phrase>semiotic</phrase> framework for a comparative study of <phrase>ontology</phrase> languages and tools.
using <phrase>logic</phrase> for querying <phrase>xml</phrase> <phrase>data</phrase>.
ubiquitous access to web <phrase>databases</phrase>.
protecting datasources over the web: policies, models, and mechanisms.
practical <phrase>case study</phrase> of a <phrase>web-based</phrase> <phrase>tutor</phrase> payment system.
<phrase>web content management</phrase> and dynamic web pages - a tutorial.
cache <phrase>management</phrase> for web-powered <phrase>databases</phrase>.
<phrase>database</phrase>-driven product catalog system.
the development of on-line <phrase>tests</phrase> based on <phrase>multiple choice</phrase> questions.
web <phrase>mining</phrase> to create a <phrase>domain specific</phrase> <phrase>web portal</phrase> <phrase>database</phrase>.
codar - a <phrase>poa</phrase>-based <phrase>corba</phrase> <phrase>database</phrase> adaptor for <phrase>web service</phrase> infrastructures.
web-powered <phrase>databases</phrase>: the low level in <phrase>c++</phrase>.
effective <phrase>databases</phrase> for text & <phrase>document management</phrase>.
<phrase>database</phrase> integrity: challenges and solutions
<phrase>encyclopedia</phrase> of <phrase>information science</phrase> and <phrase>technology</phrase> (5 volumes)
<phrase>encyclopedia</phrase> of <phrase>database</phrase> technologies and applications
<phrase>multidimensional databases</phrase>: problems and solutions
<phrase>information</phrase> modeling in the new <phrase>millennium</phrase>
<phrase>unified modeling language</phrase>: <phrase>systems analysis</phrase>, <phrase>design</phrase> and development issues
advanced topics in <phrase>database</phrase> <phrase>research</phrase>, vol. 1
advanced topics in <phrase>database</phrase> <phrase>research</phrase>, vol. 2
advanced topics in <phrase>database</phrase> <phrase>research</phrase>, vol. 3
<phrase>information</phrase> modeling methods and methodologies
spatial <phrase>databases</phrase>: technologies, techniques and trends
web-powered <phrase>databases</phrase>
advances in real-time systems.
ein datenbankkern zur speicherung variabel strukturierter feature-terme. implementierungstechniken.
eine fallbasierte lernkomponente als integraler bestandteil der moltke-werkbank zur diagnose technischer systeme.
oberflchenbasierte segmentierung von tiefenbildern.
ein planbasierter <phrase>ansatz</phrase> zur generierung multimedialer prsentationen.
effiziente pufferverwaltung in parallelen relationalen datenbanksystemen.
operationalisierung des modells der expertise <phrase>mit</phrase> karl.
integration of active and <phrase>deductive</phrase> <phrase>database</phrase> rules.
indexierung und retrieval von feature-bumen am beispiel der linguistischen analyse von textkorpora.
inkrementelle wrterbuchbasierte wortschatzerweiterungen in sprachverarbeitenden systemen. entwurf einer konstruktiven lexikonkonzeption.
parallele modelle <phrase>fr</phrase> deduktionssysteme.
hamvis: generierung von visualisierungen in einem rahmensystem zur systematischen entwicklung von benutzungsschnittstellen.
ein modellgesttztes analysesystem zum bildverstehen strukturierter dokumente.
verwendung von bildauswertungsmethoden zur erkennung und lagebestimmung von generischen polyedrischen objekten im raum.
implementierung einer effizienten anfrageauswertung <phrase>fr</phrase> ein deduktives datenbanksystem.
fehlertolerante und effiziente automatische analyse digitalisierter dokumente zur gewinnung von hypertextstrukturen.
effizientes problemlsen durch flexible wiederverwendung von fllen auf verschiedenen abstraktionsebenen.
modellkonstruktion in mike. methoden und werkzeuge.
steuerungsanstze auf der basis neuronaler netze <phrase>fr</phrase> sechsbeinige laufmaschinen.
validierung konzeptueller schemata.
fuzzy-techniken in objektorientierten datenbanksystemen zur untersttzung von entwurfsprozessen.
pragmatische programmsynthese.
dear kv, i hope you don't mind if i ask you about a non-work-related problem, though i guess if you do mind you just won't answer. i work on an <phrase>open source</phrase> project when i have the time, and we have some annoying nontechnical problems. the problems are really people, and i think you know the ones i mean: people who constantly fight with other members of the project over what seem to be the most trivial points, or who contribute very little to the project but seem to require a huge amount of help for their particular needs. i find myself thinking it would be <phrase>nice</phrase> if such people just went away, but i don't think starting a flame <phrase>war</phrase> on our <phrase>mailing lists</phrase> over these things would really help. any thoughts on this nontechnical problem?
ein informationsmodell <phrase>fr</phrase> ableitungsprozesse und ihre ergebnisse im wissensgewinnungsproze.
separierung und resolution multipler perspektiven in der konzeptuellen modellierung.
erweiterung der wissensbasierten <phrase>cad</phrase>-konstruktion um restriktionsnetztechniken.
zur darstellung und verarbeitung von wissen ber himmelsrichtungen.
adaptive neuronale netze und ihre anwendung als modelle der entwicklung kortikaler karten.
das 3-stufige frame-reprsentationsschema - eine mehrdimensional modulare basis <phrase>fr</phrase> die entwicklung von expertensystemkernen.
extraktion von linienfrmigen merkmalen und ermittlung des optischen flusses <phrase>mit</phrase> seinen ableitungen aus voll- und halbbildfolgen.
verfeinerung in objektorientierten spezifikationen.
analyse und optimierung von indexstrukturen in geo-datenbanksystemen.
externe schemata in objektorientierten datenbansystemen.
erklrungsbasiertes computer-sehen von bildfolgen.
zur natrlichsprachlichen interaktiven untersttzung im datenbankentwurf.
optimierung deklarativer anfragen in objektbanken.
die bildanalysesprache trias.
deduktion <phrase>mit</phrase> shannongraphen <phrase>fr</phrase> prdikatenlogik erster stufe.
parallele suche <phrase>mit</phrase> randomisiertem wettbewerb in inferenzsystemen.
erklrungen <phrase>fr</phrase> komplexe wissensbasen.
wissensbasierte verfahren zur synthese mathematischer beweise: eine kombinatorische anwendung.
definition of behavior in <phrase>object-oriented</phrase> <phrase>databases</phrase> by view integration.
schematransformationen in datenbanken.
ein regelsystem zur integrttssicherung in aktiven relationalen datenbanksystemen.
verteiltes und kooperatives planen in einer flexiblen fertigungsumgebung.
eine parallele architektur zur inkrementellen generierung multimodaler dialogbeitrge.
view integration <phrase>fr</phrase> objektorientierte datenbanken.
grammatikentwicklung <phrase>mit</phrase> constraint logik programmierung. implementierung einer grammatik <phrase>fr</phrase> das deutsche <phrase>mit</phrase> <phrase>prolog</phrase> iii.
replikationsmanagement in verteilten informationssystemen.
a <phrase>tour</phrase> on trigs. devolopment of an active system and application of rule patterns for active <phrase>database design</phrase>.
optimierung von speicherzugriffskosten in objektbanken: clustering und prefetching.
transaction services for <phrase>knowledge base</phrase> <phrase>management</phrase> systems. modeling aspects, <phrase>architectural</phrase> issues, and realization techniques.
diagnosis and repair of constraint violations in <phrase>database systems</phrase>.
fderierte datenbanktechnologie <phrase>fr</phrase> die molekularbiologie: konzepte, einsatz und nutzen.
das <phrase>digital</phrase> lecture board. konzeption, <phrase>design</phrase> und entwicklung eines whiteboards <phrase>fr</phrase> synchrones teleteaching.
db-gesttzte kooperationsdienste <phrase>fr</phrase> technische entwurfsanwendungen.
bedarfsorientierte dienstvermittlung in vernetzten systemen.
materialisation and parallelism in the mapping of an object <phrase>model</phrase> to a <phrase>relational</phrase> multi-processor system.
flexible kontrolle in expertensystemen zur planung und konfigurierung in technischen domnen.
wissensbasiertes lsen von ablaufplanungsproblemen durch explizite heuristiken.
ein basisdienst <phrase>fr</phrase> die zuverlssige abwicklung langdauernder aktivitten.
interoperabilitt von datenbanksystemen bei struktureller heterogenitt. architektur, beschreibungs- und ausfhrungsmodell zur untersttzung der integration und migration.
konzeptionelle modellierung von informationssystemen als verteilte objektsysteme.
trabsaktionsverwaltung in heterogenen, fderierten datenbanksystemen.
entwurf einer sprache <phrase>fr</phrase> die verhaltensorientierte konzeptionelle modellierung von informationssystemen.
schemaintegration <phrase>fr</phrase> den entwurf fderierter datenbanken.
kommunikationsoptimierung <phrase>fr</phrase> <phrase>mobile</phrase> agenten durch hierarchisches klonen.
erwartungsgesttzte analyse medizinischer befundungstexte. ein wissensbasiertes modell zur sprachverarbeitung.
einheitliche theorie <phrase>fr</phrase> korrekte parallele und fehlertolerante ausfhrung von datenbanktransaktionen.
architektur verteilter <phrase>workflow</phrase>-<phrase>management</phrase>-systeme.
alcp - ein hybrider <phrase>ansatz</phrase> zur modellierung von unsicherheit in terminologischen logiken.
das konzept der transaktionshlle zur konsistenten spezifikation von abhnigigkeiten in komplexen anwendungen.
kognitives <phrase>parsing</phrase>: reprsentation und verarbeitung sprachlichen wissens.
dynamische, situationsbezogene <phrase>hypertext</phrase>-handbcher <phrase>fr</phrase> komplexe ttigkeiten.
effiziente konsistenzprfunf in datenbanksystemen.
mokon - ein <phrase>ansatz</phrase> zur wissensbasierten konfiguration von variantenerzeugnissen.
skalierbare <phrase>multicast</phrase>-kommunikation in weitverkehrsnetzen.
source-to-source transformationen zur erklrung des programmverhaltens bei deduktiven datenbanken.
symbolisches lsen mathematischer probleme durch kooperation algorithmischer und logischer systeme.
situationsanalyse bei kontakten whrend der ausfhrung von roboterbewegungen in unsicheren umgebungen.
begleitende montageablaufplanung <phrase>fr</phrase> ein sensorgesttztes zweiarm-manipulatorsystem.
sichtenmanagement in client-server-systemen.
dynamische modularisierung lexikalischer informationen in einem wiederverwendungsszenario.
wissensbasierte analyse medizinischer bilder - das biotop-verfahren.
benutzeranpabare semantische sprachanalyse und begriffsreprsentation <phrase>fr</phrase> die medizinische dokumentation.
<phrase>heuristic</phrase> and randomised optimisation techniques in <phrase>object-oriented</phrase> <phrase>database systems</phrase>.
nderungskontrolle in deduktiven datenbanken.
verdeckungen und spezielle sichten bei der polyederrekonstruktion.
<phrase>access control</phrase> in <phrase>object-oriented</phrase> federated <phrase>database systems</phrase>.
generierung domnenspezifischer wissensreprsentationssysteme und transformation von wissensbasen <phrase>mit</phrase> einer anwendung in der rechtsinformatik.
interaktive akquisition elementarer roboterfhigkeiten.
modulare problemlsungsarchitekturen <phrase>fr</phrase> konstruktionssysteme.
toleranz- und kongruenzrelationen in relationalen datenbanken.
modellierung von expertise ber konfigurierungsaufgaben.
reduktion zur effizienten programmierung von methoden in deduktiven objektorientierten datenbanken.
an approach to <phrase>query processing</phrase> in advanced <phrase>database systems</phrase>.
<phrase>metadata</phrase>-based <phrase>middleware</phrase> for integrating <phrase>information</phrase> systems.
<phrase>semantic</phrase> <phrase>integrity constraints</phrase> in federated <phrase>database</phrase> schemata.
<phrase>model</phrase>-k: modellierung und operationalisierung von selbsteinschtzung und -steuerung durch reflexion und metawissen.
emsy - ein modellierungskonzept <phrase>fr</phrase> kologische und biologische systeme unter besonderer bercksichtigung ihrer dynamischen vernderung.
akquisition von integrittsbedingungen in datenbanken.
inkrementelle, domnenunabhngige thesauruserstellung in dokumentbasierten informationssystemen durch kombination von konstruktionsverfahren.
untersttzung anwendungsspezifischer zugriffsprofile in objektbanksystemen.
dynamische beziehungen als modell der kooperation in verteilten objektsystemen.
eine mehrschichtige architektur zur fehlerdiagnose und fehlerbehebung bei der entwicklung logischer programme.
entwurf eines objektorientierten datenbankmodells <phrase>fr</phrase> relationale datenbanksysteme.
vorgangsmodelle. ein <phrase>ansatz</phrase> zur reprsentation und analyse zeitabhngigen verhaltens bei der berwachung und diagnose technischer systeme.
terminierung und konfluenz in einer aktiven objektorientierten datenbank.
integration von aktionsplanung und konfigurierung.
distributed <phrase>machine learning</phrase>.
detektion, verfolgung und klassifikation bewegter objekte in monokularen bildfolgen am beispiel von straenverkehrsszenen.
produkorientierte automatische planung von prfoperationen bei der robotergesttzten montage.
ein verfahren zur semi-automatischen generierung von mediatorspezifikationen.
wissensbasierte verkehrsszenenanalyse zur fahreruntersttzung.
replikation in vernetzten systemen <phrase>mit</phrase> mobilen teilnehmern.
funktionsorientiertes <phrase>management</phrase> heterogener atm-netzwerke.
control mechanisms in distributed object bases. synchronization, <phrase>deadlock</phrase> detection, migration.
zeitreprsentation und merkmalsgesteuerte suche zur terminplanung.
aktive mechanismen zur konsistenzsicherung in fderationen heterogener und autonomer datenbanken.
modellbildung und architektur von verteilten <phrase>workflow</phrase>-<phrase>management</phrase>-systemen.
flexible steuerung eines sprachverstehenden systems <phrase>mit</phrase> homogener wissensbasis.
wissensbasierte greifplanung <phrase>fr</phrase> mehrfinger-roboterhnde.
entwicklung verteilter objektstrukturen <phrase>fr</phrase> skalierbare und hochgradig verfgbare informationssysteme.
konzeption eines regelbasierten systems <phrase>fr</phrase> produktdatenmanagement in cim.
digitale und allgemeine topologie in der bildhaften wissensreprsentation.
einbettung von konzepthierarchien in ein deduktives datenbanksystem.
typsichere objektbankmigration.
mistral: processing <phrase>relational</phrase> queries using a multidimensional access technique.
wissensintensives lernen <phrase>fr</phrase> zeitkritische technische diagnoseaufgaben.
selbstorganisierende neuronale netzwerkmodelle zur bewegungssteuerung.
lernen durch genetisch-neuronale <phrase>evolution</phrase>: aktive anpassung an unbekannte umgebungen <phrase>mit</phrase> selbstentwickelten parallelen netzwerken.
schema-unabhngige anfragesprachen <phrase>fr</phrase> relationale datenbanken.
qualitative analyse im rahmen qualitativen und modellbasierten schlieens.
ein dialogmodul <phrase>fr</phrase> ein spracherkennungs- und dialogsystem.
<phrase>cyclops</phrase> - wissensbasierte bildanalyse in der medizin.
hypermediabasiertes <phrase>knowledge engineering</phrase> <phrase>fr</phrase> verteilte wissensbasierte systeme.
anfrageoptimierung in objektrelationalen datenbanken durch kostenbedingte termersetzungen
dynamische semantische netze - zur kontextabhngigkeit von wortbedeutungen.
extending the <phrase>stratification</phrase> approach for <phrase>deductive</phrase> <phrase>database systems</phrase>.
wissensbasierte synthese von bildanalyseprogrammen.
adaptive roboterkontrolle <phrase>mit</phrase> konnektionistischen systemen.
<phrase>design</phrase> and implementation of a <phrase>database</phrase> <phrase>programming language</phrase> for <phrase>xml</phrase>-based applications.
towards the intelligent on-line <phrase>home office</phrase>.
the it leap and the sektornet.
<phrase>technology</phrase> as the <phrase>catalyst</phrase> of users' acceptance in <phrase>electronic commerce</phrase>.
introduction by the editors.
accord solutions for flexible working.
flexible quality-of-service <phrase>technology</phrase> for supporting voice/<phrase>data</phrase>-integrated <phrase>nomadic</phrase> networking.
the telco's goldmine - 50 mb/s to the <phrase>general</phrase> <phrase>public</phrase> over the local <phrase>telephone</phrase> lines.
support services for <phrase>business process</phrase> oriented telework. the cobip project.
new ways of flexible working: the contribution of acts, and issues for ist.
<phrase>interoperability</phrase> for <phrase>multimedia</phrase> services: the approach of the standards bodies - an overview based on acts guideline sii g06.
net working in the <phrase>knowledge economy</phrase>.
<phrase>internet service</phrase> architectures and atm - the <phrase>elisa</phrase> approach.
developing a reference <phrase>model</phrase> for networked flexible work through <phrase>industry</phrase> trials.
new <phrase>business</phrase> paradigms.
umptidumpti - anytime, anywhere, anybody - <phrase>umts</phrase> for all.
giving <phrase>mobile</phrase> users access to net-based services - a <phrase>mobile</phrase> agent approach.
<phrase>gestalt</phrase> - on-line <phrase>education</phrase> and training - <phrase>state</phrase> of the <phrase>art</phrase>.
tecodis. the experience of teleworking for <phrase>software</phrase> developers.
flexible working <phrase>technology</phrase> for <phrase>sustainable development</phrase> and social inclusion.
on modeling conformance for flexible transformation over <phrase>data</phrase> models.
<phrase>knowledge representation</phrase> and transformation in <phrase>ontology</phrase>-based <phrase>data integration</phrase>.
the '<phrase>family</phrase> of languages' approach to <phrase>semantic</phrase> <phrase>interoperability</phrase>.
<phrase>uml</phrase> for the <phrase>semantic web</phrase>: transformation-based approaches.
tracing <phrase>data</phrase> lineage using schema transformation pathways.
transforming <phrase>uml</phrase> domain descriptions into configuration <phrase>knowledge</phrase> bases.
transforming <phrase>data</phrase> models with <phrase>uml</phrase>.
schema conversion methods between <phrase>xml</phrase> and <phrase>relational</phrase> models.
rdft: a mapping meta-<phrase>ontology</phrase> for <phrase>web service</phrase> integration.
a <phrase>logic programming</phrase> approach to <phrase>rdf</phrase> document and query transformation.
<phrase>ontology</phrase> extraction for distributed environments.
<phrase>knowledge</phrase> transformation for the <phrase>semantic web</phrase>
machine <phrase>reconstruction</phrase> of <phrase>human</phrase> control strategies
flexible working - new network technologies
<phrase>natural language processing</phrase> for online applications: text retrieval, extraction & categorization
wfms: the next generation of distributed processing tools.
<phrase>semantic</phrase>-based decomposition of transactions.
an extensible approach to realizing advanced transaction models.
the reflective transaction framework.
customizable <phrase>concurrency control</phrase> for persistent <phrase>java</phrase>.
inter- and intra-transaction parallelism for combined oltp/<phrase>olap</phrase> workloads.
transaction optimization techniques.
towards distributed real-time concurrency and coordination control.
flexible commit protocols for advanced <phrase>transaction processing</phrase>.
toward formalizing recovery of (advanced) transactions.
contracts revisited.
<phrase>transaction processing</phrase> in <phrase>broadcast</phrase> disk environments.
transactions in transactional workflows.
logical handling of inconsistent and default <phrase>information</phrase>.
approximate reasoning systems: handling uncertainty and imprecision in <phrase>information</phrase> systems.
an introduction to the <phrase>fuzzy set</phrase> and possibility theory-based treatment of flexible queries and uncertain or imprecise <phrase>databases</phrase>.
uncertainty in intelligent <phrase>databases</phrase>.
a bibliography on uncertainty <phrase>management</phrase> in <phrase>information</phrase> systems.
probabilistic and <phrase>bayesian</phrase> representations of uncertainty in <phrase>information</phrase> systems: a pragmatic introduction.
uncertain, incomplete, and inconsistent <phrase>data</phrase> in scientific and statistical <phrase>databases</phrase>.
on the classification of uncertainty techniques in relation to the application needs.
introduction.
sources of uncertainty, imprecision, and inconsistency in <phrase>information</phrase> systems.
<phrase>knowledge</phrase> discovery and acquisition from imperfect <phrase>information</phrase>.
imperfect <phrase>information</phrase>: imprecision and uncertainty.
the transferable belief <phrase>model</phrase> for belief representation.
uncertainty in <phrase>information retrieval</phrase> systems.
imperfect <phrase>information</phrase> in <phrase>relational databases</phrase>.
storm: an <phrase>object-oriented</phrase> <phrase>multimedia</phrase> <phrase>dbms</phrase>.
synchronization and user interaction in distributed <phrase>multimedia</phrase> presentation systems.
<phrase>database</phrase> approach for the <phrase>management</phrase> of <phrase>multimedia</phrase> <phrase>information</phrase>.
a visual <phrase>multimedia</phrase> <phrase>query language</phrase> for temporal analysis of <phrase>video</phrase> <phrase>data</phrase>.
a <phrase>multimedia</phrase> query <phrase>specification language</phrase>.
searching and browsing a shared <phrase>video</phrase> <phrase>database</phrase>.
load-balanced <phrase>data</phrase> placement for <phrase>variable</phrase>-rate continuous <phrase>media</phrase> retrieval.
<phrase>multimedia</phrase> <phrase>dbms</phrase> - <phrase>reality</phrase> of hype?
<phrase>model</phrase> for interactive retrieval of videos and still images.
<phrase>playout</phrase> <phrase>management</phrase> in <phrase>multimedia</phrase> <phrase>database systems</phrase>.
an <phrase>object-oriented</phrase> modeling of <phrase>multimedia</phrase> <phrase>database</phrase> objects and applications.
integration of <phrase>simulation</phrase> and <phrase>multimedia</phrase> in automatically generated <phrase>internet</phrase> courses.
a visual <phrase>simulation</phrase> environment for mips based on <phrase>vhdl</phrase>.
an authoring environment for the simulnet educational platform.
babelwin: an environment for learning and monitoring <phrase>reading</phrase> and writing skills.
an <phrase>internet</phrase> <phrase>distance-learning</phrase> operating <phrase>model</phrase>.
synchronous <phrase>drawing</phrase> actions in environments of <phrase>collaborative learning</phrase> of <phrase>design</phrase>.
interconnecting courseware modules via www.
aprisa: a tool for teaching the interconnection of open systems.
adaptive <phrase>internet</phrase>-based learning with the tangow system.
interactive <phrase>mathematics</phrase> teaching with mathedu.
teaching support units.
the interactive <phrase>physics</phrase> course on the <phrase>internet</phrase>.
task based training of application users.
suma project (open <phrase>murcia</phrase> <phrase>university</phrase> services).
introducing waves using simulations controled from <phrase>html</phrase> files.
using <phrase>bayesian</phrase> networks in computerized adaptive <phrase>tests</phrase>.
<phrase>xml</phrase>-based integration of <phrase>hypermedia</phrase> <phrase>design</phrase> and <phrase>component-based</phrase> techniques in the <phrase>production</phrase> of educational applications.
<phrase>computers</phrase> in <phrase>education</phrase>: the near future.
learning <phrase>basque</phrase> in a distance-adaptive way.
collaborative planning for problem <phrase>solution</phrase> in <phrase>distance learning</phrase>.
<phrase>high</phrase> <phrase>level design</phrase> of <phrase>web-based</phrase> environments for <phrase>distance education</phrase>.
m.a.c. a <phrase>hypermedia</phrase> system for learning <phrase>electronics</phrase>.
computer-<phrase>human</phrase> learning.
<phrase>natural language processing</phrase> in educational computer <phrase>science</phrase>.
educational web sites: some issues for evaluation.
building a <phrase>virtual learning environment</phrase> using agents.
approach to intelligent adaptive testing.
evaluation criteria for <phrase>hypermedia</phrase> educational systems.
using teachers as heuristics evaluators of <phrase>educational software</phrase> interfaces.
creating collaborative environments for <phrase>web-based</phrase> training scenarios.
the next step in computer based <phrase>education</phrase>: the learning technologies standardisation.
advanced learning environments: changed views and future perspectives.
development of didactic resources for <phrase>distance learning</phrase> based on <phrase>simulation</phrase>.
tutormap. past, present and future of a <phrase>mathematics</phrase> tutoring system.
learning communities in the web: concepts and strategies.
symbolic <phrase>calculus</phrase> training by means of mathtrainer.
ejs: an authoring tool to develop <phrase>java</phrase> applications.
<phrase>adam</phrase> case. using <phrase>upper</phrase> case tools in <phrase>software engineering</phrase> <phrase>laboratory</phrase>.
sicas. interactive system for <phrase>algorithm</phrase> development and <phrase>simulation</phrase>.
simurob and jrf. teaching tools for <phrase>robot</phrase> <phrase>simulation</phrase> and <phrase>programming</phrase>.
collares ortofnicos. system for the training of the suprasegmental parameters of intensity and <phrase>rhythm</phrase> of the articulated <phrase>sound</phrase>.
exercita. a system for archiving and <phrase>publishing</phrase> <phrase>programming</phrase> exercises.
interactive <phrase>design</phrase> of adaptive courses.
using <phrase>simulation</phrase> and <phrase>virtual reality</phrase> for <phrase>distance education</phrase>.
ed68k. a <phrase>design</phrase> framework for the development of <phrase>digital</phrase> systems based on mc68000.
an experience on virtual teaching: aulanet.
guided collaborative <phrase>chess</phrase> tutoring through <phrase>game</phrase> <phrase>history</phrase> analysis.
using analysis, <phrase>design</phrase> and development of <phrase>hypermedia</phrase> applications in the educational domain.
<phrase>ubiquitous computing</phrase> and collaboration. new interaction paradigms in the classroom for the 21st century.
shared <phrase>whiteboard</phrase> <phrase>manager</phrase> and <phrase>student</phrase> notebook for the plan-g telematic platform.
creation of a <phrase>multimedia</phrase> system for learning about oscillations.
guiding the user. an element to aid <phrase>knowledge</phrase> <phrase>construction</phrase> in adaptive <phrase>hypermedia</phrase> systems.
pedagogical strategies with <phrase>hypermedia</phrase>. limiting access to hyperspace for educational purposes.
<phrase>artificial intelligence</phrase> in the hyperclass: <phrase>design</phrase> issues.
improving the <phrase>language</phrase> mastery through responsive environments.
<phrase>test</phrase> <phrase>construction</phrase> and <phrase>management</phrase> with network <phrase>adaptive control</phrase>.
foundations of <phrase>programming</phrase>: a teaching improvement.
hci curricula in <phrase>spain</phrase>. a cooperatively designed, <phrase>free</phrase> web-access syllabus.
<phrase>sql</phrase>-92 compatibility issues.
temporal <phrase>data</phrase> types.
aggregates
modification.
a second example.
schema specification.
the from clause.
rationale for a temporal extension to <phrase>sql</phrase>.
supporting multiple calendars.
the surrogate <phrase>data type</phrase>.
vacuuming.
the <phrase>data model</phrase> for time.
a <phrase>timestamp</phrase> representation.
temporal granularity.
temporal indeterminancy.
the baseline <phrase>clock</phrase>.
``now''.
valid-time selection and projection.
cursors.
the tsql2 <phrase>data model</phrase>.
transaction time support.
schema versioning.
introduction to tsql2.
tsql2 tutorial.
event tables.
an <phrase>algebra</phrase> for tsql2
an <phrase>architectural</phrase> framework.
frontmatter.
references, <phrase>author</phrase> index, <phrase>syntax</phrase> index, subject index.
<phrase>language</phrase> specification.
clustering in <phrase>metric spaces</phrase> with applications to <phrase>information retrieval</phrase>.
techniques for textual document indexing and retrieval <phrase>knowledge</phrase> sources and <phrase>data mining</phrase>.
a <phrase>science</phrase> <phrase>data</phrase> system <phrase>architecture</phrase> for <phrase>information retrieval</phrase>.
finding topics in collections of documents: a shared nearest neighbor approach.
techniques for clustering massive <phrase>data</phrase> sets.
on quantitative evaluation of clustering systems.
document clustering, visualization, and retrieval link <phrase>mining</phrase>.
clustering techniques for large <phrase>database</phrase> cleansing.
query clustering in the web context.
granular <phrase>computing</phrase> for the <phrase>design</phrase> of <phrase>information retrieval</phrase> <phrase>support systems</phrase>.
<phrase>indexing techniques</phrase> for advanced <phrase>database systems</phrase>
advanced <phrase>relational</phrase> <phrase>programming</phrase>: <phrase>mathematics</phrase> and its applications.
<phrase>mining</phrase> the <phrase>world wide web</phrase>: an <phrase>information</phrase> search approach
logics for <phrase>databases</phrase> and <phrase>information</phrase> systems (the <phrase>book</phrase> grow out of the dagstuhl seminar 9529: role of logics in <phrase>information</phrase> systems, 1995)
the discovery of the <phrase>artificial</phrase>. behavior, mind and machines before and beyond <phrase>cybernetics</phrase>
<phrase>multimedia</phrase> <phrase>mining</phrase>: a <phrase>highway</phrase> to intelligent <phrase>multimedia</phrase> documents
client <phrase>data</phrase> caching: a foundation for <phrase>high</phrase> performance <phrase>object oriented</phrase> <phrase>database systems</phrase>
<phrase>software prototyping</phrase> in <phrase>data</phrase> and <phrase>knowledge engineering</phrase>
advanced transaction models and architectures
advanced <phrase>database</phrase> indexing
advanced signature indexing for <phrase>multimedia</phrase> and <phrase>web applications</phrase>
:advanced signature indexing for <phrase>multimedia</phrase> and <phrase>web applications</phrase> is an excellent reference for professionals involved in the development of applications in <phrase>multimedia</phrase> <phrase>databases</phrase> or the web and may also serve as a <phrase>textbook</phrase> for advanced level courses in <phrase>database</phrase> and <phrase>information retrieval</phrase> systems.
<phrase>ontology</phrase>-based <phrase>query processing</phrase> for global <phrase>information</phrase> systems
<phrase>ontology</phrase>-based <phrase>query processing</phrase> for global <phrase>information</phrase> systems describes an <phrase>initiative</phrase> for enhancing <phrase>query processing</phrase> in a global <phrase>information</phrase> system. the following are some of the relevant features: providing <phrase>semantic</phrase> descriptions of <phrase>data</phrase> repositories using <phrase>ontologies</phrase>. dealing with different vocabularies so that users are not forced to use a common one. defining a strategy that permits the incremental enrichment of answers by visiting new <phrase>ontologies</phrase>. managing imprecise answers and estimations of the incurred loss of <phrase>information</phrase>.
compression and coding <phrase>algorithms</phrase>
<phrase>open source</phrase> <phrase>gis</phrase>: a <phrase>grass gis</phrase> approach
<phrase>computers</phrase> and <phrase>education</phrase> in the 21st century
<phrase>computers</phrase> and <phrase>education</phrase>. towards an interconnected <phrase>society</phrase>
the <phrase>design</phrase> and implementation of a log-structured file-system.
this dissertation presents a new technique for <phrase>disk storage</phrase> <phrase>management</phrase> called a log-structured file system. the technique writes all file system changes in large sequential transfers to a log-like structure on disk. the key benefit is a <phrase>high</phrase> write performance that is <phrase>independent</phrase> of the workload. the large transfers also enable the efficient use of large disk arrays such as raids. the technique minimizes the overhead of <phrase>computing</phrase> the redundancy <phrase>information</phrase> required by large raids. a log-structured file system achieves <phrase>high</phrase> write rates without sacrificing file retrieval performance. files are read back from the log efficiently due to the indexing <phrase>information</phrase> that is maintained the log structure also permits fast recovery from system crashes. using a recovery system based on checkpoints and roll-<phrase>forward</phrase> the log-structured file system can quickly restore the disk to a consistent <phrase>state</phrase>. an important focus of this dissertation is the technique used for <phrase>free</phrase> space <phrase>management</phrase> in a log-structured file system. the approach taken was to divide the disk into large segments to which the log was written. a segment cleaner mechanism exists to compress the <phrase>live</phrase> <phrase>information</phrase> from heavily fragmented segments. the mechanism reads in the fragmented segments, compacts the <phrase>live</phrase> <phrase>data</phrase>, and writes the <phrase>data</phrase> back to segments on disk. the dissertation includes a series of simulations that demonstrate the efficiency of a simple segment cleaning policy based on cost and benefit. the segment cleaner decides which segments to clean based on a <phrase>function</phrase> of the fraction alive in the segment and the age of the <phrase>data</phrase> in the segment. i have implemented a <phrase>prototype</phrase> log-structured file system called sprite lfs; it outperforms current <phrase>unix</phrase> <phrase>file systems</phrase> by an <phrase>order</phrase> of <phrase>magnitude</phrase> for small-file writes and matches or exceeds <phrase>unix</phrase> performance for reads and large writes. even when the overhead for cleaning is included, sprite lfs can use 70% of the disk bandwidth for writing. <phrase>unix</phrase> <phrase>file systems</phrase> typically can use only 5-10%.
the tsql2 temporal <phrase>query language</phrase>
<phrase>data quality</phrase>
clustering and <phrase>information retrieval</phrase>
loop tiling for parallelism
a framework for meta-<phrase>information</phrase> in <phrase>digital libraries</phrase>.
<phrase>metadata</phrase> handling in hyperstorm.
overview on using <phrase>metadata</phrase> to manage <phrase>multimedia</phrase> <phrase>data</phrase>.
<phrase>metadata</phrase> for <phrase>mixed-media</phrase> access.
<phrase>content-based image retrieval</phrase> using <phrase>metadata</phrase> and relaxation techniques.
<phrase>metadata</phrase> <phrase>management</phrase> for geographic <phrase>information</phrase> discovery and exchange.
using <phrase>metadata</phrase> for the intelligent browsing of structured <phrase>media</phrase> objects.
<phrase>metadata</phrase> in geographic and environmental <phrase>data management</phrase>.
<phrase>video</phrase> <phrase>data management</phrase> systems: <phrase>metadata</phrase> and <phrase>architecture</phrase>.
a metadatabase system for <phrase>semantic</phrase> image search by a <phrase>mathematical model</phrase> of meaning.
in the <phrase>design</phrase> of <phrase>multimedia</phrase> <phrase>database systems</phrase>, one of the most important issues is to extract images dynamically according to the user's impression and the image's contents. in this <phrase>paper</phrase>, we present a metadatabase system which realizes the <phrase>semantic</phrase> associative search for images by giving keywords representing the user's impression and the image's contents.this metadatabase system provides several functions for performing the <phrase>semantic</phrase> associative search for images by using the <phrase>metadata</phrase> representing the features of images. these functions are realized by using our proposed <phrase>mathematical model</phrase> of meaning. the <phrase>mathematical model</phrase> of meaning is extended to compute specific meanings of keywords which are used for retrieving images unambiguously and dynamically. the main feature of this <phrase>model</phrase> is that the <phrase>semantic</phrase> associative search is performed in the orthogonal <phrase>semantic</phrase> space. this space is created for dynamically <phrase>computing</phrase> <phrase>semantic</phrase> equivalence or similarity between the <phrase>metadata</phrase> items of the images and keywords.
the use of <phrase>metadata</phrase> for the rendering of personalized <phrase>video</phrase> delivery.
<phrase>metadata</phrase> for <phrase>content-based</phrase> retrieval of speech <phrase>recording</phrase>.
<phrase>microsoft sql server</phrase> (<phrase>chapter 27</phrase>)
<phrase>database systems</phrase> - concepts, languages and architectures
<phrase>sql</phrase> - the standard handbook
distributed <phrase>databases</phrase>: principles and systems
introduction to <phrase>algorithms</phrase>
introduction to <phrase>algorithms</phrase>, second edition
<phrase>oracle</phrase> (chapter 25)
<phrase>ibm db2</phrase> <phrase>universal</phrase> <phrase>database</phrase> (chapter 26)
<phrase>database management systems</phrase>.
introduction to modern <phrase>information retrieval</phrase>.
<phrase>multimedia</phrase> <phrase>data management</phrase>: using <phrase>metadata</phrase> to integrate and apply <phrase>digital media</phrase>
<phrase>database</phrase> system concepts, 1st edition.
<phrase>database</phrase> system concepts, <phrase>2nd edition</phrase>.
<phrase>database</phrase> system concepts, 4th edition.
:this acclaimed revision of a classic <phrase>database systems</phrase> text offers a complete background in the basics of <phrase>database design</phrase>, languages, and system implementation. it provides the latest <phrase>information</phrase> combined with <phrase>real-world</phrase> examples to help readers <phrase>master</phrase> concepts. all concepts are presented in a technically complete yet easy-to-understand style with notations kept to a minimum. a running example of a <phrase>bank</phrase> enterprise illustrates concepts at work. to further optimize comprehension, figures and examples, rather than proofs, portray concepts and anticipate <phrase>results</phrase>.
<phrase>database</phrase> system concepts, <phrase>3rd edition</phrase>.
:this acclaimed revision of a classic <phrase>database systems</phrase> text offers a complete background in the basics of <phrase>database design</phrase>, languages, and system implementation. it provides the latest <phrase>information</phrase> combined with <phrase>real-world</phrase> examples to help readers <phrase>master</phrase> concepts. all concepts are presented in a technically complete yet easy-to-understand style with notations kept to a minimum. a running example of a <phrase>bank</phrase> enterprise illustrates concepts at work. to further optimize comprehension, figures and examples, rather than proofs, portray concepts and anticipate <phrase>results</phrase>.
<phrase>database design</phrase>, revised <phrase>2nd edition</phrase>.
<phrase>database design</phrase>.
<phrase>database design</phrase>, second edition.
file organisation for <phrase>database design</phrase>.
extracting <phrase>reaction</phrase> <phrase>information</phrase> from chemical <phrase>databases</phrase>.
integrated learning in a real domain.
attribute-oriented induction in <phrase>relational databases</phrase>.
statistical technique for extracting classificatory <phrase>knowledge</phrase> from <phrase>databases</phrase>.
summary <phrase>data</phrase> estimation using decision <phrase>trees</phrase>.
<phrase>information</phrase> discovery through hierarchical maximum <phrase>entropy</phrase> <phrase>discretization</phrase> and synthesis.
using functions to encode domain and contextual <phrase>knowledge</phrase> in statistical induction.
<phrase>knowledge</phrase> discovery in <phrase>databases</phrase>: an overview.
the <phrase>trade</phrase>-off between <phrase>knowledge</phrase> and <phrase>data</phrase> in <phrase>knowledge</phrase> acquisition.
automated <phrase>knowledge</phrase> generation from a <phrase>cad</phrase> <phrase>database</phrase>.
incremental discovery of rules and structure by hierarchical and parallel clustering.
a support system for interpreting statistical <phrase>data</phrase>.
<phrase>mining</phrase> for <phrase>knowledge</phrase> in <phrase>databases</phrase>: goals and <phrase>general</phrase> description of the inlen system.
automating the discovery of causal relationships in a <phrase>medical</phrase> records <phrase>database</phrase>: the posch <phrase>ai</phrase> project.
induction of decision <phrase>trees</phrase> from complex structured <phrase>data</phrase>.
discovery of <phrase>medical</phrase> diagnostic <phrase>information</phrase>: an overview of methods and <phrase>results</phrase>.
<phrase>knowledge</phrase> discovery as a threat to <phrase>database</phrase> <phrase>security</phrase>.
minimal-length encoding and inductive inference.
discovery, analysis, and presentation of strong rules
on evaluation of domain-<phrase>independent</phrase> scientific <phrase>function</phrase>-finding systems.
justification-based refinement of expert <phrase>knowledge</phrase>.
rule discovery for <phrase>query optimization</phrase>.
unsupervised discovery in an operational control setting.
rule induction using <phrase>information theory</phrase>.
learning useful rules from inconclusive <phrase>data</phrase>.
integration of <phrase>heuristic</phrase> and <phrase>bayesian</phrase> approaches in a pattern-classification system.
discovering functional relationships from observational <phrase>data</phrase>.
on <phrase>linguistic</phrase> summaries of <phrase>data</phrase>.
the discovery, analysis, and representation of <phrase>data</phrase> dependencies in <phrase>databases</phrase>.
interactive <phrase>mining</phrase> of regularities in <phrase>databases</phrase>.
fast discovery of association rules.
predicting equity returns from securities <phrase>data</phrase>.
finding patterns in time series: a <phrase>dynamic programming</phrase> approach.
the process of <phrase>knowledge</phrase> discovery in <phrase>databases</phrase>.
<phrase>graphical</phrase> models for discivering <phrase>knowledge</phrase>.
<phrase>bayesian</phrase> classification (autoclass): theory and <phrase>results</phrase>.
<phrase>inductive logic programming</phrase> and <phrase>knowledge</phrase> discovery in <phrase>databases</phrase>.
a statistical perspective on <phrase>knowledge</phrase> discovery in <phrase>databases</phrase>.
automating the analysis and cataloging of sky surveys.
from <phrase>data mining</phrase> to <phrase>knowledge</phrase> discovery: an overview.
transformation rules and <phrase>trees</phrase>.
discovering informative patterns and <phrase>data</phrase> cleaning.
attribute-oriented induction in <phrase>data mining</phrase>.
<phrase>bayesian</phrase> networks for <phrase>knowledge</phrase> discovery.
<phrase>data</phrase> <phrase>surveyor</phrase>: searching the <phrase>nuggets</phrase> in parallel.
using inductive learning to generate rules for <phrase>semantic</phrase> <phrase>query optimization</phrase>.
explora: a multipattern and multistrategy discovery assistant.
<phrase>knowledge</phrase> discovery in <phrase>database</phrase> terminology.
selecting and reporting what is interesting.
<phrase>data mining</phrase> and <phrase>knowledge</phrase> discovery <phrase>internet</phrase> resources.
metaqueries for <phrase>data mining</phrase>.
integrating inductive and <phrase>deductive reasoning</phrase> for <phrase>data mining</phrase>.
modeling subjective uncertainty in image annotation.
from <phrase>data mining</phrase> to <phrase>knowledge</phrase> discovery: current challenges and future directions.
from contingency tables to various forms of <phrase>knowledge</phrase> in <phrase>databases</phrase>.
conceptual modeling of workflows.
a behaviorally driven approach to <phrase>object-oriented analysis and design</phrase> with <phrase>object oriented</phrase> <phrase>data modeling</phrase>.
coordinated collaboration of objects.
identifying objects by declarative queries.
temporally enhanced <phrase>database design</phrase>.
an active, <phrase>object-oriented</phrase>, <phrase>model</phrase>-equivalent <phrase>programming language</phrase>.
mapping an extended entity-relationship into a schema of complex objects.
advances in <phrase>object-oriented</phrase> modeling.
leveraging <phrase>relational</phrase> <phrase>data</phrase> assets.
modeling object dynamics.
<phrase>database</phrase> integration: the key to <phrase>data</phrase> <phrase>interoperability</phrase>.
on the <phrase>design</phrase> of behavior consistent specializations of object <phrase>life</phrase> cycles in obd and <phrase>uml</phrase>.
objects and events as modeling drivers.
the type system of lml.
a regular type <phrase>language</phrase> for <phrase>logic</phrase> programs.
<phrase>logic programming</phrase> with type specifications.
<phrase>semantic</phrase> types for <phrase>logic</phrase> programs.
a <phrase>semantics</phrase> for typed <phrase>logic</phrase> programs.
a pragmatic view of types for <phrase>logic</phrase> programs.
the type system of a <phrase>higher-order logic</phrase> <phrase>programming language</phrase>.
the type system of the <phrase>logic</phrase> <phrase>programming language</phrase> $\lambda$<phrase>prolog</phrase> is discussed. the incorporation of higher-<phrase>order</phrase> notions within this <phrase>language</phrase> requires the use of a typing scheme to distinguish between expressions of different <phrase>function</phrase> types. thus $\lambda$<phrase>prolog</phrase> is a (strongly) typed <phrase>language</phrase>, in contrast to the typeless <phrase>language</phrase> that underlies the idea of descriptive types in <phrase>logic programming</phrase>. the typing discipline that is employed in the <phrase>language</phrase> is based on the notion of simple types in the $\lambda$-<phrase>calculus</phrase>. this form of typing enforces <phrase>arity</phrase> restrictions on functions and predicates and provides a builtin functional hierarchy over terms. the <phrase>language</phrase> contains a facility for defining new primitive types and thus permits finer grained distinctions to be introduced by the user. further, the use of type variables and type <phrase>constructors</phrase> provides a form of polymorphism that is similar in certain respects to that present in the <phrase>language</phrase> ml. the notion of type checking in $\lambda$<phrase>prolog</phrase> is discussed and shown to be an operation that can be performed at the time of <phrase>compilation</phrase>. the value of typing distinctions in determining the clarity of programs and their usefulness in conjunction with type checking in preventing run-time errors due to type violations is also discussed. in addition to their <phrase>function</phrase> in type checking, types also have a <phrase>major</phrase> role in determining computations in the <phrase>logic programming</phrase> context. we discuss this <phrase>aspect</phrase> of types that is in contrast to their behavior in other <phrase>programming</phrase> paradigms and we show how this leads to a presence of types in the runtime environment. while typing has several advantages, it is sometimes to the <phrase>programmer</phrase>''s advantage to be able to omit their mention. type <phrase>reconstruction</phrase> provides a means for filling missing type <phrase>information</phrase> in and we discuss issues pertinent to this process in the context of $\lambda$<phrase>prolog</phrase>.
types and the intended meaning of <phrase>logic</phrase> programs.
dependent types in <phrase>logic programming</phrase>.
polymorphically typed <phrase>logic</phrase> programs.
using moded type systems to support abstraction in <phrase>logic</phrase> programs.
<phrase>actors</phrase>: a conceptual foundation for concurrent <phrase>object-oriented programming</phrase>.
groundwork for an <phrase>object database</phrase> <phrase>model</phrase>.
definition groups: making sources into first-class objects.
<phrase>object-oriented</phrase> specifications.
unifying functional, <phrase>object-oriented</phrase> and <phrase>relational</phrase> <phrase>programming</phrase> with logical <phrase>semantics</phrase>.
a <phrase>model</phrase> for object-based <phrase>inheritance</phrase>.
vulcan: logical concurrent objects.
the beta <phrase>programming language</phrase>.
block-structure and <phrase>object-oriented</phrase> languages.
development and implementation of an <phrase>object-oriented</phrase> <phrase>dbms</phrase>.
a mechanism for specifying the structure of large, layered systems.
an <phrase>object-oriented</phrase> framework for conceptual <phrase>programming</phrase>.
type <phrase>evolution</phrase> in <phrase>an object-oriented database</phrase>.
a substrate for <phrase>object-oriented</phrase> <phrase>interface design</phrase>.
<phrase>inheritance</phrase> and the development of encapsulated <phrase>software</phrase> systems.
the <phrase>object-oriented</phrase> classification <phrase>paradigm</phrase>.
<phrase>workflow</phrase> <phrase>management</phrase>: models, methods, and systems
structure and interpretation of computer programs
structure and interpretation of computer programs, second edition
warren's <phrase>abstract machine</phrase>: a tutorial <phrase>reconstruction</phrase>
<phrase>digital library</phrase> use: social practice in <phrase>design</phrase> and evaluation
visual <phrase>reconstruction</phrase>
<phrase>vector</phrase> models for <phrase>data</phrase>-<phrase>parallel computing</phrase>
from gutenberg to the global <phrase>information</phrase> <phrase>infrastructure</phrase>
advances in <phrase>knowledge</phrase> discovery and <phrase>data mining</phrase>.
introduction to <phrase>object-oriented</phrase> <phrase>databases</phrase>
advances in <phrase>object-oriented</phrase> <phrase>data modeling</phrase>
<phrase>knowledge</phrase> discovery in <phrase>databases</phrase>
concepts, techniques, and models of computer <phrase>programming</phrase>
the <phrase>art</phrase> of <phrase>prolog</phrase> - advanced <phrase>programming</phrase> techniques
the <phrase>art</phrase> of <phrase>prolog</phrase> - advanced <phrase>programming</phrase> techniques, 2nd ed.
the intellectual foundation of <phrase>information</phrase> <phrase>organization</phrase>
instant <phrase>electronic</phrase> access to <phrase>digital</phrase> <phrase>information</phrase> is the <phrase>single</phrase> most distinguishing attribute of the <phrase>information age</phrase>. the elaborate retrieval mechanisms that support such access are a product of <phrase>technology</phrase>. but <phrase>technology</phrase> is not enough. the effectiveness of a system for accessing <phrase>information</phrase> is a direct <phrase>function</phrase> of the <phrase>intelligence</phrase> put into organizing it. just as the practical field of <phrase>engineering</phrase> has <phrase>theoretical physics</phrase> as its underlying base, the <phrase>design</phrase> of systems for organizing <phrase>information</phrase> rests on an intellectual foundation. the subject of this <phrase>book</phrase> is the systematized body of <phrase>knowledge</phrase> that constitutes this foundation.integrating the disparate disciplines of descriptive cataloging, subject cataloging, indexing, and classification, the <phrase>book</phrase> adopts a <phrase>conceptual framework</phrase> that views the process of organizing <phrase>information</phrase> as the use of a special <phrase>language</phrase> of description called a bibliographic <phrase>language</phrase>. the <phrase>book</phrase> is divided into two parts. the first part is an analytic discussion of the intellectual foundation of <phrase>information</phrase> <phrase>organization</phrase>. the second part moves from generalities to particulars, presenting an overview of three bibliographic languages: work languages, document languages, and subject languages. it looks at these languages in terms of their <phrase>vocabulary</phrase>, <phrase>semantics</phrase>, and <phrase>syntax</phrase>. the <phrase>book</phrase> is written in an exceptionally clear style, at <phrase>a level</phrase> that makes it understandable to those outside the discipline of <phrase>library</phrase> and <phrase>information</phrase> science.digital <phrase>libraries</phrase> and <phrase>electronic publishing</phrase> series
<phrase>research</phrase> directions in <phrase>object-oriented programming</phrase>
journey to <phrase>data quality</phrase>
all organizations today confront <phrase>data quality</phrase> problems, both systemic and structural. neither <phrase>ad hoc</phrase> approaches nor fixes at the systems levelinstalling the latest <phrase>software</phrase> or developing an expensive <phrase>data</phrase> warehousesolve the <phrase>basic</phrase> problem of bad <phrase>data quality</phrase> practices. journey to <phrase>data quality</phrase> offers a roadmap that can be used by practitioners, executives, and students for planning and implementing a viable <phrase>data</phrase> and <phrase>information</phrase> <phrase>quality management</phrase> program. this practical guide, based on rigorous <phrase>research</phrase> and informed by <phrase>real-world</phrase> examples, describes the challenges of <phrase>data management</phrase> and provides the principles, strategies, tools, and techniques necessary to meet them. the authors, all leaders in the <phrase>data quality</phrase> field for many years, discuss how to make the economic case for <phrase>data quality</phrase> and the importance of getting an organization's leaders on board. they outline different approaches for assessing <phrase>data</phrase>, both subjectively (by users) and objectively (using sampling and other techniques). they describe real problems and solutions, including efforts to find the <phrase>root</phrase> causes of <phrase>data quality</phrase> problems at a <phrase>healthcare</phrase> <phrase>organization</phrase> and <phrase>data quality</phrase> initiatives taken by a large <phrase>teaching hospital</phrase>. they address setting <phrase>company</phrase> policy on <phrase>data quality</phrase> and, finally, they consider future challenges on the journey to <phrase>data quality</phrase>.
self-stabilization
heterogenous active agents
<phrase>digital libraries</phrase>
datenbanken: konzepte und sprachen
datenbanken: implementierungstechniken
datenbanken: konzepte und sprachen, 3. auflage
object identity as a <phrase>query language</phrase> primitive.
we demonstrate the power of object identities (oid's) as a <phrase>database</phrase> <phrase>query language</phrase> primitive. we develop an object-based <phrase>data model</phrase>, whose structural part generalizes most of the known complex-object <phrase>data</phrase> models: cyclicity is allowed in both its schemas and instances. our main contribution is the operational part of the <phrase>data model</phrase>, the <phrase>query language</phrase> iql, which uses oid's for three critical purposes: (1) to represent <phrase>data-structures</phrase> with sharing and cycles, (2) to manipulate sets and (3) to express any <phrase>computable</phrase> <phrase>database</phrase> query. iql can be statically type checked, can be evaluated bottom-up and naturally generalizes most popular rule-based <phrase>database</phrase> languages. the <phrase>model</phrase> can also be extended to incorporate type <phrase>inheritance</phrase>, without changes to iql. finally, we investigate an analogous value-based <phrase>data model</phrase>, whose structural part is founded on regular infinite <phrase>trees</phrase> and whose operational part is iql.
method schemas.
the concept of method schemas is proposed as a simple <phrase>model</phrase> for <phrase>object-oriented programming</phrase> with features such as classes with methods and <phrase>inheritance</phrase>, method name overloading, and late binding. an important issue is to check whether a given method schema can possibly <phrase>lead</phrase> to inconsistencies in some interpretations. the consistency problem for method schemas is studied. the problem is shown to be <phrase>undecidable</phrase> in <phrase>general</phrase>. decidability is obtained for monadic and/or <phrase>recursion</phrase>-<phrase>free</phrase> method schemas. the effect of <phrase>covariance</phrase> is considered. the issues of incremental consistency checking and of a <phrase>sound</phrase> <phrase>algorithm</phrase> for the <phrase>general</phrase> case are briefly discussed.
self-explained toolboxes.
the <phrase>object-oriented</phrase> <phrase>database</phrase> system <phrase>manifesto</phrase>.
a <phrase>query language</phrase> for <phrase>o2</phrase>.
introduction to languages.
<phrase>lisp</phrase> <phrase>o2</phrase>: a persistent <phrase>object-oriented</phrase> <phrase>lisp</phrase>.
clustering strategies in <phrase>o2</phrase>: an overview.
handling distribution in the <phrase>o2</phrase> system.
the <phrase>o2</phrase> <phrase>programming</phrase> environment.
integrating <phrase>concurrency control</phrase> into <phrase>an object-oriented database</phrase> system.
consistency of versions in <phrase>object-oriented</phrase> <phrase>databases</phrase>.
reloop, an <phrase>algebra</phrase> based <phrase>query language</phrase> for <phrase>an object-oriented database</phrase> system.
three <phrase>alternative</phrase> <phrase>workstation</phrase>-server architectures.
introduction to the <phrase>programming</phrase> environment.
introduction to the system.
the story of <phrase>o2</phrase>.
using a <phrase>database</phrase> system to implement a <phrase>debugger</phrase>.
using <phrase>database</phrase> applications to compare <phrase>programming languages</phrase>.
a guided <phrase>tour</phrase> of an <phrase>o2</phrase> applications.
introduction to the <phrase>data model</phrase>.
incremental <phrase>compilation</phrase> in <phrase>o2</phrase>.
the <phrase>o2</phrase> <phrase>database</phrase> <phrase>programming language</phrase>.
<phrase>o2</phrase>, an <phrase>object-oriented</phrase> <phrase>data model</phrase>.
the <phrase>altair</phrase> group is currently designing an <phrase>object-oriented</phrase> <phrase>data</phrase> base system called <phrase>o2</phrase>. this <phrase>paper</phrase> presents a formal description of the <phrase>object-oriented</phrase> <phrase>data model</phrase> of this system. it proposes a type system defined in the framework of a set-and-<phrase>tuple</phrase> <phrase>data model</phrase>. it models the well known <phrase>inheritance</phrase> mechanism and enforces strong typing.
building <phrase>user interfaces</phrase> with looks.
geographic applications: an experience with <phrase>o2</phrase>.
the <phrase>o2</phrase> object <phrase>manager</phrase>: an overview.
a framework for schema updates in <phrase>an object-oriented database</phrase> system.
all your <phrase>data</phrase>: the <phrase>oracle</phrase> <phrase>extensibility</phrase> <phrase>architecture</phrase>.
enabling component <phrase>databases</phrase> with <phrase>ole db</phrase>.
the <phrase>architecture</phrase> of a <phrase>database</phrase> system for <phrase>mobile</phrase> and <phrase>embedded devices</phrase>.
distributed component <phrase>database management systems</phrase>.
extensible indexing support in <phrase>db2</phrase> <phrase>universal</phrase> <phrase>database</phrase>.
component <phrase>database systems</phrase>: introduction, foundations, and overview.
conclusions and perspectives.
foreword.
building component <phrase>database systems</phrase> using <phrase>corba</phrase>.
an <phrase>architecture</phrase> for transparent access to diverse <phrase>data</phrase> sources.
transaction <phrase>management</phrase> in <phrase>database systems</phrase>
a transaction <phrase>model</phrase> for active distributed object systems
acta: the <phrase>saga</phrase> continues
acta is a <phrase>comprehensive</phrase> transaction framework that permits a transaction modeler to specify the effects of extended transactions on each other and on objects in the <phrase>database</phrase>. acta allows the specification of (1) the interactions between transactions in terms of relationships between significant (transaction manage- ment) events, such as begin, commit, abort, delegate, <phrase>split</phrase>, and join, pertaining to different transactions and (2) transactions'' effects on objects'' <phrase>state</phrase> and concur- rency status (i.e., synchronization <phrase>state</phrase>). various extended traditional models have been proposed to deal with applica- tions that involve reactive (endless), open-ended (<phrase>long</phrase>-lived) and collaborative (interactive) activities. one such <phrase>model</phrase> is sagas [gs87] <phrase>independent</phrase> (component) transactions t1, t2,..., <phrase>tn</phrase> which can interleave in any way with component trans- actions of other sagas. components can commit even before the <phrase>saga</phrase> commits. however, if the <phrase>saga</phrase> subsequently aborts, effects of the committed components are nullified through the <phrase>invocation</phrase> of compensating transactions. after giving a brief introduction to the modeling primitives of acta, we illustrate their use by giving a complete formal characterization of sagas. sub- sequently, the reasoning power of acta is shown by proving properties of sagas. finally, the flexibility of acta is displayed through a series of variations to the original <phrase>model</phrase> of sagas, each variation coming out of changes to the formal characterization of sagas.
introduction to advanced transaction models
a flexible framework for transaction <phrase>management</phrase> in <phrase>engineering</phrase> environments
dynamic restructuring of transactions
multidatabase transaction and <phrase>query processing</phrase> in <phrase>logic</phrase>
a transaction <phrase>model</phrase> for an open publication environment
a <phrase>cooperative</phrase> transaction <phrase>model</phrase> for <phrase>design</phrase> <phrase>databases</phrase>
using polytransactions to manage interdependent <phrase>data</phrase>
facility for non standard <phrase>database systems</phrase>
the s-transaction <phrase>model</phrase>
the <phrase>contract</phrase> <phrase>model</phrase>
concepts and applications of multilevel transactions and open nested transactions
an analysis of a dynamic <phrase>query optimization</phrase> scheme for different <phrase>data</phrase> <phrase>distributions</phrase>.
a survey of <phrase>indexing techniques</phrase> for <phrase>object-oriented</phrase> <phrase>database management systems</phrase>.
towards a unification of rewrite-based optimization techniques for <phrase>object-oriented</phrase> queries.
<phrase>algebraic</phrase> <phrase>query optimization</phrase> in the cooms structurally <phrase>object-oriented</phrase> <phrase>database</phrase> system.
extensible <phrase>query optimization</phrase> and parallel execution in <phrase>volcano</phrase>.
tagging as an <phrase>alternative</phrase> to object creation.
<phrase>query optimization</phrase> in <phrase>deductive</phrase> object bases.
<phrase>query optimization</phrase> in object bases: exploiting <phrase>relational</phrase> techniques.
adt-based type system for <phrase>sql</phrase>.
evaluation aspects of an <phrase>object-oriented</phrase> <phrase>deductive</phrase> <phrase>database</phrase> <phrase>language</phrase>.
recently, f-<phrase>logic</phrase> has been proposed as an attempt to extend <phrase>deductive</phrase> <phrase>databases</phrase> by typical concepts of <phrase>object-oriented</phrase> languages. among these concepts are complex objects, (term-based) object identity, methods, classes, typing, <phrase>inheritance</phrase> and browsing. in kifer et al. <phrase>syntax</phrase> and <phrase>model</phrase>-theoretic <phrase>semantics</phrase> is discussed; however many algorithmic aspects which arise when <phrase>computing</phrase> the corresponding models are left open. in this <phrase>paper</phrase> we start to <phrase>bridge</phrase> this gap. several topics in the context of the evaluation of programs are discussed in detail; among these are weak <phrase>recursion</phrase>, global <phrase>stratification</phrase> and dynamic type-checking.
challenges for <phrase>query processing</phrase> in <phrase>object-oriented</phrase> <phrase>databases</phrase>.
integration of composite objects into <phrase>relational</phrase> <phrase>query processing</phrase>: the <phrase>sql</phrase>/xnf approach.
physical <phrase>database design</phrase> for <phrase>an object-oriented database</phrase> system.
optimization of complex-object queries in prima - statement of problems.
implementation of the <phrase>object-oriented</phrase> <phrase>data model</phrase> tm.
an <phrase>engineering</phrase> <phrase>database</phrase> benchmark.
the <phrase>wisconsin</phrase> benchmark: past, present, and future.
benchmark <phrase>software</phrase> distribution: release 1.0.
introduction.
do we all look at <phrase>homeless</phrase> people in the same way? the innocence of kids can teach us how to look at the world in a different way.
the neal nelson <phrase>database</phrase> benchmark: a benchmark based on the realities of <phrase>business</phrase>.
the set query benchmark.
doing your own benchmark.
the <phrase>history</phrase> of debitcredit and the tpc.
tpc benchmark a: standard specification.
tpc benchmark b: standard specification.
asap: an <phrase>ansi</phrase> <phrase>sql</phrase> standard scaleable and portable benchmark for <phrase>relational database</phrase> systems.
tpc benchmark b: standard specification.
doing your own benchmark.
tpc-d: <phrase>benchmarking</phrase> for <phrase>decision support</phrase>.
the <phrase>engineering</phrase> <phrase>database</phrase> benchmark.
overview of the full-text document retrieval benchmark.
the <phrase>wisconsin</phrase> benchmark: past, present, and future.
overview of the spec benchmarks.
<phrase>database</phrase> and <phrase>transaction processing</phrase> performance handbook.
the set query benchmark.
tpc-c - the standard benchmark for <phrase>online transaction processing</phrase> (oltp).
the <phrase>history</phrase> of debitcredit and the tpc.
asap - an <phrase>ansi</phrase> <phrase>sql</phrase> standard scaleable and portable benchmark for <phrase>relational database</phrase> systems.
the neal nelson <phrase>database</phrase> benchmark: a benchmark based on the realities of <phrase>business</phrase>.
tpc benchmark a: standard specification.
towards a theory of declarative <phrase>knowledge</phrase>.
performance evaluation of <phrase>data</phrase> intensive <phrase>logic</phrase> programs.
foundations of <phrase>semantic</phrase> <phrase>query optimization</phrase> for <phrase>deductive</phrase> <phrase>databases</phrase>
converting and-control to or-control by <phrase>program transformation</phrase>.
<phrase>negation</phrase> as failure using tight derivations for <phrase>general</phrase> <phrase>logic</phrase> programs.
compiling the gcwa in indefinite <phrase>deductive</phrase> <phrase>databases</phrase>.
intelligent query answering in rule based systems
<phrase>logic programming</phrase> and parallel complexity
unification revisited
on the declarative <phrase>semantics</phrase> of <phrase>logic</phrase> programs with <phrase>negation</phrase>.
equivalences of <phrase>logic</phrase> programs
a <phrase>logic</phrase>-based <phrase>language</phrase> for <phrase>database</phrase> updates.
introduction
<phrase>uml</phrase> is at a crossroads. which of the proposed revisions will bring it closer to meeting user needs and winning tool-vendor commitment? what if uml2 instead combined the best features of each proposal?
on the declarative <phrase>semantics</phrase> of <phrase>deductive</phrase> <phrase>databases</phrase> and <phrase>logic</phrase> programs.
a <phrase>theorem-proving</phrase> approach to <phrase>database</phrase> integrity.
optimizing <phrase>datalog</phrase> programs.
<phrase>datalog</phrase> programs, i.e., <phrase>prolog</phrase> programs without <phrase>function</phrase> symbols, are considered it is assumed that a <phrase>variable</phrase> appearing in the head of a rule must also appear in the body of the rule. the input of a program is a set of ground <phrase>atoms</phrase> (which are given in addition to the program's rules) and, therefore, can be viewed as an assignment of relations to some of the program's predicates. two programs are equivalent if they produce the same result for all possible assignments of relations to the <phrase>extensional</phrase> predicates (i.e., the predicates that do not appear as heads of rules). two programs are uniformly equivalent if they produce the same result for all possible assignments of initial relations to all the predicates (i.e., both <phrase>extensional</phrase> and intentional). the equivalence problem for <phrase>datalog</phrase> programs is known to be <phrase>undecidable</phrase>. it is shown that uniform equivalence is <phrase>decidable</phrase>, and an <phrase>algorithm</phrase> is given for minimizing a <phrase>datalog</phrase> program under uniform equivalence. a technique for removing parts of a program that are redundant under equivalence (but not under uniform equivalence) is developed. a procedure for testing uniform equivalence is also developed for the case in which the <phrase>database</phrase> satisfies some constraints.
<phrase>negation</phrase> in <phrase>logic programming</phrase>.
a superjoin <phrase>algorithm</phrase> for <phrase>deductive</phrase> <phrase>databases</phrase>.
on domain <phrase>independent</phrase> <phrase>databases</phrase>.
<phrase>middleware</phrase> for <phrase>location-based services</phrase>.
<phrase>database</phrase> aspects of <phrase>location-based services</phrase>.
<phrase>data transmission</phrase> in <phrase>mobile</phrase> <phrase>communication</phrase> systems.
lbs <phrase>interoperability</phrase> through standards.
<phrase>data</phrase> collection.
introduction.
<phrase>navigation</phrase> systems: a spatial <phrase>database</phrase> perspective.
<phrase>general</phrase> aspects of <phrase>location based services</phrase>.
<phrase>case study</phrase>: the development of the find <phrase>friends</phrase> application.
active rule <phrase>management</phrase> in chimera
the hipac project
active <phrase>database</phrase> facilities in ode
the ariel project
the postgres rules system
the a-rdl system
the starburst rule system
introduction to active <phrase>database systems</phrase>
standards and commercial systems
applications of active <phrase>databases</phrase>
conclusions and future directions
introduction to spatial <phrase>databases</phrase>: applications to <phrase>gis</phrase>
optimizing compilers for modern architectures: a dependence-<phrase>based approach</phrase>
<phrase>universal</phrase> <phrase>database management</phrase>: a guide to <phrase>object/relational</phrase> <phrase>technology</phrase>
building <phrase>an object-oriented database</phrase> system, the story of <phrase>o2</phrase>
transactional <phrase>information</phrase> systems: theory, <phrase>algorithms</phrase>, and the practice of <phrase>concurrency control</phrase> and recovery
principles of <phrase>transaction processing</phrase> for systems professionals.
computer systems that learn: classification and prediction methods from <phrase>statistics</phrase>, neural nets, <phrase>machine learning</phrase> and <phrase>expert systems</phrase>
migrating legacy systems: gateways, interfaces, and the incremental approach
<phrase>data warehousing</phrase>: using the <phrase>wal-mart</phrase> <phrase>model</phrase>
<phrase>data</phrase> on the web: from relations to semistructured <phrase>data</phrase> and <phrase>xml</phrase>
<phrase>stochastic</phrase> local search: foundations & applications
the object <phrase>data</phrase> standard: odmg 3.0
how to build a <phrase>digital</phrase> libary
:given modern society's need to control its ever-increasing body of <phrase>information</phrase>, <phrase>digital libraries</phrase> will be among the most important and influential institutions of this century. with their versatility, accessibility, and <phrase>economy</phrase>, these focused collections of everything <phrase>digital</phrase> are fast becoming the "<phrase>banks</phrase>" in which the world's wealth of <phrase>information</phrase> is stored. how to build a <phrase>digital library</phrase> is the only <phrase>book</phrase> that offers all the <phrase>knowledge</phrase> and tools needed to construct and maintain a <phrase>digital library</phrase>-no <phrase>matter</phrase> how large or small. two internationally recognized experts provide a fully developed, step-by-step method, as well as the <phrase>software</phrase> that makes it all possible. how to build a <phrase>digital library</phrase> is the perfectly self-contained resource for individuals, agencies, and institutions wishing to put this powerful tool to work in their burgeoning <phrase>information</phrase> treasuries. features sketches the <phrase>history</phrase> of <phrase>libraries</phrase>-both traditional and <phrase>digital</phrase>-and their impact on present practices and future directions offers in-depth coverage of today's practical standards used to represent and store <phrase>information</phrase> digitally uses greenstone, freely accessible <phrase>open-source software</phrase>-available with interfaces in the world's <phrase>major</phrase> languages (including <phrase>spanish</phrase>, <phrase>chinese</phrase>, and <phrase>arabic</phrase>) written for both technical and non-technical audiences web-enhanced with <phrase>software</phrase> documentation, color illustrations, full-text index, <phrase>source code</phrase>, and more <phrase>author</phrase> biography: ian h. <phrase>witten</phrase> is a <phrase>professor</phrase> of computer <phrase>science</phrase> at the <phrase>university</phrase> of <phrase>waikato</phrase> in new <phrase>zealand</phrase>. he directs the new <phrase>zealand</phrase> <phrase>digital library</phrase> <phrase>research</phrase> project. his <phrase>research</phrase> interests include <phrase>information retrieval</phrase>, <phrase>machine learning</phrase>, text compression, and <phrase>programming</phrase> by demonstration. he received an <phrase>ma</phrase> in <phrase>mathematics</phrase> from <phrase>cambridge university</phrase>, <phrase>england</phrase>; an <phrase>msc</phrase> in computer <phrase>science</phrase> from the <phrase>university</phrase> of <phrase>calgary</phrase>, <phrase>canada</phrase>; and a <phrase>phd</phrase> in <phrase>electrical engineering</phrase> from <phrase>essex university</phrase>, <phrase>england</phrase>. he is a <phrase>fellow</phrase> of the <phrase>acm</phrase> and of the <phrase>royal society</phrase> of new <phrase>zealand</phrase>. he has published widely on <phrase>digital libraries</phrase>, <phrase>machine learning</phrase>, text compression, <phrase>hypertext</phrase>, <phrase>speech synthesis</phrase> and <phrase>signal processing</phrase>, and computer <phrase>typography</phrase>. he has written several books, the latest being managing gigabytes (1999) and <phrase>data mining</phrase> (2000), both from morgan kaufmann. <phrase>david</phrase> bainbridge is a senior <phrase>lecturer</phrase> in computer <phrase>science</phrase> at the <phrase>university</phrase> of <phrase>waikato</phrase>, new <phrase>zealand</phrase>. he holds a <phrase>phd</phrase> in optical <phrase>music</phrase> recognition from the <phrase>university</phrase> of <phrase>canterbury</phrase>, new <phrase>zealand</phrase> where he studied as a <phrase>commonwealth</phrase> <phrase>scholar</phrase>. since moving to <phrase>waikato</phrase> in 1996 he has continued to broadened his interest in <phrase>digital media</phrase>, while retaining a particular emphasis on <phrase>music</phrase>. an active <phrase>member</phrase> of the new <phrase>zealand</phrase> <phrase>digital library</phrase> project, he manages the group's <phrase>digital</phrase> <phrase>music</phrase> <phrase>library</phrase>, meldex, and has collaborated with several <phrase>united nations</phrase> agencies, the <phrase>bbc</phrase> and various <phrase>public libraries</phrase>. <phrase>david</phrase> has also worked as a <phrase>research</phrase> <phrase>engineer</phrase> for <phrase>thorn emi</phrase> in the <phrase>area</phrase> of photo-realistic imaging and graduated from the <phrase>university</phrase> of <phrase>edinburgh</phrase> in 1991 as the class medalist in computer <phrase>science</phrase>.
the <phrase>object database</phrase> standard: odmg-93
<phrase>data mining</phrase>: practical <phrase>machine learning</phrase> tools and techniques with <phrase>java</phrase> implementations
the <phrase>object database</phrase> standard: odmg-93 (release 1.1)
managing gigabytes: compressing and indexing documents and images, second edition
the <phrase>object database</phrase> standard: odmg-93 (release 1.2)
priniples of <phrase>database</phrase> <phrase>query processing</phrase> for advanced applications
the <phrase>object database</phrase> standard: odmg 2.0
advanced <phrase>database systems</phrase>.
<phrase>sql</phrase> for <phrase>smarties</phrase>: advanced <phrase>sql</phrase> <phrase>programming</phrase>
readings in <phrase>object-oriented</phrase> <phrase>database systems</phrase>
joe celko's <phrase>sql</phrase> puzzles & answers
active <phrase>database systems</phrase>: triggers and rules for advanced <phrase>database</phrase> processing.
<phrase>sql</phrase> for <phrase>smarties</phrase>: advanced <phrase>sql</phrase> <phrase>programming</phrase>, second edition
practical file system <phrase>design</phrase> with the be file system
joe celko's <phrase>data</phrase> and <phrase>databases</phrase>: concepts in practice
unleashing <phrase>web 2.0</phrase>: from concepts to <phrase>creativity</phrase>
vossen is both an is & cs <phrase>professor</phrase> at the <phrase>university</phrase> of muenster, and also served as <phrase>european</phrase> <phrase>editor-in-chief</phrase> for elseviers international <phrase>information</phrase> systems <phrase>journal</phrase>. hagemann is his <phrase>phd</phrase> <phrase>student</phrase> whose <phrase>area</phrase> of <phrase>research</phrase> is web <phrase>technology</phrase>.
using the new <phrase>db2</phrase>: ibm's <phrase>object-relational</phrase> <phrase>database</phrase> system.
a complete guide to <phrase>db2</phrase> <phrase>universal</phrase> <phrase>database</phrase>
<phrase>engineering</phrase> a <phrase>compiler</phrase>
fuzzy modeling tools for <phrase>data mining</phrase> and <phrase>knowledge</phrase> discovery: <phrase>knowledge</phrase> discovery, fuzzy rule induction, and autonomous agents for <phrase>databases</phrase> and <phrase>spreadsheets</phrase>
component <phrase>database systems</phrase>
<phrase>database transaction</phrase> models for advanced applications.
<phrase>camelot</phrase> and <phrase>avalon</phrase>: a distributed transaction facility
<phrase>information visualization</phrase> in <phrase>data mining</phrase> and <phrase>knowledge</phrase> discovery
<phrase>information visualization</phrase> in <phrase>data mining</phrase> and <phrase>knowledge</phrase> discovery
<phrase>database</phrase>-driven web sites
<phrase>query processing</phrase> for advanced <phrase>database systems</phrase>, selected contributions from a workshop on "<phrase>query processing</phrase> in <phrase>object-oriented</phrase>, complex-object and nested relation <phrase>databases</phrase>", interationales begegnungs- und forschungszentrum <phrase>fr</phrase> informatik, schloss dagstuhl, <phrase>germany</phrase>, june 1991
essentials of <phrase>artificial intelligence</phrase>
the benchmark handbook for <phrase>database</phrase> and transaction systems (1st edition).
the benchmark handbook for <phrase>database</phrase> and transaction systems (<phrase>2nd edition</phrase>).
<phrase>transaction processing</phrase>: concepts and techniques
<phrase>moving objects</phrase> <phrase>databases</phrase>
web farming for the <phrase>data warehouse</phrase>
<phrase>data mining</phrase>: concepts and techniques
computer <phrase>architecture</phrase>: a quantitative approach, <phrase>2nd edition</phrase>
recovery in parallel <phrase>database systems</phrase>, second edition
web <phrase>dragons</phrase>: inside the myths of search <phrase>engine</phrase> <phrase>technology</phrase>
a guide to developing client/server <phrase>sql</phrase> applications
the <phrase>jasmine</phrase> <phrase>object database</phrase>: <phrase>multimedia</phrase> applications for the web
elements of <phrase>machine learning</phrase>.
practical <phrase>digital libraries</phrase>: books, <phrase>bytes</phrase>, and <phrase>bucks</phrase>
distributed <phrase>algorithms</phrase>
:in distributed <phrase>algorithms</phrase>, <phrase>nancy</phrase> lynch provides a blueprint for designing, implementing, and analyzing distributed <phrase>algorithms</phrase>. she directs her <phrase>book</phrase> at a wide audience, including students, programmers, system designers and researchers. distributed <phrase>algorithms</phrase> contains the most significant <phrase>algorithms</phrase> and impossibility <phrase>results</phrase> in the <phrase>area</phrase>, all in a simple <phrase>automata</phrase>-theoretic setting. the <phrase>algorithms</phrase> are proved correct, and their complexity is analyzed according to precisely defined complexity measures. the problems <phrase>covered</phrase> include <phrase>resource allocation</phrase>, <phrase>communication</phrase>, consensus among distributed processes, <phrase>data</phrase> consistency, <phrase>deadlock</phrase> detection, leader <phrase>election</phrase>, global snapshots, and many others. the material is organized according to the system <phrase>model</phrase> - first by the timing <phrase>model</phrase> and then by the interprocess <phrase>communication</phrase> mechanism. the material on system models is isolated in separate chapters for easy reference. the presentation is completely rigorous, yet is intuitive enough for immediate comprehension. this <phrase>book</phrase> familiarizes readers with important problems, <phrase>algorithms</phrase>, and impossibility <phrase>results</phrase> in the <phrase>area</phrase>: readers can then recognize the problems when they arise in practice, apply the <phrase>algorithms</phrase> to solve them, and use the impossibility <phrase>results</phrase> to determine whether problems are unsolvable. the <phrase>book</phrase> also provides readers with the <phrase>basic</phrase> <phrase>mathematical</phrase> tools for designing new <phrase>algorithms</phrase> and proving new impossibility <phrase>results</phrase>. in addition, it teaches readers how to reason carefully about distributed <phrase>algorithms</phrase> - to <phrase>model</phrase> them formally, devise precise specifications for their required behavior, prove their correctness, and evaluate their performance with realistic measures.
atomic transactions
understanding <phrase>sql</phrase> and <phrase>java</phrase> together: a guide to sglj, <phrase>jdbc</phrase>, and related technologies
understanding the new <phrase>sql</phrase>: a complete guide, second edition, volume i
understanding <phrase>sql</phrase> stored procedures: a complete guide to <phrase>sql</phrase>/<phrase>psm</phrase>
understanding the new <phrase>sql</phrase>: a complete guide
foundations of <phrase>deductive</phrase> <phrase>databases</phrase> and <phrase>logic programming</phrase>
advanced <phrase>compiler</phrase> <phrase>design</phrase> and implementation
commonsense reasoning
<phrase>database design</phrase> for <phrase>smarties</phrase>: using <phrase>uml</phrase> for <phrase>data modeling</phrase>
<phrase>machine learning</phrase>: a theoretical approach.
<phrase>database</phrase> principles, <phrase>programming</phrase>, performance
<phrase>database</phrase>: principles, <phrase>programming</phrase>, and performance, second edition
<phrase>data quality</phrase>: the accuracy <phrase>dimension</phrase>
computer <phrase>architecture</phrase>: a quantitative approach.
computer <phrase>organization</phrase> & <phrase>design</phrase>: the hardware/<phrase>software</phrase> interface
computer <phrase>organization</phrase> & <phrase>design</phrase>: the hardware/<phrase>software</phrase> interface, second edition
mico: an <phrase>open source</phrase> <phrase>corba</phrase> implementation
<phrase>data</phrase> preparation for <phrase>data mining</phrase>
c4.5: programs for <phrase>machine learning</phrase>
:classifier systems <phrase>play</phrase> a <phrase>major</phrase> role in <phrase>machine learning</phrase> and <phrase>knowledge-based systems</phrase>, and ross quinlan's work on <phrase>id3</phrase> and c4.5 is widely acknowledged to have made some of the most significant contributions to their development. this <phrase>book</phrase> is a complete guide to the c4.5 system as implemented in c for the <phrase>unix</phrase> environment. it contains a <phrase>comprehensive</phrase> guide to the system's use , the <phrase>source code</phrase> (about 8,800 lines), and implementation notes. the <phrase>source code</phrase> and sample datasets are also available on a 3.5-inch <phrase>floppy</phrase> <phrase>diskette</phrase> for a <phrase>sun</phrase> <phrase>workstation</phrase>. c4.5 starts with large sets of cases belonging to known classes. the cases, described by any mixture of nominal and numeric properties, are scrutinized for patterns that allow the classes to be reliably discriminated. these patterns are then expressed as models, in the form of decision <phrase>trees</phrase> or sets of if-then rules, that can be used to classify new cases, with emphasis on making the models understandable as well as accurate. the system has been applied successfully to tasks involving tens of thousands of cases described by hundreds of properties. the <phrase>book</phrase> starts from simple core learning methods and shows how they can be elaborated and extended to deal with typical problems such as missing <phrase>data</phrase> and over <phrase>hitting</phrase>. advantages and disadvantages of the c4.5 approach are discussed and illustrated with several case studies. this <phrase>book</phrase> and <phrase>software</phrase> should be of interest to developers of classification-based <phrase>intelligent systems</phrase> and to students in <phrase>machine learning</phrase> and <phrase>expert systems</phrase> courses.
introduction to <phrase>data compression</phrase>
<phrase>location-based services</phrase>
developing time-<phrase>oriented database</phrase> applications in <phrase>sql</phrase>
readings in <phrase>database systems</phrase>, first edition
readings in <phrase>database systems</phrase>, second edition
<phrase>object-relational</phrase> dbmss: the next great <phrase>wave</phrase>
:discover why <phrase>object-relational</phrase> <phrase>technology</phrase> is ideal for supporting a broad <phrase>spectrum</phrase> of <phrase>data</phrase> types and application areas, from <phrase>financial services</phrase> to <phrase>multimedia</phrase> <phrase>data</phrase>. in this completely revised and updated edition, <phrase>database</phrase> experts <phrase>michael stonebraker</phrase> and <phrase>paul brown</phrase> explore the <phrase>object-relational</phrase> <phrase>paradigm</phrase> and examine the most recent developments in the field. specially written for <phrase>database</phrase> application programmers, <phrase>database</phrase> analysts, and it managers, this <phrase>book</phrase> includes detailed <phrase>information</phrase> on how to classify <phrase>dbms</phrase> applications, where <phrase>object-relational</phrase> dbmss fit in the <phrase>database</phrase> world, and what mechanisms are required to support such an <phrase>engine</phrase>.
readings in <phrase>database systems</phrase>, third edition
the <phrase>mathematics</phrase> of <phrase>inheritance</phrase> systems
<phrase>object-relational</phrase> dbmss, second edition
a many-sorted <phrase>calculus</phrase> based on resolution and paramodulation.
the first-<phrase>order</phrase> <phrase>calculus</phrase> whose well formed formulas are clauses and whose sole inference rules are factorization, resolution and paramodulation is extended to a many-sorted <phrase>calculus</phrase>. as a basis for <phrase>automated theorem proving</phrase>, this many-sorted <phrase>calculus</phrase> leads to a remarkable reduction of the search space and also to simpler proofs. the soundness and completeness of the new <phrase>calculus</phrase> and the sort-theorem, which relates the many-sorted <phrase>calculus</phrase> to its one-sorted counterpart, are shown. in addition <phrase>results</phrase> about term rewriting and unification in a many-sorted <phrase>calculus</phrase> are obtained. practical examples and a proof protocol of an automated theorem prover based on the many-sorted <phrase>calculus</phrase> are presented.
principles of <phrase>multimedia</phrase> <phrase>database systems</phrase>
<phrase>database</phrase> modeling & <phrase>design</phrase>, third edition
inside <phrase>odbc</phrase>.
inside com.
the <phrase>data compression</phrase> <phrase>book</phrase>, <phrase>2nd edition</phrase>
managing gigabytes: compressing and indexing documents and images.
plattformen.
cscl im fernstudium.
selbst organisierte szenarien.
cscl in hochschulseminaren: zwei beispielszenarien aus der praxis.
virtuelle kooperative lernrume.
kooperation in greren lerngruppen.
cscl als herausforderung an die lehrerbildung.
kollaboratives lernen studierender <phrase>mit</phrase> hilfe von <phrase>knowledge</phrase> forum.
pdagogische und didaktische grundlagen.
einleitung und begriffe.
entwicklungsprozess.
kooperative lernrume.
<phrase>software</phrase>- und systementwicklung.
informatikgrundlagen und mensch-computer-kommunikation.
problemorientiertes lernen.
kooperation in kleineren lerngruppen.
einfhrung und bereitstellung.
lern- und kommunikationspsychologische grundlagen.
projektorientierung.
gruppen und gruppenarbeit.
mediendidaktische konzeption.
konzepte <phrase>fr</phrase> die lerngruppe.
qualittssicherung.
spezifikationen, normen und standards <phrase>fr</phrase> lernmaterialien.
kooperatives lernen in organisationen.
cscl in der betrieblichen weiterbildung.
neue lernformen in der berufsausbildung: eine fallstudie.
cscl in der schule.
forschungsmethoden.
koordinationswerkzeuge zur bildung von lerngruppen.
moderation.
coaching.
konzepte zur administration.
kommunikation.
medienwahl.
the <phrase>software</phrase> abstract is intended to tell its reader enough about the <phrase>software</phrase> to know where to find all the other <phrase>information</phrase> that exists about it, anywhere. a copy of the abstract opens every document about the <phrase>software</phrase>. it has a fairly rigid format, so a <phrase>sequence</phrase> of abstracts can be scanned swiftly, the same <phrase>information</phrase> always being found at the same place. the format of the <phrase>software</phrase> abstract is defined in figure 1. although most <phrase>software</phrase> abstracts are written to define individual programs, the format is also an effective tool for describing any <phrase>software</phrase> entity: <phrase>subroutine</phrase>, program, <phrase>software</phrase> system or subsystem, <phrase>data</phrase> file, or <phrase>library</phrase>. it has even been used successfully to document books and articles about documenting (schneider, <phrase>french</phrase>, lucas 77). the abstract is constrained to fit on one side of one sheet of <phrase>paper</phrase>. this is the reason that certain sections of the abstract are considered optional.an example of <phrase>software</phrase> abstract is shown in figure 2. the example demonstrates the considerable informational <phrase>latitude</phrase> that can be incorporated into a <phrase>software</phrase> abstract without violating the strict format guidelines. the <phrase>acm</phrase> portal is published by the <phrase>association for computing machinery</phrase>. <phrase>copyright</phrase> 2010 <phrase>acm</phrase>, inc. terms of usage <phrase>privacy policy</phrase> code of <phrase>ethics</phrase> contact us useful downloads: <phrase>adobe acrobat</phrase> <phrase>quicktime</phrase> <phrase>windows media player</phrase> real player
adaptivitt <phrase>fr</phrase> individuelles lernen.
werkzeuge <phrase>fr</phrase> spezielle lernmethoden.
kommunikationskonzepte.
cscl <phrase>fr</phrase> lernbehinderte und hochbegabte.
<phrase>motivation</phrase>.
lerngruppen.
perspektiven.
konzepte <phrase>fr</phrase> den lehrenden.
bedarfsanalysen.
sicherheit: gewhrleistung und begrenzung des informationsflusses.
logikorientierte datenbanken - <phrase>eine einfhrung</phrase>.
konzepte objektorientierter datenmodelle.
<phrase>eine einfhrung</phrase> in frame-logik.
natrlichsprachliche interaktion <phrase>mit</phrase> datenbanken.
konzepte des datenbank-entwurfs.
modellbildung <phrase>fr</phrase> datenbank-transaktionen.
datenbankkonzepte <phrase>fr</phrase> wissensbasierte systeme.
datenstrukturen <phrase>fr</phrase> geodatenbanken.
dieser bericht gibt eine uebersicht ueber den gegenwaertigen stand des entwurfs effizienter zugriffsstrukturen <phrase>fur</phrase> geometrische objekte in datenbanken.
operationen und kalkle <phrase>fr</phrase> komplex strukturierte werte und objekte.
studien- und forschungsfhrer informatik der neuen bundeslnder (fakulttentag informatik, arbeitskreis "informatik an deutschen universitten und wissenschaftlichen hochschulen"), 2. aufl.
theoretische informatik: eine anwendungsorientierte einfhrung, 2. auflage
<phrase>java</phrase> 2, von den grundlagen bis zu threads und netzen, 2. auflage
einfhrung in die objektorientierte programmierung <phrase>mit</phrase> <phrase>java</phrase>, 2. auflage
einfhrung in die objektorientierte programmierung <phrase>mit</phrase> <phrase>java</phrase>, 1. auflage
einfhrung in die theoretische informatik: formale sprachen und automatentheorie
bersetzerbau
datenbanksysteme - <phrase>eine einfhrung</phrase>, 4. auflage
datenbanksysteme - <phrase>eine einfhrung</phrase>, 5. auflage
datenbanksysteme - <phrase>eine einfhrung</phrase>, 6. auflage
algorithmen in <phrase>java</phrase>
rechneraufbau und rechnerstrukturen, 9. auflage
datenbanken im unternehmen: analyse, modellbildung und einsatz
hnlichkeitssuche in <phrase>multimedia</phrase>-datenbanken - retrieval, suchalgorithmen und anfragebehandlung
ideen der informatik: grundlegende modelle und konzepte
datenmodelle, datenbanksprachen und datenbankmanagementsysteme, 4. auflage
cscl-kompendium. lehr- und handbuch zum computeruntersttzten kooperativen lernen
datenmodelle, datenbanksprachen und datenbankmanagementsysteme, 5. auflage
datenbanksysteme - <phrase>eine einfhrung</phrase>, 7. auflage
bungsbuch datenbanksysteme, 2. auflage
<phrase>javaserver pages</phrase>
:<phrase>javaserver pages</phrase> (jsp) <phrase>technology</phrase> provides an easy way to create dynamic web pages. jsp uses a <phrase>component-based</phrase> approach that allows web developers to easily combine static <phrase>html</phrase> for look-and-feel with <phrase>java</phrase> components for dynamic features. the simplicity of this <phrase>component-based</phrase> <phrase>model</phrase>, combined with the <phrase>cross-platform</phrase> power of <phrase>java</phrase>, allows a <phrase>web development</phrase> environment with enormous potential. <phrase>javaserver pages</phrase> shows how to develop <phrase>java</phrase>-based <phrase>web applications</phrase> without having to be a <phrase>hardcore</phrase> <phrase>programmer</phrase>. the <phrase>author</phrase> provides an overview of jsp concepts and discusses how jsp fits into the larger picture of <phrase>web applications</phrase>. web page authors will benefit from the chapters on generating dynamic content, handling session <phrase>information</phrase>, accessing <phrase>databases</phrase>, authenticating users, and personalizing content. in the <phrase>programming</phrase>-oriented chapters, <phrase>java</phrase> programmers learn how to create <phrase>java</phrase> components and custom jsp tags for web authors to use in jsp pages. about the <phrase>author</phrase>: hans bergsten is the founder of gefion <phrase>software</phrase>, whose main product is a servlet-based component suite for developing <phrase>web applications</phrase>. hans is also an active participant in the development of the jsp specification.
<phrase>java</phrase> <phrase>cookbook</phrase>
<phrase>java</phrase> examples in a nutshell
:this <phrase>book</phrase> is a companion volume to <phrase>java</phrase> in a nutshell. while <phrase>java</phrase> in a nutshell is a quick-reference at <phrase>heart</phrase>, it also includes an accelerated introduction to <phrase>java</phrase> <phrase>programming</phrase>. <phrase>java</phrase> examples in a nutshell picks up where that <phrase>book</phrase> <phrase>leaves</phrase> off, providing a suite of example programs for novice <phrase>java</phrase> programmers and experts alike. this <phrase>book</phrase> doesn't hold your hand or supply detailed explanations of <phrase>java</phrase> <phrase>syntax</phrase> or method calls; it simply delivers well-commented, working examples that explore the wide <phrase>range</phrase> of what is possible with <phrase>java</phrase> 1.1. each chapter concludes with <phrase>programming</phrase> exercises that suggest further avenues for building on what you have learned.
<phrase>java</phrase> in a nutshell - a desktop quick reference for <phrase>java</phrase> programmers, covers <phrase>java</phrase> 1.0
<phrase>java</phrase> power reference
<phrase>java</phrase> foundation classes in a nutshell
<phrase>java</phrase> fundamental classes reference
<phrase>xml</phrase> in a nutshell
this authoritative new edition of <phrase>xml</phrase> in a nutshell provides developers with a complete guide to the rapidly evolving <phrase>xml</phrase> space. serious users of <phrase>xml</phrase> will find topics on just about everything they need, including fundamental <phrase>syntax</phrase> rules, details of <phrase>dtd</phrase> and <phrase>xml schema</phrase> creation, <phrase>xslt</phrase> transformations, and apis used for processing <phrase>xml</phrase> documents. simply put, this is the only reference of its kind among <phrase>xml</phrase> books.
<phrase>java</phrase> 2d graphics
one weakness of <phrase>java</phrase> has been its graphics capabilities. <phrase>java</phrase> 1.0 and 1.1 only included simple primitives for line <phrase>drawing</phrase>: lines could only be one <phrase>pixel</phrase> wide, they could only be solid, and there wasn't any good way to draw curves. <phrase>font</phrase> <phrase>management</phrase> and <phrase>color management</phrase> were also weak. <phrase>java</phrase> 2d (collectively called the "2d <phrase>api</phrase>") signals a <phrase>major</phrase> improvement in java's graphics capabilities. it covers many of the classes in <phrase>java</phrase> 1.2 that address graphics handling and improves on many weaknesses that were present in the previous versions of <phrase>java</phrase>. the 2d <phrase>api</phrase> allows you to produce <phrase>high</phrase>-quality, <phrase>professional</phrase> images on a screen or printer. <phrase>java</phrase> 2d graphics describes the 2d <phrase>api</phrase> from top to bottom, demonstrating how to set line styles and pattern fills as well as more advanced techniques of <phrase>image processing</phrase> and <phrase>font</phrase> handling. you'll see how to create and manipulate the three types of graphics objects: shapes, text, and images. other topics include image <phrase>data</phrase> storage, <phrase>color management</phrase>, <phrase>font</phrase> <phrase>glyphs</phrase>, and <phrase>printing</phrase>. <phrase>java</phrase> 2d graphics assumes no prior <phrase>knowledge</phrase> of graphics. chock full of detailed explanations and examples, this <phrase>book</phrase> provides beginning <phrase>java</phrase> programmers with a solid foundation in 2d graphics and helps more advanced programmers create and use <phrase>high</phrase>-quality images in their applications. topics <phrase>covered</phrase> in the <phrase>book</phrase> include: the rendering pipelineshapes and pathsgeometrypainting with solid colors, gradients, and texturesstroking paths, including dashed linestransformations: <phrase>translation</phrase>, rotation, shearing, and scalingalpha compositingclippingrasterizing and antialiasingfonts and textfont metricsglyphscolors and color spacessrgb and ciexyzicc color profilesimages, image color models, and image dataimage processingimage <phrase>data</phrase> storagegraphics devicesprinting
<phrase>java</phrase> and <phrase>xml</phrase>
<phrase>java</phrase> revolutionized the <phrase>programming</phrase> world by providing a platform-<phrase>independent</phrase> <phrase>programming language</phrase>. <phrase>xml</phrase> takes the <phrase>revolution</phrase> a step further with a platform-<phrase>independent</phrase> <phrase>language</phrase> for interchanging <phrase>data</phrase>. <phrase>java</phrase> and <phrase>xml</phrase> shows how to put the two together, building <phrase>real-world</phrase> applications in which both the code and the <phrase>data</phrase> are truly portable.
<phrase>java virtual machine</phrase>
<phrase>windows 98</phrase> in a nutshell
<phrase>database</phrase> <phrase>programming</phrase> with <phrase>jdbc</phrase> and <phrase>java</phrase>
<phrase>java</phrase> performance tuning
cjkv <phrase>information processing</phrase>: <phrase>chinese</phrase>, <phrase>japanese</phrase>, <phrase>korean</phrase> & <phrase>vietnamese</phrase> <phrase>computing</phrase>
<phrase>cgi</phrase> <phrase>programming</phrase> on the <phrase>world wide web</phrase>, 1st edition
<phrase>programming</phrase> the be operating system: writing programs for the be operating system
<phrase>natural language processing</phrase> with thought treasure
<phrase>tree</phrase> <phrase>automata</phrase>
unification theory.
<phrase>classical</phrase> vs non-<phrase>classical</phrase> logics (the universality of <phrase>classical logic</phrase>).
<phrase>higher order logic</phrase>.
meta-languages, reflection principles, and <phrase>self-reference</phrase>.
<phrase>mathematical induction</phrase>.
logical basis for the <phrase>automation</phrase> of reasoning: case studies.
<phrase>meme</phrase>, mast, and meta-<phrase>meme</phrase>: new tools for motif discovery in <phrase>protein</phrase> sequences.
discovering concepts in structural <phrase>data</phrase>.
motif discovery in <phrase>protein structure</phrase> <phrase>databases</phrase>.
assembling blocks.
a framework for biological pattern discovery on networks of workstations.
discovering patterns in <phrase>dna</phrase> sequences by the algorithmic significance method.
<phrase>rna</phrase> structure analysis: a multifaceted approach.
systematic detection of <phrase>protein</phrase> structural motifs.
pattern discovery and classification in biosequences.
overview: a system for tracking and managing the <phrase>results</phrase> from <phrase>sequence</phrase> comparison programs.
representation and matching of small flexible <phrase>molecules</phrase> in large <phrase>databases</phrase> of 3d molecular <phrase>information</phrase>.
text <phrase>algorithms</phrase>
handbook of <phrase>logic</phrase> in <phrase>artificial intelligence</phrase> and <phrase>logic programming</phrase>, volume2, deduction methodologies
pattern discovery in biomolecular <phrase>data</phrase>: tools, techniques and applications
the vbase <phrase>object database</phrase> environment.
commonloops: merging <phrase>lisp</phrase> and <phrase>object-oriented programming</phrase>.
commonloops blends <phrase>object-oriented programming</phrase> smoothly and tightly with the procedure-oriented <phrase>design</phrase> of <phrase>lisp</phrase>. functions and methods are combined in a more <phrase>general</phrase> abstraction. <phrase>message passing</phrase> is invoked via normal <phrase>lisp</phrase> <phrase>function</phrase> call. methods are viewed as partial descriptions of procedures. <phrase>lisp</phrase> <phrase>data</phrase> types are integrated with object classes. with these integrations, it is easy to incrementally move a program between the procedure and <phrase>object-oriented</phrase> styles.one of the most important properties of commonloops is its extensive use of meta-objects. we discuss three kinds of meta-objects: objects for classes, objects for methods, and objects for discriminators. we argue that these meta-objects make practical both efficient implementation and experimentation with new ideas for <phrase>object-oriented</phrase> programming.commonloops' small kernel is powerful enough to implement the <phrase>major</phrase> <phrase>object-oriented</phrase> systems in use today.
complex entities for <phrase>engineering</phrase> applications.
overview of the iris <phrase>dbms</phrase>.
<phrase>database</phrase> description with sdm: a <phrase>semantic</phrase> <phrase>database</phrase> <phrase>model</phrase>
a tutorial on <phrase>semantic</phrase> <phrase>database</phrase> modeling
sim: a <phrase>database</phrase> system based on the <phrase>semantic</phrase> <phrase>data model</phrase>.
picquery: a <phrase>high</phrase> level <phrase>query language</phrase> for pictorial <phrase>database management</phrase>.
a reasonably <phrase>comprehensive</phrase> set of <phrase>data</phrase> accessing and manipulation operations that should be supported by a generalized pictorial <phrase>database management</phrase> system (pdbms) is proposed. a corresponding <phrase>high</phrase>-level <phrase>query language</phrase>, picquery, is presented and illustrated through examples. picquery has been designed with a flavor similar to qbe as the highly nonprocedural and conservational <phrase>language</phrase> for the pictorial <phrase>database management</phrase> system picdms. picquery and a <phrase>relational</phrase> qbe-like <phrase>language</phrase> would form the <phrase>language</phrase> by which a user could access conventional <phrase>relational databases</phrase> and at the same time pictorial <phrase>databases</phrase> managed by picdms or other robust pdbms. this languageinterface is part of an <phrase>architecture</phrase> aimed toward <phrase>data</phrase> heterogeneity transparency over pictorial and nonpictorial <phrase>databases</phrase>.
managing change in a <phrase>computer-aided design</phrase> <phrase>database</phrase>.
<phrase>object-oriented</phrase> concepts can make a <phrase>design</phrase> <phrase>database</phrase> more reactive to changes in its contents. by embedding change <phrase>semantics</phrase> in the <phrase>database</phrase> <phrase>model</phrase>, the <phrase>design engineer</phrase> can be relieved of managing the detailed effects of changes. however, mechanisms are needed to limit the scope of change propagation and to unambiguously identify the objects to which propagated changes should apply. we propose new mechanisms based on group check-in/check-out, <phrase>browser</phrase> contexts and paths, configuration constraints, and rules, to support a powerful automatic change capability within a <phrase>design</phrase> <phrase>database</phrase>.
integrating an <phrase>object-oriented programming</phrase> system with a <phrase>database</phrase> system.
there are two <phrase>major</phrase> issues to address to achieve integration of an <phrase>object-oriented programming</phrase> system with a <phrase>database</phrase> system. one is the <phrase>language</phrase> issue: an <phrase>object-oriented programming language</phrase> must be augmented with <phrase>semantic</phrase> <phrase>data modeling</phrase> concepts to provide a robust set of <phrase>data modeling</phrase> concepts to allow modeling of entities for important <phrase>real-world</phrase> applications. another is the computational-<phrase>model</phrase> issue: application programmers should be able to access and manipulate objects as though the objects are in an infinite <phrase>virtual memory</phrase>; in other words, they should not have to be aware of the existence of a <phrase>database</phrase> system in their computations with the <phrase>data structures</phrase> the <phrase>programming language</phrase> allows. this <phrase>paper</phrase> discusses these issues and presents the solutions which we have incorporated into the orion <phrase>object-oriented</phrase> <phrase>database</phrase> system at <phrase>mcc</phrase>.
object <phrase>management</phrase> in distributed <phrase>information</phrase> systems.
development of an <phrase>object-oriented</phrase> <phrase>dbms</phrase>.
we describe the <phrase>results</phrase> of developing the <phrase>gemstone</phrase> <phrase>object-oriented</phrase> <phrase>database server</phrase>, which supports a <phrase>model</phrase> of objects similar to that of <phrase>smalltalk</phrase>-80. we begin with a summary of the goals and requirements for the system: an extensible <phrase>data model</phrase> that captures behavioral <phrase>semantics</phrase>, no <phrase>artificial</phrase> bounds on the number or size of <phrase>database</phrase> objects, <phrase>database</phrase> amenities (concurrency, transactions, recovery, associative access, authorization) and an interactive development environment. <phrase>object-oriented</phrase> languages, <phrase>smalltalk</phrase> in particular, answer some of these requirements. we discuss satisfying the remaining requirements in an <phrase>object oriented</phrase> context, and <phrase>report</phrase> briefly on the status of the development efforts. this <phrase>paper</phrase> is <phrase>directed</phrase> at an audience familiar with <phrase>object-oriented</phrase> languages and their implementation, but perhaps unacquainted with the difficulties and techniques of <phrase>database</phrase> system development. it updates the original <phrase>report</phrase> on the project [cm], and expands upon a more recent article [mdp].
the postgres <phrase>data model</phrase>.
type <phrase>evolution</phrase> in <phrase>an object-oriented database</phrase>.
a prological definition of hasl: a <phrase>purely functional</phrase> <phrase>language</phrase> with unification-based conditional binding expressions.
constraining-unification and the <phrase>programming language</phrase> <phrase>unicorn</phrase>.
up to this point direct implementations of axiomatic or equational specifications have been limited because the implementation mechanisms used are incapable of capturing the full <phrase>semantics</phrase> of the specifications. the <phrase>programming language</phrase> <phrase>unicorn</phrase> was designed and implemented with the intention of exploring the full potential of <phrase>programming</phrase> with equations. <phrase>unicorn</phrase> introduces a new <phrase>language</phrase> mechanism, called constraining-unification. when coupled with <phrase>semantic</phrase> unification, constraining-unification closely models the <phrase>semantics</phrase> of equational specifications thereby allowing for the implementation of a wider class of specifications. unlike the <phrase>language</phrase> mechanisms of rewrite-rule and <phrase>logic programming</phrase>, constraining-unification is <phrase>free</phrase> of <phrase>order</phrase> dependencies. the same <phrase>results</phrase> are <phrase>produced</phrase> regardless of the <phrase>order</phrase> in which the <phrase>axioms</phrase> are stated. the use of viewpoints contributes to the flexibility of the <phrase>unicorn</phrase> <phrase>language</phrase>. preconditions for partial operations can be specified without added machinery.
<phrase>leaf</phrase>: a <phrase>language</phrase> which integrates <phrase>logic</phrase>, equations and functions.
the applog <phrase>language</phrase>.
the unification of functional and <phrase>logic</phrase> languages.
eqlog: equality, types, and generic modules for <phrase>logic programming</phrase>.
<phrase>logic</phrase> <phrase>programming language</phrase> scheme.
uniform - a <phrase>language</phrase> based upon unification which unifies (much of) <phrase>lisp</phrase>, <phrase>prolog</phrase>, and <phrase>act</phrase> 1.
uniform is an <phrase>ai</phrase> <phrase>programming language</phrase> under development based upon augmented unification. it is an attempt to combine, in a simple coherent-framework, the most important features of <phrase>lisp</phrase>, <phrase>actor</phrase> languages such as <phrase>act</phrase> 1 and <phrase>smalltalk</phrase>, and <phrase>logic programming</phrase> languages such as <phrase>prolog</phrase>. among the unusual abilities of the <phrase>language</phrase> is its ability to use the same program as a <phrase>function</phrase>, an <phrase>inverse function</phrase>, a predicate, a pattern, or a generator. all of these uses can be performed upon <phrase>concrete</phrase>, symbolic, and partially instantiated <phrase>data</phrase>. uniform features automatic <phrase>inheritance</phrase> from multiple super classes, facilities for manipulation of programs, a limited ability to determine program equivalence, and a unification-<phrase>oriented database</phrase>.
equality for <phrase>prolog</phrase>.
the <phrase>language</phrase> <phrase>prolog</phrase> has been extended by allowing the inclusion of assertions about equality. when a unification of two terms that do not unify <phrase>syntactically</phrase> is attempted, an equality theorem may be used to prove the two terms equal. if it is possible to prove that the two terms are equal the unification succeeds with the <phrase>variable</phrase> bindings introduced by the equality proof. it is shown that this mechanism significantly improves the power of <phrase>prolog</phrase>. sophisticated <phrase>data</phrase> abstraction with all the advantages of <phrase>object-oriented programming</phrase> is available. techniques for passing partially instantiated <phrase>data</phrase> are described that extends the "multiuse" capabilities of the <phrase>language</phrase>, improve the efficiency of some programs, and allow the implementation of <phrase>arithmetic</phrase> relations that are both <phrase>general</phrase> and efficient. the modifications to standard <phrase>prolog</phrase> are simple and straightforward and in addition the computational overhead for the extra <phrase>linguistic</phrase> power is not significant. equality theorems will probably <phrase>play</phrase> an important role in future <phrase>logic programming</phrase> systems.
tablog: a new approach to <phrase>logic programming</phrase>.
on the relationship between <phrase>logic</phrase> and functional languages.
qute: a <phrase>functional language</phrase> based on unification.
fresh: a higher-<phrase>order</phrase> <phrase>language</phrase> with unification and multiple <phrase>results</phrase>.
funlog: a computational <phrase>model</phrase> integrating <phrase>logic programming</phrase> and <phrase>functional programming</phrase>.
introduction to <phrase>data structures</phrase> and <phrase>algorithms</phrase> related to <phrase>information retrieval</phrase>.
string searching <phrase>algorithms</phrase>.
signature files.
the <phrase>student</phrase> forum will provide an opportunity for students working in the <phrase>area</phrase> of dependable <phrase>computing</phrase> to present and discuss their <phrase>research</phrase> objectives, approaches and preliminary <phrase>results</phrase>. the forum is centered around a conference <phrase>track</phrase> during which the selected "<phrase>student</phrase> <phrase>research</phrase> papers" are presented.
file organizations for optical disks.
<phrase>lexical analysis</phrase> and stoplists.
extended <phrase>boolean</phrase> models.
introduction to <phrase>information</phrase> storage and retrieval systems.
stemming <phrase>algorithms</phrase>.
new indices for text: pat <phrase>trees</phrase> and pat arrays.
relevance <phrase>feedback</phrase> and other query modification techniques.
ranking <phrase>algorithms</phrase>.
inverted files.
special-purpose hardware for <phrase>information retrieval</phrase>.
clustering <phrase>algorithms</phrase>.
<phrase>thesaurus</phrase> <phrase>construction</phrase>.
parallel <phrase>information retrieval</phrase> <phrase>algorithms</phrase>.
<phrase>boolean</phrase> operations.
<phrase>hashing</phrase> <phrase>algorithms</phrase>.
<phrase>multimedia</phrase> communications - synchronization.
<phrase>design</phrase> of <phrase>large-scale</phrase> <phrase>multimedia</phrase>-on-demand storage servers and storage hierarchies.
third-generation distributed <phrase>hypermedia</phrase> systems.
visual interfaces to <phrase>multimedia</phrase> <phrase>databases</phrase>.
<phrase>video</phrase> and image content representation and retrieval.
modeling time-based <phrase>media</phrase>.
image <phrase>database</phrase> <phrase>prototypes</phrase>.
composite models.
document <phrase>model</phrase> issues for <phrase>hypermedia</phrase>.
<phrase>content-based</phrase> indexing and retrieval.
<phrase>video</phrase> <phrase>database systems</phrase> - recent trends in <phrase>research</phrase> and development activities.
<phrase>video</phrase> segmentation for <phrase>video</phrase> <phrase>data management</phrase>.
<phrase>multimedia</phrase> interfaces - <phrase>multimedia</phrase> content indication.
<phrase>memory management</phrase>: codecs.
<phrase>concurrency control</phrase> performance modeling: alternatives and implications.
a number of recent studies have examined the performance of <phrase>concurrency control</phrase> <phrase>algorithms</phrase> for <phrase>database management systems</phrase>. the <phrase>results</phrase> reported to date, rather than being definitive, have tended to be contradictory. in this <phrase>paper</phrase>, rather than presenting &ldquo;yet another <phrase>algorithm</phrase> performance study,&rdquo; we critically investigate the assumptions made in the models used in past studies and their implications. we employ a fairly complete <phrase>model</phrase> of a <phrase>database</phrase> environment for studying the relative performance of three different approaches to the <phrase>concurrency control</phrase> problem under a <phrase>variety</phrase> of modeling assumptions. the three approaches studied represent different extremes in how transaction conflicts are dealt with, and the assumptions addressed pertain to the <phrase>nature</phrase> of the <phrase>database</phrase> system's resources, how transaction restarts are modeled, and the amount of <phrase>information</phrase> available to the <phrase>concurrency control</phrase> <phrase>algorithm</phrase> about transactions' reference <phrase>strings</phrase>. we show that differences in the underlying assumptions explain the seemingly contradictory performance <phrase>results</phrase>. we also address the question of how realistic the various assumptions are for actual <phrase>database systems</phrase>.
on <phrase>mixing</phrase> queries and transactions via multiversion locking.
conflict detection tradeoffs for replicated <phrase>data</phrase>.
<phrase>serializability</phrase>-based correctness criteria.
<phrase>database</phrase> <phrase>concurrency control</phrase> using <phrase>data</phrase> flow <phrase>graphs</phrase>.
a specialized <phrase>data</phrase> flow <phrase>graph</phrase>, <phrase>database</phrase> flow <phrase>graph</phrase> (dbfg) is introduced. dbfgs may be used for scheduling <phrase>database</phrase> operations, particularly in an <phrase>mimd</phrase> <phrase>database</phrase> machine environment. a dbfg explicitly maintains intertransaction and intratransaction dependencies, and is constructed from the transaction flow <phrase>graphs</phrase> (tfg) of active transactions. a tfg, in turn, is the generalization of a query <phrase>tree</phrase> used, for example, in direct [15]. all dbfg schedules are serializable and <phrase>deadlock</phrase> <phrase>free</phrase>. operations needed to create and maintain the dbfg structure as transactions are added or removed from the system are discussed. <phrase>simulation</phrase> <phrase>results</phrase> show that dbfg scheduling performs as well as two-phase locking.
<phrase>firm</phrase> real-time <phrase>concurrency control</phrase>.
reduction in transaction conflicts using <phrase>semantics</phrase>-based <phrase>concurrency control</phrase>.
in this chapter, we consider <phrase>algorithms</phrase> that <phrase>permit</phrase> every site in a partitioned <phrase>database</phrase> system to perform new updates. since this may result in <phrase>independent</phrase> updates to items in different <phrase>partitions</phrase>, conflicts among transactions are bound to occur. <phrase>commutative</phrase> transactions can be used to reduce the number of these conflicts. we develop a probabilistic <phrase>model</phrase> to estimate the possible reduction in conflicts if we incorporate this notion of commutativity. the <phrase>results</phrase> show that the additional efforts to recognize this commutativity is not beneficial unless the number of transactions that commute with each other is significantly large.
<phrase>concurrency control</phrase> mechanisms and their <phrase>taxonomy</phrase>.
transactions and <phrase>database</phrase> processing.
<phrase>extensibility</phrase> and asynchrony in the brown-object storage system.
performance of <phrase>concurrency control</phrase> <phrase>algorithms</phrase> for real-time <phrase>database systems</phrase>.
the <phrase>design</phrase> and performance evaluation of a lock <phrase>manager</phrase> for a <phrase>memory</phrase>-resident <phrase>database</phrase> system.
<phrase>concurrency control</phrase> and recovery methods for <phrase>b+-tree</phrase> indexes: aries/kvl and aries/im.
commit_lsn: a novel and simple method for reducing locking and latching in <phrase>transaction processing</phrase> systems.
a two-phase approach to predictably scheduling real-time transactions.
an analytic <phrase>model</phrase> of transaction interference.
synchronizing <phrase>long</phrase>-lived computations.
two-phase locking performance and its thrashing behavior.
implementation considerations and performance evaluation of object-based <phrase>concurrency control</phrase> protocols.
modeling and analysis of <phrase>concurrency control</phrase> schemes.
modeling performance impact of hot spots.
<phrase>database design</phrase> based on entity and realtionship.
<phrase>data</phrase> models and the <phrase>ansi</phrase>/<phrase>sparc</phrase> <phrase>architecture</phrase>.
network <phrase>database design</phrase> methods.
compter-assisted hierarchical <phrase>database design</phrase>.
requirement specification techniques.
<phrase>semantic</phrase> <phrase>data</phrase> models.
<phrase>relational</phrase> <phrase>database design</phrase>.
schema implemetation and restructuring.
processing-requirement modeling and its applications in logical <phrase>database design</phrase>.
an interactive system for <phrase>database design</phrase> and integration.
the <phrase>design</phrase> of the <phrase>unix</phrase> operating system.
<phrase>syntax</phrase> of <phrase>programming languages</phrase>: theory and practice
<phrase>graph drawing</phrase>: <phrase>algorithms</phrase> for the visualization of <phrase>graphs</phrase>
<phrase>linear programming</phrase>: active set analysis and computer programs.
<phrase>object-oriented</phrase> multidatabase systems: a <phrase>solution</phrase> for advanced applications
<phrase>research</phrase> foundations in <phrase>object-oriented</phrase> and <phrase>semantic</phrase> <phrase>database</phrase> system
<phrase>operating systems</phrase> theory
<phrase>internetworking</phrase> with <phrase>tcp/ip</phrase> - principles, protocols, and architectures, fourth edition
how to program advanced <phrase>java</phrase>
a discipline of <phrase>programming</phrase>.
<phrase>data mining</phrase>: introductory and advanced topics
<phrase>information retrieval</phrase>: <phrase>data structures</phrase> & <phrase>algorithms</phrase>
<phrase>database</phrase> system implementation
<phrase>graphic</phrase> <phrase>java</phrase> 2, <phrase>mastering</phrase> the jfc: volume <phrase>ii</phrase>: swing, <phrase>3rd edition</phrase>
handbook of <phrase>multimedia</phrase> <phrase>information management</phrase>.
brinch hansen on pascal compilers
<phrase>communicating sequential processes</phrase>
this <phrase>paper</phrase> suggests that input and output are <phrase>basic</phrase> primitives of <phrase>programming</phrase> and that parallel composition of <phrase>communicating sequential processes</phrase> is a fundamental program structuring method. when combined with a development of dijkstra's guarded command, these concepts are surprisingly versatile. their use is illustrated by sample solutions of a <phrase>variety</phrase> of familiar <phrase>programming</phrase> exercises.
<phrase>algorithms</phrase> for clustering <phrase>data</phrase>
the implementation of <phrase>functional programming languages</phrase>.
<phrase>object-oriented</phrase> <phrase>database management</phrase>: applications in <phrase>engineering</phrase> and computer <phrase>science</phrase>
the c <phrase>programming language</phrase>
the c <phrase>programming language</phrase>, second edition
elements of the theory of computation
object <phrase>databases</phrase> in practice.
the .net training course
<phrase>object-oriented</phrase> <phrase>software</phrase> <phrase>construction</phrase>, 1st editon
:<phrase>object-oriented</phrase> <phrase>software</phrase> <phrase>construction</phrase>, second edition is the <phrase>comprehensive</phrase> reference on all aspects of object <phrase>technology</phrase>, from <phrase>design</phrase> principles to <phrase>object-oriented</phrase> techniques, <phrase>design</phrase> by <phrase>contract</phrase>, <phrase>object-oriented</phrase> analysis, concurrency, persistence, abstract <phrase>data</phrase> types and many more. written by a pioneer in the field, contains an in-depth analysis of both methodological and technical issues.two-<phrase>color printing</phrase> provides for clear figures and readable <phrase>software</phrase> extracts. comes with a <phrase>cd-rom</phrase> containing: the complete hyperlinked text, for easy reference; <phrase>software</phrase> to read the text on <phrase>major</phrase> <phrase>industry</phrase> platforms; supplementary material (reusable components, <phrase>mathematical</phrase> complements); and a complete <phrase>graphical</phrase> <phrase>object-oriented</phrase> development environment supporting the concepts of the <phrase>book</phrase>.
introduction to the theory of <phrase>programming languages</phrase>
eiffel: the <phrase>language</phrase>
an <phrase>object-oriented</phrase> environment: principles and applications
reusable <phrase>software</phrase>: the base <phrase>object-oriented</phrase> component <phrase>libraries</phrase>
object success
<phrase>object-oriented</phrase> <phrase>software</phrase> <phrase>construction</phrase>, <phrase>2nd edition</phrase>
<phrase>object-oriented</phrase> applications
principles of <phrase>distributed database</phrase> systems.
principles of <phrase>distributed database</phrase> systems, second edition
<phrase>combinatorial optimization</phrase>: <phrase>algorithms</phrase> and complexity
the <phrase>art</phrase> of <phrase>compiler</phrase> <phrase>design</phrase>: theory and practice
<phrase>unix</phrase> <phrase>database management systems</phrase>
<phrase>object-oriented</phrase> modeling and <phrase>design</phrase>
file structures: an analytic approach.
<phrase>database</phrase> tuning - a principled approach
computer networks
from the book:this <phrase>book</phrase> is now in its third edition. each edition has corresponded to a different phase in the way computer networks were used. when the first edition appeared in 1980, networks were an <phrase>academic</phrase> curiosity. when the second edition appeared in 1988, networks were used by <phrase>universities</phrase> and large businesses. when the third edition appeared in 1996, computer networks, especially the worldwide <phrase>internet</phrase>, had become a <phrase>daily</phrase> <phrase>reality</phrase> for millions of people. furthermore, the networking hardware and <phrase>software</phrase> have completely changed since the second edition appeared in 1988, nearly all networks were based on <phrase>copper</phrase> <phrase>wire</phrase>. now, many are based on <phrase>fiber optics</phrase> or <phrase>wireless communication</phrase>. proprietary networks, such as sna have become far less important than <phrase>public</phrase> networks, especially the <phrase>internet</phrase>. the <phrase>osi</phrase> protocols have quietly vanished,, and the <phrase>tcp/ip</phrase> protocol suite has become dominant. in fact, so much has changed, the <phrase>book</phrase> has almost been rewritten from scratch. although chap. 1 has the same introductory <phrase>function</phrase> as it did in the second edition, the contents have been completely revised and brought up to date. for example, instead of basing the hook on the seven-layer <phrase>osi model</phrase>. a five-layer <phrase>hybrid</phrase> <phrase>model</phrase> (shown in fig. 1-21) is now used and introduced in chap. 1. while not exactly identical to the <phrase>tcp/ip</phrase> <phrase>model</phrase>, it is much closer to the <phrase>tcp/ip</phrase> <phrase>model</phrase> in <phrase>spirit</phrase> than it is to the <phrase>osi model</phrase> used in the second edition. also, the new running examples used throughout the <phrase>book</phrase> - the <phrase>internet</phrase> and al m networks are introduced here, along with some <phrase>gigabit</phrase> networks and other popular networks. in chap. 2, the focus has moved from <phrase>copper</phrase> <phrase>wire</phrase> to <phrase>fiber optics</phrase> and <phrase>wireless communication</phrase>,since these arc the technologies of the future. the <phrase>telephone</phrase> system has become almost entirely <phrase>digital</phrase> in the past decade, so the material on it has been largely rewritten, with new material on <phrase>broadband</phrase> <phrase>isdn</phrase> added. the material on cellular <phrase>radio</phrase> has been greatly expanded, and new material on low-<phrase>orbit</phrase> <phrase>satellites</phrase> has been added to the chapter. the <phrase>order</phrase> of discussion of the <phrase>data link layer</phrase> and the mac sublayer has been reversed, since experience with students shows that they understand the mac sublayer better after they have studied the <phrase>data link layer</phrase>. the example protocols there have been kept, as they have <phrase>proven</phrase> very popular, but they have been rewritten in c. new material on the <phrase>internet</phrase> and atm <phrase>data link</phrase> layers has been added. the mac sublayer principles of chap. 4. have been revised to reflect new protocols, including <phrase>wavelength division multiplexing</phrase>, <phrase>wireless</phrase> <phrase>lans</phrase>, and <phrase>digital radio</phrase>. the discussion of bridges has been revised, and new material has been added on <phrase>high</phrase>-speed <phrase>lans</phrase>. most of the routing <phrase>algorithms</phrase> of chap. 5 have been replaced by more modern ones, including distance <phrase>vector</phrase> and link <phrase>state</phrase> routing. the sections on congestion control have been completely <phrase>redone</phrase>, and material on the running examples, the <phrase>internet</phrase> and atm is all new. chap. 6 is still about the <phrase>transport</phrase> layer, but here, too, <phrase>major</phrase> changes have occurred, primarily, the addition of a large amount of new material about the <phrase>internet</phrase>, atm, and network performance. chap. 7, on the <phrase>application layer</phrase>, is now the longest chapter in the <phrase>book</phrase>. the material on network <phrase>security</phrase> has been doubled in length, and new material has been added on dns, <phrase>snmp</phrase>, <phrase>email</phrase>, <phrase>usenet</phrase>, the <phrase>world wide web</phrase>, <phrase>html</phrase>, <phrase>java</phrase>, <phrase>multimedia</phrase>, <phrase>video</phrase> on demand, and the mbone. of the 395 figures in the third edition, 276 (70 percent) are completely new and some of the others have been revised. of the 371 references to the <phrase>literature</phrase>, 282 (76 percent) are to books and papers that have appeared since the second edition was published. of these, over 100 are to works published in 1995 and 1996 alone. all in all, probably 75 percent of the entire <phrase>book</phrase> is <phrase>brand</phrase> new, and parts of the remaining 25 percent have been heavily revised. since this is effectively a new <phrase>book</phrase>, the <phrase>cover</phrase> was redesigned to avoid confusion with the second edition. computer books are full of <phrase>acronyms</phrase>. this one is no exception. by the time you are finished <phrase>reading</phrase> this one, all of the following should ring a bell: aal, amps, arp, asn, atm, <phrase>bgp</phrase>, <phrase>cdma</phrase>, cdpd, csma, dqdb, dns, faq, fdm, ftp, fttc, ftth, <phrase>gsm</phrase>, <phrase>hdlc</phrase>, <phrase>hec</phrase>, hlppl, tab, lcmp, idea, <phrase>ietf</phrase>, 1pv6, <phrase>iso</phrase>, <phrase>itu</phrase>, <phrase>lata</phrase>, mac, maca, man, <phrase>mib</phrase>, mime, nap, <phrase>nntp</phrase>, <phrase>nsa</phrase>, nsap, <phrase>osi</phrase>, <phrase>ospf</phrase>, <phrase>pcm</phrase>, pcn, pcs, pem, pgp, <phrase>ppp</phrase>, <phrase>pstn</phrase>, ptt, <phrase>pvc</phrase>, <phrase>qam</phrase>, rarp, rfc, rsa, sabme, sap, sar, sdh, sdlc, sha, smi, sna, <phrase>snmp</phrase>, snrme, spx, <phrase>tcp</phrase>, <phrase>udp</phrase>, <phrase>vhf</phrase>, <phrase>vlf</phrase>, <phrase>vsat</phrase>, warc, <phrase>wdm</phrase>, wwv, and www. but don't worry. each one will be carefully defined before it is used. to help instructors using this <phrase>book</phrase> as a text for course, the <phrase>author</phrase> has prepared three teaching <phrase>aids</phrase>: a problem solutions manual. <phrase>postscript</phrase> files containing all the figures (for making overhead sheets). a simulator (written in c) for the example protocols of chap. 3. the solutions manual is available from <phrase>prentice hall</phrase> (but only to instructors). the file with the figures and the simulator are available via the <phrase>world wide web</phrase>. to get them, please see the author's home page: http://www.cs.vu.nl/~ast/. the <phrase>book</phrase> was typeset in times <phrase>roman</phrase> using <phrase>troff</phrase>, which, after all these years, is still the only way to go. while <phrase>troff</phrase> is not as trendy as <phrase>wysiwyg</phrase> systems, the reader is invited to compare the <phrase>typesetting</phrase> quality of this <phrase>book</phrase> with books <phrase>produced</phrase> by <phrase>wysiwyg</phrase> systems. my only concession to pcs and <phrase>desktop publishing</phrase> is that for the first time, the <phrase>art</phrase> was <phrase>produced</phrase> using <phrase>adobe illustrator</phrase>, instead of being drawn on <phrase>paper</phrase>. also for the first time, the <phrase>book</phrase> was <phrase>produced</phrase> entirely electronically. the <phrase>postscript</phrase> output from <phrase>troff</phrase> was sent over the <phrase>internet</phrase> to the printer, where the <phrase>film</phrase> for making the offset plates was <phrase>produced</phrase>. no intermediate <phrase>paper</phrase> copy was printed and photographed, as is normally done. andrew s. tanenbaum
modern <phrase>operating systems</phrase>
<phrase>design</phrase> of <phrase>database</phrase> structures
<phrase>geographic information systems</phrase> and cartographic modelling.
core swing: advanced <phrase>programming</phrase>
a first course in <phrase>database systems</phrase>.
pcte - the standard for open repositories.
seamless <phrase>object-oriented</phrase> <phrase>software architecture</phrase> - <phrase>analysis and design</phrase> of reliable systems
a <phrase>model</phrase> implementation of standard pascal
principles of <phrase>database design</phrase>, volume i: logical organizations
konstruktion von anfrageoptimierern <phrase>fr</phrase> objektbanken.
on the complexity of <phrase>broadcast</phrase> and <phrase>gossip</phrase> in different <phrase>communication</phrase> modes.
branching programs and <phrase>binary</phrase> decision diagrams
<phrase>topological</phrase> concepts for hierarchies of variables, types and controls.
modifications of the oettli-prager theorem with application to the <phrase>eigenvalue</phrase> problem.
introduction: symbolic <phrase>algebraic</phrase> methods and verification methods.
symbolic-numeric <phrase>algorithms</phrase> for <phrase>polynomials</phrase>: some recent <phrase>results</phrase>.
on the isoefficiency of the parallel <phrase>descartes</phrase> method.
matrix methods for solving <phrase>algebraic</phrase> systems.
a feasibility result for interval <phrase>gaussian elimination</phrase> relying on <phrase>graph</phrase> structure.
<phrase>solution</phrase> of systems of <phrase>polynomial</phrase> equation by using <phrase>bernstein</phrase> expansion.
symbolic-<phrase>algebraic</phrase> computations in <phrase>modeling language</phrase> for <phrase>mathematical</phrase> <phrase>programming</phrase>.
<phrase>translation</phrase> of <phrase>taylor series</phrase> into lft expansions.
quasi convex-concave extension.
rewriting, induction and decision procedures: a <phrase>case study</phrase> of presburger <phrase>arithmetic</phrase>.
<phrase>derivative</phrase>-based subdivision in multi-dimensional verified <phrase>gaussian quadrature</phrase>.
on the shape of the fixed points of [f]([c])=[a][x]+[b].
exact computation with leda_real - theory and geometric applications.
numerical verification method for solutions of nonlinear hyperbolic equations.
<phrase>geometric series</phrase> bounds for the local errors of taylor methods for linear n-th-<phrase>order</phrase> odes.
safe numerical error bounds for solutions of nonlinear elliptic boundary value problems.
fast verification <phrase>algorithms</phrase> in <phrase>matlab</phrase>.
the linear complementarity problem with interval <phrase>data</phrase>.
some <phrase>numerical methods</phrase> for nonlinear least squares problems.
a new insight of the shortley-weller approximation for <phrase>dirichlet</phrase> problems.
how <phrase>orthogonality</phrase> is <phrase>lost</phrase> in krylov methods.
symbolic-numeric qd-<phrase>algorithms</phrase> with application in <phrase>function</phrase> theory and <phrase>linear algebra</phrase>.
<phrase>content-based</phrase> querying.
searching distributed <phrase>hypermedia</phrase>.
<phrase>query processing</phrase>.
introduction [requirements for a <phrase>multimedia</phrase> <phrase>database</phrase>].
user interaction in a <phrase>virtual world</phrase> environment.
operating system support [for <phrase>multimedia</phrase> <phrase>databases</phrase>].
<phrase>communication</phrase> support [for <phrase>multimedia</phrase> <phrase>databases</phrase>].
indexing of <phrase>multimedia</phrase> <phrase>data</phrase>.
critical success factors [for <phrase>multimedia</phrase> <phrase>databases</phrase>].
<phrase>multimedia</phrase> and its impact on <phrase>database</phrase> system architectures.
current and emerging applications [requirements for a <phrase>multimedia</phrase> <phrase>database</phrase>].
the sql3 server interface.
the <phrase>sgml</phrase>/hytime server interface.
an expert interface for effective man-machine interaction.
an interactive customization program for a <phrase>natural language</phrase> <phrase>database</phrase> query system.
talking it over: the <phrase>natural language</phrase> dialog system ham-ans.
the <phrase>semantic</phrase>-based <phrase>natural language</phrase> interface to <phrase>relational databases</phrase>.
studies in the evaluation of a domain-<phrase>independent</phrase> <phrase>natural language</phrase> query system.
diagram: a <phrase>grammar</phrase> for dialogues.
an explanatory overview is given of diagram, a large and complex <phrase>grammar</phrase> used in an <phrase>artificial intelligence</phrase> system for interpreting <phrase>english</phrase> dialogue. diagram is an augmented <phrase>phrase-structure grammar</phrase> with rule procedures that allow phrases to inherit attributes from their constituents and to acquire attributes from the larger phrases in which they themselves are constituents. these attributes are used to set context-sensitive constraints on the acceptance of an analysis. constraints can be imposed by conditions on dominance as well as by conditions on <phrase>constituency</phrase>. rule procedures can also assign scores to an analysis to rate it as probable or unlikely. less likely analyses can be ignored by the procedures that interpret the utterance. for every expression it analyzes, diagram provides an annotated description of the structure. the annotations supply important <phrase>information</phrase> for other parts of the system that interpret the expression in the context of a dialogue. <phrase>major</phrase> <phrase>design</phrase> decisions are explained and illustrated. some contrasts with transformational grammars are pointed out and problems that motivate a plan to use metarules in the future are discussed. (metarules derive new rules from a set of base rules to achieve the kind of generality previously captured by transformational grammars but without having to perform transformations on <phrase>syntactic</phrase> analyses.)
considerations for the development of <phrase>natural-language</phrase> interfaces to <phrase>database management systems</phrase>.
an <phrase>engine</phrase> for intelligent graphics.
algorithmic <phrase>number theory</phrase> and its relationship to <phrase>computational complexity</phrase>.
algorithmic techniques for geometric optimization.
<phrase>programming</phrase> satan's computer.
differential bdds.
templates for <phrase>linear algebra</phrase> problems.
the <phrase>art</phrase> behind ideas.
a <phrase>quantum</phrase> jump in computer <phrase>science</phrase>.
<phrase>mathematical</phrase> system models as a basis of <phrase>software engineering</phrase>.
abstracting unification: a key step in the <phrase>design</phrase> of <phrase>logic</phrase> program analyses.
<phrase>multimedia</phrase> authoring tools: <phrase>state</phrase> of the <phrase>art</phrase> and <phrase>research</phrase> challenges.
<phrase>symmetry</phrase> and induction in <phrase>model checking</phrase>.
trends in active vision.
avoiding the undefined by underspecification.
we use the <phrase>appeal</phrase> of simplicity and an aversion to complexity in selecting a method for handling partial functions in <phrase>logic</phrase>. we conclude that avoiding the undefined by using underspecification is the preferred choice.
towards a computational theory of <phrase>genome</phrase> rearrangements.
towards a theory of recursive structures.
in computer <phrase>science</phrase>, one is interested mainly in finite objects. insofar as infinite objects are of interest, they must be <phrase>computable</phrase>, i.e., recursive, thus admitting an effective finite representation. this leads to the notion of a recursive <phrase>graph</phrase>, or, more generally, a recursive structure or <phrase>data</phrase> base. in this <phrase>paper</phrase> we summarize our recent work on recursive structures and <phrase>data</phrase> bases, including (1) the <phrase>high</phrase> undecidability of many problems on recursive <phrase>graphs</phrase>, (<phrase>ii</phrase>) somewhat surprising ways of deducting <phrase>results</phrase> on the classification of np optimization problems from <phrase>results</phrase> on the <phrase>degree</phrase> of undecidability of their infinitary analogues, and (iii) completeness <phrase>results</phrase> for query languages on recursive <phrase>data</phrase> bases. the <phrase>acm</phrase> portal is published by the <phrase>association for computing machinery</phrase>. <phrase>copyright</phrase> 2010 <phrase>acm</phrase>, inc. terms of usage <phrase>privacy policy</phrase> code of <phrase>ethics</phrase> contact us useful downloads: <phrase>adobe acrobat</phrase> <phrase>quicktime</phrase> <phrase>windows media player</phrase> real player
<phrase>algebraic topology</phrase> and <phrase>distributed computing</phrase>: a primer.
computational models for distributed <phrase>multimedia</phrase> applications.
computational <phrase>machine learning</phrase> in theory and praxis.
<phrase>hypermedia</phrase> systems as <phrase>internet</phrase> tools.
scalable <phrase>computing</phrase>.
efficient use of parallel & <phrase>distributed systems</phrase>: from theory to practice.
<phrase>edge-coloring</phrase> <phrase>algorithms</phrase>.
all the needles in a haystack: can exhaustive search overcome <phrase>combinatorial</phrase> chaos?
chu spaces and their interpretation as concurrent objects.
fundamental limitations on search <phrase>algorithms</phrase>: <phrase>evolutionary</phrase> <phrase>computing</phrase> in perspective.
<phrase>petri net</phrase> models of distributed <phrase>algorithms</phrase>.
<phrase>information retrieval</phrase> and <phrase>information</phrase> reasoning.
reasoning about actions and change with ramification.
formulations and formalisms in <phrase>software architecture</phrase>.
recurrent <phrase>neural networks</phrase>.
the oz <phrase>programming</phrase> <phrase>model</phrase>.
<phrase>experimental</phrase> validation of models of parallel computation.
<phrase>artificial life</phrase> and <phrase>real world</phrase> <phrase>computing</phrase>.
alternating <phrase>automata</phrase> and program verification.
<phrase>database transaction</phrase> models.
quo vadetis, parallel machine models?
<phrase>standard generalized markup language</phrase>: <phrase>mathematical</phrase> and <phrase>philosophical</phrase> issues.
fuzzy sets as a tool for modeling.
<phrase>data mining</phrase> for selection of <phrase>manufacturing</phrase> processes.
<phrase>fractal</phrase> <phrase>mining</phrase> - <phrase>self similarity</phrase>-based clustering and its applications.
<phrase>text mining</phrase> and <phrase>information extraction</phrase>.
<phrase>outlier</phrase> detection.
<phrase>statistical methods</phrase> for <phrase>data mining</phrase>.
<phrase>data mining</phrase> within a <phrase>regression</phrase> framework.
constraint-based <phrase>data mining</phrase>.
<phrase>data mining</phrase> query languages.
geometric methods for <phrase>feature extraction</phrase> and dimensional reduction - a guided <phrase>tour</phrase>.
<phrase>data mining</phrase> for imbalanced datasets: an overview.
<phrase>dimension</phrase> reduction and <phrase>feature selection</phrase>.
<phrase>reinforcement-learning</phrase>: an overview from a <phrase>data mining</phrase> perspective.
parallel and grid-based <phrase>data mining</phrase> - <phrase>algorithms</phrase>, models and systems for <phrase>high</phrase>-performance kdd.
link analysis.
<phrase>relational</phrase> <phrase>data mining</phrase>.
<phrase>weka</phrase> - a <phrase>machine learning</phrase> <phrase>workbench</phrase> for <phrase>data mining</phrase>.
<phrase>evolutionary algorithms</phrase> for <phrase>data mining</phrase>.
web <phrase>mining</phrase>.
bias vs. <phrase>variance</phrase> decomposition for <phrase>regression</phrase> and classification.
<phrase>data mining</phrase> <phrase>model</phrase> comparison.
frequent set <phrase>mining</phrase>.
meta-learning.
rule induction.
<phrase>mining</phrase> <phrase>high</phrase>-dimensional <phrase>data</phrase>.
lers - a <phrase>data mining</phrase> system.
<phrase>mining</phrase> <phrase>data</phrase> streams.
handling missing attribute values.
logics for <phrase>data mining</phrase>.
quality assessment approaches in <phrase>data mining</phrase>.
association rules.
visualization and <phrase>data mining</phrase> for <phrase>high</phrase> dimensional datasets.
on the use of <phrase>fuzzy logic</phrase> in <phrase>data mining</phrase>.
dataengine - tools for intelligent <phrase>data analysis</phrase> and control.
<phrase>mining</phrase> with rare cases.
<phrase>data mining</phrase> for financial applications.
<phrase>data mining</phrase> for <phrase>software testing</phrase>.
<phrase>data mining</phrase> in <phrase>medicine</phrase>.
<phrase>data mining</phrase> for <phrase>target</phrase> <phrase>marketing</phrase>.
gainsmarts <phrase>data mining</phrase> system for <phrase>marketing</phrase>.
<phrase>wavelet</phrase> methods in <phrase>data mining</phrase>.
granular <phrase>computing</phrase> and rough sets.
introduction to <phrase>knowledge</phrase> discovery in <phrase>databases</phrase>.
introduction to supervised methods.
decomposition methodology for <phrase>knowledge</phrase> discovery and <phrase>data mining</phrase>.
<phrase>data</phrase> cleansing - a prelude to <phrase>knowledge</phrase> discovery.
wizsoft's wizwhy.
collaborative <phrase>data mining</phrase>.
organizational <phrase>data mining</phrase>.
a review of web document clustering approaches.
<phrase>mining</phrase> time series <phrase>data</phrase>.
<phrase>data mining</phrase> of <phrase>design</phrase> <phrase>products</phrase> and processes.
ensemble methods for classifiers.
decision <phrase>trees</phrase>.
clustering methods.
interestingness measures - on determining what is interesting.
spatial <phrase>data mining</phrase>.
<phrase>support vector machines</phrase>.
learning <phrase>information</phrase> patterns in biological <phrase>databases</phrase>.
<phrase>data mining</phrase> for <phrase>intrusion detection</phrase>.
<phrase>oracle</phrase> <phrase>data mining</phrase> - <phrase>data mining</phrase> in the <phrase>database</phrase> environment.
building <phrase>data mining</phrase> solutions with <phrase>ole db</phrase> for dm and <phrase>xml</phrase> for analysis.
a <phrase>data mining</phrase> component is included in <phrase>microsoft sql server</phrase> 2000 and <phrase>sql server</phrase> 2005, one of the most popular dbmss. this gives a push for <phrase>data mining</phrase> technologies to move from a niche towards the mainstream. apart from a few <phrase>algorithms</phrase>, the main contribution of <phrase>sql server</phrase> <phrase>data mining</phrase> is the implementation of <phrase>ole db</phrase> for <phrase>data mining</phrase>. <phrase>ole db</phrase> for <phrase>data mining</phrase> is an <phrase>industrial</phrase> standard <phrase>led</phrase> by <phrase>microsoft</phrase> and supported by a number of isvs. it leverages two existing <phrase>relational</phrase> technologies: <phrase>sql</phrase> and <phrase>ole db</phrase>. it defines a <phrase>sql</phrase> <phrase>language</phrase> for <phrase>data mining</phrase> based on a <phrase>relational</phrase> concept. more recently, <phrase>microsoft</phrase>, hyperion, <phrase>sas</phrase> and a few other bi vendors formed the <phrase>xml</phrase> for analysis council. <phrase>xml</phrase> for analysis covers both <phrase>olap</phrase> and <phrase>data mining</phrase>. the goal is to allow <phrase>consumer</phrase> applications to query various bi packages from different platforms. this <phrase>paper</phrase> gives an overview of <phrase>ole db</phrase> for <phrase>data mining</phrase> and <phrase>xml</phrase> for analysis. it also shows how to build <phrase>data mining</phrase> application using these apis.
<phrase>data mining</phrase> for <phrase>crm</phrase>.
<phrase>information</phrase> <phrase>fusion</phrase> - methods and aggregation operators.
<phrase>data mining</phrase> in <phrase>telecommunications</phrase>.
<phrase>discretization</phrase> methods.
causal discovery.
<phrase>neural networks</phrase>.
<phrase>bayesian</phrase> networks.
<phrase>object-oriented</phrase> <phrase>galileo</phrase>.
<phrase>proteus</phrase>: the <phrase>dbms</phrase> <phrase>user interface</phrase> as an object.
handling constraints and their exceptions: an attached constraint handler for <phrase>object-oriented</phrase> <phrase>cad</phrase> <phrase>databases</phrase>.
the <phrase>architecture</phrase> of the exodus extensible <phrase>dbms</phrase>.
summary.
managing complex objects in the <phrase>darmstadt</phrase> <phrase>database</phrase> kernel system.
<phrase>object-oriented</phrase> <phrase>database systems</phrase>: the notion and the issues.
the efficient support of functionally-defined <phrase>data</phrase> in cactis.
<phrase>inheritance</phrase> issues in <phrase>computer-aided design</phrase> <phrase>databases</phrase>.
godal: an object-centered <phrase>database</phrase> <phrase>language</phrase>.
an <phrase>object-oriented</phrase> interface to a <phrase>relational database</phrase>.
a <phrase>data modeling</phrase> methodology for the <phrase>design</phrase> and implementation of <phrase>information</phrase> systems.
formal specifications that precisely and correctly define the <phrase>semantics</phrase> of <phrase>software</phrase> systems become increasingly important as the complexity of such systems increase. the emerging set of <phrase>semantic</phrase> <phrase>data</phrase> models which support both structural and operational abstractions are excellent tools for formal specifications. in this <phrase>paper</phrase> we introduce a methodology, based on an <phrase>object-oriented</phrase> <phrase>data model</phrase>, for the <phrase>design</phrase> and development of large <phrase>software</phrase> systems. the methodology is demonstrated by applying the <phrase>object-oriented</phrase> <phrase>data model</phrase> to the specification of a <phrase>database</phrase> system which implements the given <phrase>model</phrase>. the specification serves several purposes: it formally defines the precise <phrase>semantics</phrase> of the operations of the <phrase>data model</phrase>, it provides a basis from which the corresponding <phrase>database</phrase> system <phrase>software</phrase> can be systematically derived, and it <phrase>tests</phrase> and demonstrates the adequacy of such a <phrase>model</phrase> for defining <phrase>software</phrase> systems in <phrase>general</phrase>. the <phrase>design</phrase> methodology introduced combines techniques from <phrase>data modeling</phrase>, formal specifications, and <phrase>software engineering</phrase>.
an overview of <phrase>pdm</phrase>: an <phrase>object-oriented</phrase> <phrase>data model</phrase>.
generating <phrase>object-oriented</phrase> <phrase>database systems</phrase> with the <phrase>data model</phrase> <phrase>compiler</phrase>.
<phrase>an object-oriented database</phrase> for trellis.
<phrase>design</phrase> issues for <phrase>object-oriented</phrase> <phrase>database systems</phrase>.
a shared object hierarchy.
this <phrase>paper</phrase> describes the <phrase>design</phrase> and proposed implementation of a shared object hierarchy. the object hierarchy is stored in a <phrase>relational database</phrase> and objects referenced by an application program are cached in the program's <phrase>address space</phrase>. the <phrase>paper</phrase> describes the <phrase>database</phrase> representation for the object hierarchy and the use of postgres, a next-generation <phrase>relational database management system</phrase>, to implement object referencing efficiently. the shared object hierarchy system will be used to implement object fads, an <phrase>object-oriented programming</phrase> environment for interactive <phrase>database</phrase> applications that will be the main <phrase>programming</phrase> interface to postgres.
<phrase>observer</phrase>: an object server for <phrase>an object-oriented database</phrase> system.
towards on <phrase>object-oriented</phrase> <phrase>data model</phrase> for a mechanical <phrase>cad</phrase> <phrase>database</phrase> system.
associate access support in <phrase>gemstone</phrase>.
object <phrase>management</phrase> in postgres using procedures.
this <phrase>paper</phrase> presents the object <phrase>management</phrase> facilities being designed into a next-generation <phrase>data</phrase> <phrase>manager</phrase>, postgres. this system is unique in that it does not invent a new <phrase>data model</phrase> for support of objects but chooses instead to extend the <phrase>relational model</phrase> with a powerful abstract <phrase>data</phrase> typing capability and procedures as full-fledged <phrase>data</phrase> base objects. the reasons to remain with the <phrase>relational model</phrase> are indicated in this <phrase>paper</phrase> along with the postgres <phrase>relational</phrase> extensions.
persistent <phrase>memory</phrase>: a storage system for <phrase>object-oriented</phrase> <phrase>databases</phrase>.
views, objects, and <phrase>databases</phrase>.
using adaptive <phrase>information extraction</phrase> for effective <phrase>human</phrase>-centred document annotation.
<phrase>xml</phrase> <phrase>information retrieval</phrase> and <phrase>information extraction</phrase>.
on knowledgeable unsupervised <phrase>text mining</phrase>.
towards collaborative <phrase>information retrieval</phrase>: three approaches.
evaluating retrieval performance using clickthrough <phrase>data</phrase>.
concept drift and the importance of example.
<phrase>text mining</phrase>.
the xdoc document suite - a <phrase>workbench</phrase> for document <phrase>mining</phrase>.
feature-rich <phrase>memory</phrase>-based classification for shallow <phrase>nlp</phrase> and <phrase>information extraction</phrase>.
visual interfaces for <phrase>semantic</phrase> <phrase>information retrieval</phrase> and browsing.
<phrase>information visualization</phrase> versus the <phrase>semantic web</phrase>.
<phrase>ontology</phrase>-based <phrase>information visualization</phrase>.
the <phrase>xml</phrase> <phrase>revolution</phrase> and the <phrase>semantic web</phrase>.
concluding remarks: today's vision of envisioning the <phrase>semantic</phrase> future.
svg and <phrase>x3d</phrase>: new <phrase>xml</phrase> technologies for 2d and 3d visualization.
interactive interfaces for mapping <phrase>e-commerce</phrase> <phrase>ontologies</phrase>.
using <phrase>scalable vector graphics</phrase> and georgraphical <phrase>information</phrase> systems in a back <phrase>pain</phrase> study.
topic maps visualization.
web rendering systems: techniques, classification criteria and challenges.
recommender systems for the web.
<phrase>web services</phrase>: description, interfaces and <phrase>ontology</phrase>.
interactive visualization of paragraph-level <phrase>metadata</phrase> to support corporate document use.
informatikbetrachtungen.
was ist informatik?
informatik und wirtschaftsinformatik.
informatik - allgemeinbildung <phrase>fr</phrase> die informationsgesellschaft.
computeruntersttztes problemorientiertes lernen.
was lehren wir eigentlich, wenn wir informatik lehren?
<phrase>software</phrase>-entwicklung im industriellen mastab.
wissen und lernen.
sind informatiker <phrase>auch</phrase> gute <phrase>software</phrase>-ingenieuere?
softwaresystemtechnik - eine informatik-ingenieurdisziplin.
progress toward automating the development of <phrase>database</phrase> system <phrase>software</phrase>.
<phrase>query processing</phrase> in a multidatabase system
updating <phrase>relational</phrase> views.
common subexpression isolation in multiple <phrase>query optimization</phrase>
introduction to <phrase>query processing</phrase>
processing cyclic queries.
<phrase>query processing</phrase> using the concecutive retrieval <phrase>property</phrase>.
<phrase>global optimization</phrase> of <phrase>relational</phrase> queries: a first step
<phrase>query processing</phrase> in r*
supporting complex objects in a <phrase>relational</phrase> system for <phrase>engineering</phrase> <phrase>databases</phrase>.
physical <phrase>database design</phrase>: techniques for improved <phrase>database</phrase> performance.
a <phrase>query language</phrase> for statistical <phrase>databases</phrase>
querying <phrase>relational</phrase> views of networks
<phrase>relational</phrase> <phrase>query processing</phrase> on the non-von <phrase>supercomputer</phrase>.
the intelligent <phrase>database</phrase> machine (<phrase>idm</phrase>).
<phrase>database</phrase> access requirements of <phrase>knowledge-based systems</phrase>.
the <phrase>property</phrase> of separability and its application to physical <phrase>database design</phrase>.
<phrase>distributed database</phrase> <phrase>query processing</phrase>.
datenbanksprachen und datenbankbenutzung.
realisierung von operationalen schnittstellen.
architektur von datenbanksystemen.
datenbankentwurf.
manahmen zur wahrung von sicherheits- und integritsbedingungen.
datenmodelle.
<phrase>jade</phrase> - a <phrase>java</phrase> agent development framework.
<phrase>jason</phrase> and the <phrase>golden fleece</phrase> of agent-<phrase>oriented programming</phrase>.
<phrase>programming</phrase> <phrase>multi-agent systems</phrase> in 3apl.
impact: a <phrase>multi-agent</phrase> framework with declarative <phrase>semantics</phrase>.
jadex: a bdi reasoning <phrase>engine</phrase>.
artimis rational dialogue agent <phrase>technology</phrase>: an overview.
the defacto system: coordinating <phrase>human</phrase>-agent teams for the future of disaster response.
claim and sympa: a <phrase>programming</phrase> environment for intelligent and <phrase>mobile</phrase> agents.
jack intelligent agents: an <phrase>industrial</phrase> strength platform.
implementation-aware embedded control systems.
network fundamentals.
<phrase>microcontrollers</phrase>.
<phrase>bluetooth</phrase> in control.
introduction to <phrase>hybrid</phrase> systems.
real-time scheduling for <phrase>embedded systems</phrase>.
from control loops to real-time programs.
discrete-event systems.
basics of <phrase>data acquisition</phrase> and control.
<phrase>temporal logic</phrase> <phrase>model checking</phrase>.
the <phrase>cornell</phrase> <phrase>robocup</phrase> <phrase>robot</phrase> <phrase>soccer</phrase> team: 1999-2003.
control of autonomous <phrase>mobile</phrase> <phrase>robots</phrase>.
sopcs: systems on programmable chips.
embedded <phrase>sensor</phrase> networks.
<phrase>feedback</phrase> control with <phrase>communication</phrase> constraints.
control of <phrase>single</phrase>-input <phrase>single</phrase>-output systems.
vehicle applications of controller <phrase>area</phrase> network.
finite <phrase>automata</phrase>.
<phrase>digital</phrase> signal processors.
fundamentals of <phrase>dynamical systems</phrase>.
fundamentals of <phrase>rtos</phrase>-based <phrase>digital</phrase> controller implementation.
network protocols for networked control systems.
switched systems.
<phrase>labview</phrase> real-time for networked/embedded control.
an overview of <phrase>hybrid</phrase> systems control.
control issues in systems with loop delays.
networked control systems: a <phrase>model</phrase>-<phrase>based approach</phrase>.
embedded real-time control via <phrase>matlab</phrase>, <phrase>simulink</phrase>, and xpc <phrase>target</phrase>.
programmable <phrase>logic</phrase> controllers.
an introduction to <phrase>hybrid</phrase> <phrase>automata</phrase>.
basics of sampling and quantization.
basics of computer <phrase>architecture</phrase>.
control using <phrase>feedback</phrase> over <phrase>wireless</phrase> <phrase>ethernet</phrase> and <phrase>bluetooth</phrase>.
<phrase>wireless</phrase> control with <phrase>bluetooth</phrase>.
control loops in rtlinux.
abduction and analogy in <phrase>chance discovery</phrase>.
the preparedmind: the role of representational change in <phrase>chance discovery</phrase>.
application to understanding consumers latent desires.
discovering deep building blocks for competent <phrase>genetic algorithms</phrase> using <phrase>chance discovery</phrase> via keygraphs.
<phrase>anatomy</phrase> of rare events in a complex adaptive system.
self-organizing <phrase>complex systems</phrase>.
topic <phrase>diffusion</phrase> in a <phrase>community</phrase>.
chance discoveries from the www.
prediction, forecasting, and <phrase>chance discovery</phrase>.
dimensional representations of <phrase>knowledge</phrase> in an <phrase>online community</phrase>.
agent communications for <phrase>chance discovery</phrase>.
<phrase>chance discovery</phrase> for consumers.
application to questionnaire analysis.
modeling the process of <phrase>chance discovery</phrase>.
keygraph: visualized structure among event clusters.
detection of <phrase>earthquake</phrase> risks with keygraph.
logics of argumentation for <phrase>chance discovery</phrase>.
the storification of chances.
<phrase>human</phrase>-to-<phrase>human</phrase> <phrase>communication</phrase> for <phrase>chance discovery</phrase> in <phrase>business</phrase>.
enhancing <phrase>daily</phrase> conversations.
active <phrase>mining</phrase> with visual <phrase>human</phrase> interface.
awareness and imagination of hidden factors and rare events.
effects of scenic <phrase>information</phrase>.
decisions by chance and on chance: meanings of chance in recent <phrase>news</phrase> stories.
agent naming and coordination: <phrase>actor</phrase> based models and infrastructures.
<phrase>middleware</phrase> technologies: <phrase>corba</phrase> and <phrase>mobile</phrase> agents.
a market-based <phrase>model</phrase> for <phrase>resource allocation</phrase> in agent systems.
coordination and <phrase>security</phrase> on the <phrase>internet</phrase>.
coordination models: a guided <phrase>tour</phrase>.
coordinating agents using agent <phrase>communication</phrase> languages conversations.
reusable patterns for agent coordination.
inter-organizational workflows for enterprise coordination.
coordination and control in computational <phrase>ecosystems</phrase>: a vision of the future.
<phrase>high</phrase>-level enabling coordination technologies, introduction.
brokering and <phrase>matchmaking</phrase> for coordination of agent societies: a survey.
<phrase>scalability</phrase> in linda-like coordination systems.
constraints solving as the coordination of inference engines.
coordination models and languages: <phrase>state</phrase> of the <phrase>art</phrase>, introduction.
<phrase>basic</phrase> enabling technologies, introduction.
preface: coordination of <phrase>internet</phrase> agents.
models and technologies for the coordination of <phrase>internet</phrase> agents: a survey.
coordination and mobility.
<phrase>tuple</phrase>-based technologies for coordination.
run-time systems for coordination.
agent coordination via <phrase>scripting</phrase> languages.
foreword: coordination and the <phrase>internet</phrase>.
applications of coordination <phrase>technology</phrase>, introduction.
emerging issues of coordination, introduction.
visions, introduction.
agent-oriented <phrase>software engineering</phrase> for <phrase>internet</phrase> applications.
<phrase>grid computing</phrase>-anstze <phrase>fr</phrase> verteiltes virtuelles prototyping.
bedeutung von <phrase>peer-to-peer</phrase> technologien <phrase>fr</phrase> die distribution von medienprodukten im <phrase>internet</phrase>.
<phrase>peer-to-peer</phrase>-<phrase>computing</phrase> - wettbewerbsvorteil <phrase>fr</phrase> <phrase>intel</phrase>.
sicherheitsaspekte von <phrase>p2p</phrase> anwendungen in unternehmen.
vertrauen und reputation in <phrase>p2p</phrase>-netzwerken.
die anatomie des grid.
project jxta.
<phrase>napster</phrase> in der videobranche?
urheberrecht und <phrase>peer-to-peer</phrase>-dienste.
<phrase>instant messaging</phrase> - nutzenpotentiale und herausforderungen.
<phrase>gnutella</phrase>.
moneybee - vernetzung knstlicher intelligenz.
<phrase>peer-to-peer</phrase> anwendungsbereiche und herausforderungen.
zetagrid.
<phrase>web services</phrase> und <phrase>peer-to-peer</phrase>-netzwerke.
rule analysis
<phrase>architecture</phrase> of active <phrase>database systems</phrase>
eca functionality in a distributed environment.
<phrase>naos</phrase>.
tool support.
exact: an approach to coping with heterogeneous rule execution models.
<phrase>rap</phrase>: the <phrase>rock</phrase> & roll active <phrase>programming</phrase> system.
active <phrase>database systems</phrase>: expectations, commercial experience, and beyond.
<phrase>database</phrase> internal applications.
comparing <phrase>deductive</phrase> and active <phrase>databases</phrase>.
chimera: a <phrase>language</phrase> for designing rule applications.
<phrase>samos</phrase>.
performance assessment.
ariel.
active real-time <phrase>database systems</phrase>.
active <phrase>database</phrase> features in sql3.
summary.
optimization
introduction
<phrase>pfl</phrase>: an active functional dbpl.
monitoring complex rule conditions
reach.
<phrase>ontologies</phrase> for <phrase>knowledge management</phrase>.
<phrase>ontologies</phrase> in f-<phrase>logic</phrase>.
<phrase>web ontology language</phrase>: <phrase>owl</phrase>.
description logics.
<phrase>ontologies</phrase> and <phrase>metadata</phrase> for elearning.
<phrase>ontologies</phrase> and <phrase>hypertext</phrase>.
<phrase>knowledge</phrase> patterns.
<phrase>ontologies</phrase> in support of <phrase>problem solving</phrase>.
the role of <phrase>ontologies</phrase> in <phrase>ecommerce</phrase>.
<phrase>ontology</phrase> matching: a <phrase>machine learning</phrase> approach.
<phrase>semantic</phrase> layering with <phrase>magpie</phrase>.
retrieving and exploring <phrase>ontology</phrase>-based <phrase>information</phrase>.
supporting user tasks through visualisation of <phrase>light</phrase>-weight <phrase>ontologies</phrase>.
<phrase>ontology</phrase> evaluation.
<phrase>ontology</phrase> of the process <phrase>specification language</phrase>.
an overview of ontoclean.
building a very large <phrase>ontology</phrase> from <phrase>medical</phrase> thesauri.
<phrase>ontology</phrase> reconciliation.
<phrase>ontology</phrase> and the <phrase>lexicon</phrase>.
<phrase>ontology</phrase> learning.
the <phrase>resource description framework</phrase> (<phrase>rdf</phrase>) and its <phrase>vocabulary</phrase> description <phrase>language</phrase> rdfs.
<phrase>ontology</phrase>-based recommender systems.
<phrase>ontology</phrase>-based <phrase>content management</phrase> in a virtual <phrase>organization</phrase>.
an <phrase>ontology</phrase>-based platform for <phrase>semantic</phrase> <phrase>interoperability</phrase>.
an <phrase>ontology</phrase>-<phrase>composition algebra</phrase>.
<phrase>ontology engineering</phrase> environments.
tools for mapping and merging <phrase>ontologies</phrase>.
the <phrase>knowledge</phrase> portal "ontoweb".
an extensible <phrase>ontology</phrase> <phrase>software</phrase> environment.
<phrase>ontologies</phrase> in <phrase>bioinformatics</phrase>.
on-to-<phrase>knowledge</phrase> methodology (otkm).
<phrase>ontologies</phrase> in agent architectures.
<phrase>ink</phrase> as a first-class <phrase>datatype</phrase> in <phrase>multimedia</phrase> <phrase>databases</phrase>.
a <phrase>data</phrase> access structure for filtering distance queries in image retrieval.
<phrase>multimedia</phrase> athoring systems.
<phrase>stream</phrase>-based versus structured <phrase>video</phrase> objects: issues, solutions, and challenges.
a unified approach to <phrase>data modeling</phrase> and retrieval for a class of image <phrase>database</phrase> applications.
indexing for retrieval by similarity.
<phrase>metadata</phrase> for building the <phrase>multimedia</phrase> patch <phrase>quilt</phrase>.
querying <phrase>multimedia</phrase> <phrase>databases</phrase> in <phrase>sql</phrase>.
towards a theory of <phrase>multimedia</phrase> <phrase>database systems</phrase>.
the storage and retrieval of continuous <phrase>media</phrase> <phrase>data</phrase>.
retrieval of pictures using approximate matching.
<phrase>design</phrase> and implementation of qbism, a 3d <phrase>medical</phrase> image <phrase>database</phrase> system.
<phrase>dataflow</phrase> and <phrase>education</phrase>: <phrase>data</phrase>-driven and demand-driven distributed computation.
contrasting themes in the <phrase>semantics</phrase> of imperative concurrency.
<phrase>functional programming</phrase> and the <phrase>language</phrase> <phrase>tale</phrase>.
<phrase>design</phrase>, specification and validation of hierarchies of protocols in <phrase>distributed systems</phrase>.
infinitary languages: <phrase>basic</phrase> theory an applications to concurrent systems.
the quest goes on: a survey of proofsystems for partial correctness of csp.
<phrase>logic programming</phrase>: the foundations, the approach and the role of concurrency.
process theory: <phrase>semantics</phrase>, specification and verification.
applications of <phrase>temporal logic</phrase> to the specification and verification of reactive systems: a survey of current trends.
<phrase>petri nets</phrase>: <phrase>basic</phrase> notions, structure, behaviour.
concepts for concurrent <phrase>programming</phrase>.
survey of biodata analysis from a <phrase>data mining</phrase> perspective.
<phrase>mining</phrase> <phrase>chemical compounds</phrase>.
<phrase>data mining</phrase> methods for a <phrase>systematics</phrase> of <phrase>protein</phrase> subcellular location.
phyloinformatics: toward a <phrase>phylogenetic</phrase> <phrase>database</phrase>.
declarative and efficient querying on <phrase>protein</phrase> <phrase>secondary</phrase> structures.
anticlustal: <phrase>multiple sequence alignment</phrase> by antipole clustering.
<phrase>piecewise</phrase> constant modeling of sequential <phrase>data</phrase> using reversible jump <phrase>markov chain monte carlo</phrase>.
<phrase>gene mapping</phrase> by pattern discovery.
scalable <phrase>index structures</phrase> for biological <phrase>data</phrase>.
introduction to <phrase>data mining</phrase> in <phrase>bioinformatics</phrase>.
predicting <phrase>protein folding</phrase> pathways.
summary: a structured folding pathway, which is a time ordered <phrase>sequence</phrase> of folding events, plays an important role in the <phrase>protein folding</phrase> process and hence, in the conformational search. pathway prediction, thus gives more insight into the folding process and is a valuable guiding tool to search the conformation space. in this <phrase>paper</phrase>, we propose a novel 'unfolding' approach to predict the folding pathway. we apply <phrase>graph</phrase>-based methods on a weighted <phrase>secondary</phrase> structure <phrase>graph</phrase> of a <phrase>protein</phrase> to predict the <phrase>sequence</phrase> of unfolding events. when viewed in reverse this yields the folding pathway. we demonstrate the success of our approach on several <phrase>proteins</phrase> whose pathway is partially known.
<phrase>rna</phrase> structure comparison and alignment.
active <phrase>xml</phrase>: a <phrase>data</phrase>-centric perspective on <phrase>web services</phrase>.
web dynamics, structure, and page quality.
an event-condition-<phrase>action</phrase> <phrase>language</phrase> for <phrase>xml</phrase>.
search <phrase>engine</phrase> ability to cope with the changing web.
active <phrase>xquery</phrase>.
adaptive <phrase>web-based</phrase> educational <phrase>hypermedia</phrase>.
dream: distributed reliable event-based application <phrase>management</phrase>.
a survey of architectures for adaptive <phrase>hypermedia</phrase>.
learning web request patterns.
how large is the worldwide web?
methods for miningweb communities: bibliometric, spectral, and flow.
webvigil: an approach to just-in-time <phrase>information</phrase> propagation in large network-centric environments.
web dynamics - setting the scene.
navigating the <phrase>world wide web</phrase>.
theory of random networks and their role in communications networks.
crawling the web.
combining link and content <phrase>information</phrase> in <phrase>web search</phrase>.
<phrase>mp</phrase> - <phrase>mobile</phrase> portals, profiles and <phrase>personalization</phrase>.
<phrase>evolution</phrase> of web structure and content - introduction.
searching and navigating the web - introduction.
events and change on the web - introduction.
personalized access to the web - introduction.
behavioural virtual agents.
we discuss the application of behavioural architectures, in the robotic sense, to virtual agents. 'virtual <phrase>teletubbies</phrase>' are used as an example of the issues involved. we conclude that the use of such architectures has implications for the whole style in which a <phrase>virtual world</phrase> is modelled.
<phrase>logic</phrase>-based <phrase>knowledge representation</phrase>.
after a <phrase>short</phrase> analysis of the requirements that a <phrase>knowledge representation</phrase> <phrase>language</phrase> must satisfy, we introduce description logics, modal logics, and nonmonotonic logics as formalisms for representing terminological <phrase>knowledge</phrase>, time-dependent or subjective <phrase>knowledge</phrase>, and incomplete <phrase>knowledge</phrase> respectively. at the end of each section, we briefly comment on the connection to <phrase>logic programming</phrase>.
an overview of planning under certainty.
a <phrase>taxonomy</phrase> of <phrase>theorem-proving</phrase> strategies.
this article presents a <phrase>taxonomy</phrase> of strategies for fully-automated <phrase>general</phrase>-purpose first-<phrase>order</phrase> <phrase>theorem proving</phrase>. it covers <phrase>forward</phrase>-reasoning ordering-based strategies and backward-reasoning subgoal-reduction strategies, which do not appear together often. unlike traditional presentations that emphasize logical inferences, this classification strives to give equal weight to the inference and search components of <phrase>theorem proving</phrase>, which are equally important in practice. for this purpose, a formal notion of search plan is given and shown to apply to all classes of strategies. for each class, the form of derivation is specified, and it is shown how inference system and search plan cooperate to generate it.
<phrase>knowledge representation</phrase> for <phrase>stochastic</phrase> decision process.
reasoning about <phrase>stochastic</phrase> <phrase>dynamical systems</phrase> and planning under uncertainty has come to <phrase>play</phrase> a fundamental role in <phrase>ai</phrase> <phrase>research</phrase> and applications. the representation of such systems, in particular, of actions with <phrase>stochastic</phrase> effects, has accordingly been given increasing attention in recent years. in this article, we survey a number of techniques for representing <phrase>stochastic processes</phrase> and actions with <phrase>stochastic</phrase> effects using dynamic <phrase>bayesian</phrase> networks and influence diagrams, and briefly describe how these support effective inference for tasks such as monitoring, forecasting, explanation and decision making. we also compare these techniques to several <phrase>action</phrase> representations adopted in the <phrase>classical</phrase> reasoning about <phrase>action</phrase> and planning communities, describing how traditional problems such as the frame and ramification problems are dealt with in <phrase>stochastic</phrase> settings, and how these solutions compare to recent approaches to this problem in the <phrase>classical</phrase> (deterministic) <phrase>literature</phrase>. we argue that while <phrase>stochastic</phrase> dynamics introduce certain complications when it comes to such issues, for the most part, intuitions underlying <phrase>classical</phrase> models can be extended to the <phrase>stochastic</phrase> setting.
a survey of automated deduction.
we <phrase>survey research</phrase> in the <phrase>automation</phrase> of <phrase>deductive</phrase> inference, from its beginnings in the early <phrase>history</phrase> of <phrase>computing</phrase> to the present day. we identify and describe the <phrase>major</phrase> areas of <phrase>research</phrase> interest and their applications. the <phrase>area</phrase> is characterised by its wide <phrase>variety</phrase> of proof methods, forms of automated deduction and applications.
the <phrase>world wide web</phrase> as a place for agents.
the word <phrase>wide web</phrase> was born as an <phrase>internet service</phrase> supporting a simple distributed <phrase>hypertext</phrase> <phrase>management</phrase> system. since its start a number of technologies have been proposed to enhance its capabilities. in this <phrase>paper</phrase> we describe our concept of an active web, namely how we <phrase>design</phrase> the <phrase>software architecture</phrase> of interactive <phrase>cooperative</phrase> applications based on the word <phrase>wide web</phrase>. an active web includes agents able to use the services offered by word <phrase>wide web</phrase> clients and servers. in an active web both users and agents can interoperate using a set of <phrase>basic</phrase> mechanisms for <phrase>communication</phrase> and synchronization. the active web we describe here is based on coordination <phrase>technology</phrase>: we explore two <phrase>alternative</phrase> implementations, both based on <phrase>java</phrase> enriched with <phrase>alternative</phrase> coordination kernels.
lifelike pedagogical agents and <phrase>affective computing</phrase>: an exploratory synthesis.
obdd-based <phrase>universal</phrase> planning: specifying and solving planning problems for synchronized agents in non-deterministic domains.
recently <phrase>model checking</phrase> representation and search techniques were shown to be efficiently applicable to planning, in particular to non-deterministic planning. such planning approaches use ordered <phrase>binary</phrase> decision diagrams (obdds) to encode a planning domain as a non-<phrase>deterministic finite automaton</phrase> (nfa) and then apply fast <phrase>algorithms</phrase> from <phrase>model checking</phrase> to search for a <phrase>solution</phrase>. obdds can effectively scale and can provide <phrase>universal</phrase> plans for complex planning domains. we are particularly interested in addressing the complexities arising in non-deterministic, <phrase>multi-agent</phrase> domains. in this chapter, we present umop,1 a new <phrase>universal</phrase> obdd-based planning framework for non-deterministic, <phrase>multi-agent</phrase> domains, which is also applicable to deterministic singleagent domains as a special case. we introduce a new planning domain description <phrase>language</phrase>, nadl,2 to specify non-deterministic <phrase>multi-agent</phrase> domains. the <phrase>language</phrase> contributes the explicit definition of controllable agents and uncontrollable environment agents. we describe the <phrase>syntax</phrase> and <phrase>semantics</phrase> of nadl and show how to build an efficient obdd-based representation of an nadl description. the umop planning system uses nadl and different obdd-based <phrase>universal</phrase> planning <phrase>algorithms</phrase>. it includes the previously developed strong and strong cyclic planning <phrase>algorithms</phrase> [9,10]. in addition, we introduce our new optimistic planning <phrase>algorithm</phrase>, which relaxes optimality guarantees and generates plausible <phrase>universal</phrase> plans in some domains where no strong or strong cyclic <phrase>solution</phrase> exist. we present empirical <phrase>results</phrase> from domains ranging from deterministic and <phrase>single</phrase>-agent with no environment actions to <phrase>nondeterministic</phrase> and <phrase>multi-agent</phrase> with complex environment actions. umop is shown to be a rich and efficient planning system.
combibining <phrase>artificial intelligence</phrase> and <phrase>databases</phrase> for <phrase>data integration</phrase>.
<phrase>data integration</phrase> is a problem at the intersection of the fields of <phrase>artificial intelligence</phrase> and <phrase>database systems</phrase>. the goal of a <phrase>data integration</phrase> system is to provide a uniform interfacc to a multitude of <phrase>data</phrase> sources, whether they are within one enterprise or on the <phrase>world-wide web</phrase>. the key challenges in <phrase>data integration</phrase> arise because the <phrase>data</phrase> sources being integrated have been designed independently for autonomous applications, and their contents are related in subtle ways. as a result, a <phrase>data integration</phrase> system requires rich formalisms for describing contents of <phrase>data</phrase> sources and relating between contents of different sources. this <phrase>paper</phrase> discusses works aimed at applying techniques from <phrase>artificial intelligence</phrase> to the problem of <phrase>data integration</phrase>. in addition to employing <phrase>knowledge representation</phrase> techniques for describing contents of <phrase>information</phrase> sources, projects have also made use of <phrase>machine learning</phrase> techniques for extracting <phrase>data</phrase> from sources and planning techniques for <phrase>query optimization</phrase>. the <phrase>paper</phrase> also outlines future opportunities for applying <phrase>ai</phrase> techniques in the context of <phrase>data integration</phrase>.
``underwater <phrase>love</phrase>'': building tristo and isolda`s personalities.
an oz-centric review of interactive <phrase>drama</phrase> and believable agents.
believable agents are autonomous agents that exhibit rich personalities. interactive dramas take place in <phrase>virtual worlds</phrase> inhabited by believable agents with whom an audience interacts. in the course of this interaction, the audience experiences a story. this <phrase>paper</phrase> presents the <phrase>research</phrase> <phrase>philosophy</phrase> behind the oz project, a <phrase>research</phrase> group at <phrase>cmu</phrase> that has spent the last ten years studying believable agents and interactive <phrase>drama</phrase>. the <phrase>paper</phrase> then surveys current work from an oz perspective.
<phrase>robots</phrase> with the best of intentions.
intelligent <phrase>mobile</phrase> <phrase>robots</phrase> need the ability to integrate robust <phrase>navigation</phrase> facilities with higher level reasoning. this <phrase>paper</phrase> is an attempt at combining <phrase>results</phrase> and techniques from the areas of <phrase>robot</phrase> <phrase>navigation</phrase> and of intelligent agency. we propose to integrate an existing <phrase>navigation</phrase> system based on <phrase>fuzzy logic</phrase> with a deliberator based on the so-called bdi <phrase>model</phrase>. we discuss some of the subtleties involved in this integration, and illustrate it on a simulated example. experiments on a real <phrase>mobile robot</phrase> are under way.
<phrase>agent-based</phrase> <phrase>project management</phrase>.
integrated <phrase>project management</phrase> means that <phrase>design</phrase> and <phrase>construction</phrase> planning are interleaved with plan execution, allowing both the <phrase>design</phrase> and plan to be changed as necessary. this requires that the right effects of change need to be propagated through the plan and <phrase>design</phrase>. when this is distributed among designers and planners, no one may have all of the <phrase>information</phrase> to perform such propagation and it is important to identify what effects should be propagated to whom, and when. we describe a set of dependencies among plan and <phrase>design</phrase> elements that allow such notification by a set of <phrase>message-passing</phrase> <phrase>software</phrase> agents. the result is to provide a novel level of computer support for complex projects.
a system for defeasible argumentation, with defeasible priorities.
inspired by legal reasoning, this <phrase>paper</phrase> presents an argument-based system for <phrase>defeasible reasoning</phrase>, with a <phrase>logic-programming</phrase>-like <phrase>language</phrase>, and based on dung's argumentation-theoretic approach to the <phrase>semantics</phrase> of <phrase>logic programming</phrase>. the <phrase>language</phrase> of the system has both weak and explicit <phrase>negation</phrase>, and conflicts between arguments are decided with the help of priorities on the rules. these priorities are not fixed, but are themselves defeasibly derived as conclusions within the system.
handling uncertainty in control of autonomous <phrase>robots</phrase>.
autonomous <phrase>robots</phrase> need the ability to move purposefully and without <phrase>human</phrase> intervention in <phrase>real-world</phrase> environments that have not been specifically engineered for them. these environments are characterized by the pervasive presence of uncertainty: the need to cope with this uncertainty constitutes a <phrase>major</phrase> challenge for autonomous <phrase>robots</phrase>. in this note, we discuss this challenge, and present some specific solutions based on our experience on the use of <phrase>fuzzy logic</phrase> in <phrase>mobile</phrase> <phrase>robots</phrase>. we focus on three issues: how to realize robust <phrase>motion control</phrase>; how to flexibly execute <phrase>navigation</phrase> plans; and how to approximately estimate the robot's location.
the event <phrase>calculus</phrase> explained.
this article presents the event <phrase>calculus</phrase>, a <phrase>logic</phrase>-based formalism for representing actions and their effects. a circumscriptive <phrase>solution</phrase> to the <phrase>frame problem</phrase> is deployed which reduces to monotonic predicate completion. using a number of benchmark examples from the <phrase>literature</phrase>, the formalism is shown to apply to a <phrase>variety</phrase> of domains, including those featuring actions with indirect effects, actions with <phrase>nondeterministic</phrase> effects, concurrent actions, and continuous change.
towards a <phrase>logic programming</phrase> <phrase>infrastructure</phrase> for <phrase>internet</phrase> <phrase>programming</phrase>.
after reviewing a number of <phrase>internet</phrase> tools and technologies originating in the field of <phrase>logic programming</phrase> and discussing promissing directions of ongoing <phrase>research</phrase>, we describe a <phrase>logic programming</phrase> based networking <phrase>infrastructure</phrase> which combines reasoning and <phrase>knowledge</phrase> processing with flexible coordination of dynamic <phrase>state</phrase> changes and computation mobility, as well as and its use for the <phrase>design</phrase> of intelligent <phrase>mobile</phrase> agent programs. a <phrase>lightweight</phrase> <phrase>logic</phrase> <phrase>programming language</phrase>, jinni, implemented in <phrase>java</phrase> is introduced as a flexible <phrase>scripting</phrase> tool for gluing together <phrase>knowledge</phrase> processing components and <phrase>java</phrase> objects in networked client/server applications and thin client environments as well as through applets over the web. <phrase>mobile</phrase> threads, implemented by capturing first <phrase>order</phrase> continuations in a compact <phrase>data structure</phrase> sent over the network, allow jinni to interoperate with remote <phrase>high</phrase> performance binprolog servers for <phrase>cpu</phrase>-intensive <phrase>knowledge</phrase> processing. a controlled <phrase>natural language</phrase> to <phrase>prolog</phrase> <phrase>translator</phrase> with support of third <phrase>party</phrase> <phrase>speech recognition</phrase> and text-to-speech <phrase>translation</phrase> allows interaction with users not familiar with <phrase>logic programming</phrase>.
towards autonomous, perceptive, and intelligent virtual <phrase>actors</phrase>.
this <phrase>paper</phrase> explains methods to provide autonomous virtual humans with the skills necessary to perform stand-alone role in <phrase>films</phrase>, <phrase>games</phrase> and interictive <phrase>television</phrase>. we present current <phrase>research</phrase> developments in the virtual <phrase>life</phrase> of autonomous synthetic <phrase>actors</phrase>. after a brief description of our geometric. physical, and auditory <phrase>virtual environments</phrase>, we introduce the <phrase>perception</phrase> <phrase>action</phrase> principles with a few simple examples. we emphasize the concept of virtual sensors for virtual humans. in particular, we describe our experiences in implementing virtual sensors such as vision sensors, tactile sensors, and hearing sensors. we then describe <phrase>knowledge</phrase>-based <phrase>navigation</phrase>. <phrase>knowledge</phrase>-based locomotion and in more details <phrase>sensor</phrase>-based <phrase>tennis</phrase>.
temporally invariant junction <phrase>tree</phrase> for interference in dynamic <phrase>bayesian network</phrase>.
dynamic <phrase>bayesian</phrase> networks (dbns) extend <phrase>bayesian</phrase> networks from static domains to dynamic domains. the only known generic method for exact inference in dbns is based on dynamic expansion and reduction of active slices. it is effective when the domain evolves relatively slowly, but is reported to be "too expensive" for fast evolving domain where inference is under time <phrase>pressure</phrase>. this study explores the stationary feature of problem domains to improve the efficiency of exact inference in dbns. we propose the <phrase>construction</phrase> of a temporally invariant template of a dbn directly supporting exact inference and discuss issues in the <phrase>construction</phrase>. this method eliminates the need for the computation associated with dynamic expansion and reduction of the existing method. the method is demonstrated by <phrase>experimental</phrase> result.
termersetzungssysteme : grundlagen der prototyp-generierung algebraischer spezifikationen
the <phrase>programming language</phrase> ada <phrase>reference manual</phrase>, proposed standard document, <phrase>united states department of defense</phrase>
premo: a framework for <phrase>multimedia</phrase> <phrase>middleware</phrase> - specification, rationale, and <phrase>java</phrase> binding
the <phrase>programming language</phrase> ada <phrase>reference manual</phrase>, <phrase>american national standards institute</phrase>, inc., <phrase>ansi</phrase>/mil-std-1815a-1983
<phrase>software</phrase>-architekturen <phrase>fr</phrase> verteilte systeme
least squares orthogonal distance fitting of curves and surfaces in space
<phrase>architecture</phrase> of distributed computer systems
unobstructed shortest paths in <phrase>polyhedral</phrase> environments
exploitation of fine-<phrase>grain</phrase> parallelism.
<phrase>relational database</phrase> <phrase>technology</phrase>
mikroarchitekturen und mikroprogrammierung: formale beschreibung und optimierung
<phrase>object-oriented</phrase> <phrase>database</phrase> <phrase>programming</phrase>
a tight, practical integration of relations and functions.
the <phrase>design</phrase> of well-structured and correct programs
<phrase>cooperative</phrase> interfaces to <phrase>information</phrase> systems
symbolic <phrase>algebraic</phrase> methods and verification methods
the <phrase>classical</phrase> <phrase>decision problem</phrase>
reasoning with <phrase>logic programming</phrase>
abstract <phrase>state</phrase> machines. a method for <phrase>high</phrase>-level system <phrase>design</phrase> and analysis
incremental speech <phrase>translation</phrase>.
a resolution principle for a <phrase>logic</phrase> with restricted quantifiers
<phrase>multimedia</phrase> <phrase>databases</phrase> in perspective
co-op, a group <phrase>decision support</phrase> system for <phrase>cooperative</phrase> multiple criteria group decision making
<phrase>propositional</phrase>, probabilistic and evidential reasoning: integrating numerical and symbolic approaches
von datenbanken zu expertensystemen
automatic verification of sequential infinite-<phrase>state</phrase> processes
r/3 einfhrung: methoden und werkzeuge
a review of ada tasking
<phrase>sap r/3</phrase> implementation: methods and tools
generierung natrlicher sprache <phrase>mit</phrase> generalisierten phrasenstruktur-grammatiken
this dissertation describes <phrase>nl</phrase> generation with generalized phrase structure grammars (gpsg). it thoroughly discusses the theory of gpsg and argues that it can, in its 1985 version, not efficiently be implemented. therefore, some modifications are suggested that overcome this problem. using the modified formalism, two different approaches to gpsg-based generation are presented. the <phrase>grammar</phrase>-driven approach is shown to suffer from <phrase>indeterminism</phrase>, whereas the structure-driven approach can be efficiently implemented. the latter method has been explored within the <phrase>berlin</phrase> <phrase>machine translation</phrase> system, where the generator starts from sentence-<phrase>semantic</phrase> input structures designed for transfer. since the <phrase>semantics</phrase> is not integrated into the <phrase>grammar</phrase>, an explicit mapping from partial <phrase>semantic</phrase> onto <phrase>syntactic</phrase> structures is necessary, which is accomplished by using techniques known from <phrase>ai</phrase> <phrase>production</phrase> systems. both generators allow for multilingual generation, which is demonstrated for <phrase>english</phrase> and <phrase>german</phrase>.
querying <phrase>databases</phrase> privately: a new approach to <phrase>private</phrase> <phrase>information retrieval</phrase>
portal <phrase>language</phrase> description - second, extended edition
logidata+: <phrase>deductive</phrase> <phrase>databases</phrase> with complex objects
<phrase>b2b</phrase> integration: concepts and <phrase>architecture</phrase>
ada 95 quality and style, guideline for <phrase>professional</phrase> programmers
fundamental <phrase>algorithms</phrase> for <phrase>permutation</phrase> groups
interactive <phrase>relational</phrase> <phrase>database design</phrase> - a <phrase>logic programming</phrase> implementation
computer <phrase>science</phrase> today: recent trends and developments
the mosix distributed operating system - <phrase>load balancing</phrase> for <phrase>unix</phrase>
tractable reasoning in <phrase>artificial intelligence</phrase>
<phrase>mental representation</phrase> and processing of geographic <phrase>knowledge</phrase> - a computational approach
the <phrase>concurrency control</phrase> problem for <phrase>database systems</phrase>
ada 95 rationale, the <phrase>language</phrase>, the standard <phrase>libraries</phrase>
<phrase>logic programming</phrase> and <phrase>databases</phrase>
a survey of verification techniques for parallel programs
time series package (tspack)
entzifferte geheimnisse: methoden und maximen der kryptologie, 3. auflage
parallel execution of parlog
entzifferte geheimnisse: methoden und maximen der kryptologie, 2. auflage
mapping scientific frontiers: the quest for <phrase>knowledge</phrase> visualization
the <phrase>munich</phrase> project cip, volume i: the wide <phrase>spectrum</phrase> <phrase>language</phrase> cip-l
<phrase>information</phrase> visualisation and <phrase>virtual environments</phrase>
the <phrase>munich</phrase> project cip, volume <phrase>ii</phrase>: the <phrase>program transformation</phrase> system cip-s
a guide to <phrase>modula-2</phrase>
<phrase>mobile</phrase> agents: control <phrase>algorithms</phrase>
modelling in <phrase>molecular biology</phrase>
<phrase>software</phrase>-bewertung: ein semantischer <phrase>ansatz</phrase> <phrase>fr</phrase> infomationsmae
active visual inference of surface shape
theory reasoning in connection calculi
<phrase>programming</phrase> in <phrase>prolog</phrase>
zur logik der logik-programmierung: ein konstruktiver <phrase>ansatz</phrase>
<phrase>programming</phrase> in <phrase>prolog</phrase>, <phrase>2nd edition</phrase>
concepts in <phrase>user interfaces</phrase>: a reference <phrase>model</phrase> for the command and response languages (by members of <phrase>ifip</phrase> wg2.7)
<phrase>programming</phrase> in <phrase>prolog</phrase>, <phrase>3rd edition</phrase>
concepts, <phrase>design</phrase>, and performance analysis of a parallel <phrase>prolog</phrase> machine
<phrase>prolog</phrase> by example: how to learn, teach and use it
concurrent reactive plans, anticipation and forestalling execution failures
autonomous <phrase>robots</phrase> that accomplish their jobs in partly unknown and changing environments often learn important <phrase>information</phrase> while carrying out their jobs. to be reliable and effcient, they have to <phrase>act</phrase> appropriately in novel situations and respond immediately to unpredicted events. they also have to reconsider their intended course of <phrase>action</phrase> when it is likely to have flaws. for example, whenever a <phrase>robot</phrase> detects another <phrase>robot</phrase>, it should predict that robot's effect on its plan and -- if necessary -- revise its plan to make it more robust. to accomplish these patterns of activity we equip <phrase>robots</phrase> with structured reactive plans (srps), concurrent control programs that can not only be interpreted but also reasoned about and manipulated. these plans specify how the <phrase>robot</phrase> is to respond to sensory input in <phrase>order</phrase> to accomplish its jobs. in this <phrase>book</phrase> we describe a computational <phrase>model</phrase> of forestalling common flaws in <phrase>autonomous robot</phrase> behavior. to this end, we develop a representation for srps in which declarative statements for goals, perceptions, and beliefs make the structure and purpose of srps explicit and thereby simplify and speed up reasoning about srps and their projections. we have also developed a notation for transforming srps, which does not only make the physical effects of plan execution explicit, but also the process of plan interpretation, as well as temporal, causal, and <phrase>teleological</phrase> relationships among plan interpretation, the world, and the physical behavior of the <phrase>robot</phrase>. using this notation a planning system can diagnose and forestall common flaws in <phrase>robot</phrase> plans that cannot be dealt with in other planning representations. finally, we have extended the <phrase>language</phrase> for writing srps with constructs that allow for a flexible integration of planning and execution and thereby turned it into a <phrase>single</phrase> <phrase>high</phrase>-level <phrase>language</phrase> that can handle both planning and execution actions. experiments in a simulated world show that by simultaneously forestalling flaws and executing srps, the <phrase>robot</phrase> can perform its jobs more reliably than, and almost as effciently as, it could using fixed control programs.
large sparse numerical optimization
plan-based control of robotic agents: improving the capabilities of autonomous <phrase>robots</phrase>
an introduction to the pl/cv2 <phrase>programming</phrase> <phrase>logic</phrase>
hierarchical <phrase>neural networks</phrase> for image interpretation
erfahrung und berechnung: kritik der expertensystemtechnik
korrekte zugriffe zu verteilten daten
the <phrase>data mining</phrase> and <phrase>knowledge</phrase> discovery handbook.
dynamische nicht-normalisierte relationen und symbolische bildbeschreibung
indexstrukturen in datenbanksystemen
automatic generation of computer <phrase>animation</phrase>: using <phrase>ai</phrase> for <phrase>movie</phrase> <phrase>animation</phrase>
systems of reductions
queueing networks with discrete time scale - explicit expressions for the <phrase>steady state</phrase> behavior of discrete time <phrase>stochastic</phrase> networks
ray <phrase>shooting</phrase>, depth orders and hidden surface removal
the <phrase>design</phrase> of rijndael: <phrase>aes</phrase> - the <phrase>advanced encryption standard</phrase>
statistical <phrase>decision theory</phrase> and <phrase>bayesian analysis</phrase>, <phrase>2nd edition</phrase>.
entwurf und verifikation mikroprogrammierter rechnerarchitekturen
experience <phrase>management</phrase>: foundations, development methodology, and <phrase>internet</phrase>-based applications
metasoft primer, towards a <phrase>metalanguage</phrase> for applied <phrase>denotational semantics</phrase>
efficient <phrase>query processing</phrase> in <phrase>geographic information systems</phrase>
developing <phrase>industrial</phrase> <phrase>case-based reasoning</phrase> applications: the inreca-methodology
the <phrase>logic</phrase> system of concept <phrase>graphs</phrase> with <phrase>negation</phrase> and its relationship to <phrase>predicate logic</phrase>
how to multiply matrices faster
managing <phrase>information</phrase> highways - the prism <phrase>book</phrase>: principles, methods, and case studies for designing <phrase>telecommunications</phrase> <phrase>management</phrase> systems
on the integration of <phrase>algebraic</phrase> functions
intelligent <phrase>data analysis</phrase>, an introduction, 2nd editon
the adaptation of virtual man-computer interfaces to user requirements in dialogs
<phrase>public-key cryptography</phrase>: <phrase>state</phrase> of the <phrase>art</phrase> and future directions, e.i.s.s. workshop, oberwolfach, <phrase>germany</phrase>, july 3-6, 1991, final <phrase>report</phrase>
introduction to <phrase>cryptography</phrase>: principles and applications
due to the rapid growth of <phrase>digital</phrase> <phrase>communication</phrase> and <phrase>electronic</phrase> <phrase>data</phrase> exchange, <phrase>information security</phrase> has become a crucial issue in <phrase>industry</phrase>, <phrase>business</phrase>, and administration. modern <phrase>cryptography</phrase> provides essential techniques for securing <phrase>information</phrase> and protecting <phrase>data</phrase>. in the first part, this <phrase>book</phrase> covers the key concepts of <phrase>cryptography</phrase> on an <phrase>undergraduate</phrase> level, from <phrase>encryption</phrase> and <phrase>digital signatures</phrase> to <phrase>cryptographic</phrase> protocols. essential techniques are demonstrated in protocols for key exchange, user identification, <phrase>electronic</phrase> elections and <phrase>digital</phrase> cash. in the second part, more advanced topics are addressed, such as the <phrase>bit</phrase> <phrase>security</phrase> of one-way functions and computationally perfect pseudo random <phrase>bit</phrase> generators. the <phrase>security</phrase> of <phrase>cryptographic</phrase> schemes is a central topic. typical examples of provably secure <phrase>encryption</phrase> and signature schemes and their <phrase>security</phrase> proofs are given. though particular attention is given to the <phrase>mathematical</phrase> foundations, no special background in <phrase>mathematics</phrase> is presumed. the necessary <phrase>algebra</phrase>, <phrase>number theory</phrase> and <phrase>probability theory</phrase> are included in the appendix. each chapter closes with a collection of exercises. answers to the exercises are provided on the web page for this <phrase>book</phrase>.
qualitt und testbarkeit hochintegrierter schaltungen: qualittssicherung durch regelbasierte systeme
<phrase>incomplete information</phrase>: structure, inference, complexity
:this <phrase>monograph</phrase> presents a systematic, exhaustive and up-to-date overview of <phrase>formal methods</phrase> and theories for <phrase>data analysis</phrase> and inference inspired by the concept of rough set. the <phrase>book</phrase> studies structures with <phrase>incomplete information</phrase> from the logical, <phrase>algebraic</phrase> and computational perspective. the formalisms developed are non-<phrase>invasive</phrase> in that only the actual <phrase>information</phrase> is needed in the process of analysis without external sources of <phrase>information</phrase> being required. the <phrase>book</phrase> is intended for researchers, lecturers and graduate students who wish to get acquainted with the rough set style approach to <phrase>information</phrase> systems with <phrase>incomplete information</phrase>.
<phrase>algebraic</phrase> system specification and development - a survey and annotated bibliography
attribute grammars: definitions, systems, and bibliography.
casl user manual - introduction to using the common <phrase>algebraic</phrase> <phrase>specification language</phrase>
das ist informatik
semirings for soft constraint solving and <phrase>programming</phrase>
<phrase>combinatorics</phrase> on traces
automatische synthese rekursiver programme als beweisverfahren
flexible, realzeitfhige kollisionsvermeidung in mehrroboter-systemen
ein universelles konzept zum flexiblen informationsschutz in und <phrase>mit</phrase> rechensystemen
integrity primitives for secure <phrase>information</phrase> systems, final <phrase>report</phrase> of <phrase>race</phrase> integrity primitives evaluation ripe-<phrase>race</phrase> 1040
primality testing in <phrase>polynomial</phrase> time, from <phrase>randomized</phrase> <phrase>algorithms</phrase> to "<phrase>primes</phrase> is in p"
network <phrase>calculus</phrase>: a theory of deterministic queuing systems for the <phrase>internet</phrase>
mathematik <phrase>fr</phrase> informatiker i: die methode der mathematik
extensions of the unity methodology - compositionality, fairness and <phrase>probability</phrase> in parallelism
<phrase>embedded systems</phrase> <phrase>design</phrase>: the <phrase>artist</phrase> roadmap for <phrase>research</phrase> and development
the <phrase>stability theory</phrase> of <phrase>stream</phrase> ciphers
<phrase>microcomputer</phrase> <phrase>problem solving</phrase> using pascal
die strukturierte analyse markovscher modelle
featurebasierte integration von <phrase>cad</phrase>/cam-systemen
an optimized <phrase>translation</phrase> process and its application to <phrase>algol 68</phrase>
on <phrase>object-oriented</phrase> <phrase>database systems</phrase>
an analytical description of chill, the <phrase>ccitt</phrase> <phrase>high</phrase> level <phrase>language</phrase>
<phrase>stochastic</phrase> <phrase>automata</phrase>: stability, nondeterminism, and prediction
modern cryptology - a tutorial
complementary definitions of <phrase>programming language</phrase> <phrase>semantics</phrase>
massiv parallele programmierung <phrase>mit</phrase> dem parallaxis-modell
<phrase>error detection</phrase> and recovery in <phrase>robotics</phrase>
<phrase>algebraic</phrase> specification techniques in <phrase>object oriented programming</phrase> environments
efficient <phrase>graph</phrase> rewriting and its implementation
<phrase>input/output</phrase> intensive massively <phrase>parallel computing</phrase> - <phrase>language</phrase> support, automatic parallelization, advanced optimization, and runtime systems
resolution methods for the <phrase>decision problem</phrase>
introduction to <phrase>cryptography</phrase>
<phrase>artificial</phrase> animals for computer <phrase>animation</phrase>, <phrase>biomechanics</phrase>, locomotion, <phrase>perception</phrase>, and behavior
fehlermaskierung durch verteilte systeme
entwurfstransaktionen <phrase>fr</phrase> modulare objektsysteme: synchronisierung in objektorientierten datenbanksystemen
fundamentals of <phrase>algebraic</phrase> specification 1: equations und initial <phrase>semantics</phrase>
modellbildung, wissensrevision und wissensreprsentation im maschinellen lernen
rechtsprechung und computer in den neunziger jahren: am beispiel von begriff und typologie der krperschaft des ffentlichen rechts
simple program schemes and formal languages
<phrase>knowledge</phrase> discovery in <phrase>databases</phrase> - techniken und anwendungen
generierung portabler <phrase>compiler</phrase>: das portable system <phrase>poco</phrase>
exercises in computer <phrase>systems analysis</phrase>
advanced symbolic analysis for compilers: new techniques and <phrase>algorithms</phrase> for symbolic program analysis and optimization
ausnahmebehandlung in objektorientierten programmiersprachen
<phrase>problem-solving</phrase> methods: understanding, description, development, and reuse
foundations of equational <phrase>logic programming</phrase>
lucas <phrase>associative array</phrase> processor: <phrase>design</phrase>., <phrase>programming</phrase> and application studies
an <phrase>attribute grammar</phrase> for the <phrase>semantic</phrase> analysis of ada
a systematic catalogue of reusable abstract <phrase>data</phrase> types
it-<phrase>security</phrase> and <phrase>privacy</phrase> - <phrase>design</phrase> and use of <phrase>privacy</phrase>-enhancing <phrase>security</phrase> mechanisms
specification and compositional verification of real-time systems
datenbanksystem <phrase>fr</phrase> <phrase>cad</phrase>-arbeitspltze
kollisionsfreie bahnen <phrase>fr</phrase> industrieroboter: ein planungsverfahren
a collection of <phrase>test</phrase> problems for constrained <phrase>global optimization</phrase> problems
approximative <phrase>public</phrase>-key-kryptosysteme
<phrase>text mining</phrase>, theoretical aspects and applications
parallele algorithmen
automatic ambiguity resolution in <phrase>natural language processing</phrase> - an empirical approach
:this <phrase>book</phrase> introduces a new approach to the important <phrase>nlp</phrase> issue of automatic ambiguity resolution, based on statistical models of text. this approach is compared with previous work and proved to yield higher accuracy for <phrase>natural language</phrase> analysis. at the same time, an effective implementation strategy is described, which is directly useful for <phrase>natural language</phrase> analysis. the <phrase>book</phrase> is noteworthy for demonstrating a new empirical approach to <phrase>nlp</phrase>; it is essential <phrase>reading</phrase> for researchers in <phrase>natural language processing</phrase> or <phrase>computational linguistics</phrase>.
<phrase>sil</phrase> - a <phrase>simulation</phrase> <phrase>language</phrase>, user's guide
mechanismen zur synchronisation paralleler prozesse
funktioneller <phrase>test</phrase> der auflsung von zugriffskonflikten in mehrrechnersystemen
translating <phrase>relational</phrase> queries into iterative programs
fm8501: a verified <phrase>microprocessor</phrase>
vocus: a visual attention system for object detection and goal-<phrase>directed</phrase> search
agent-<phrase>oriented programming</phrase>, from <phrase>prolog</phrase> to guarded definite clauses.
ein roboteraktionsplanungssystem
formal foundations for <phrase>software</phrase> engineeing methods
analysis of <phrase>drum</phrase> and <phrase>disk storage</phrase> units
parallel, distributed and multiagent <phrase>production</phrase> systems
datenverarbeitung im hochschulbereich der <phrase>usa</phrase>: stand und entwicklungstendenzen
datenverwaltung in verteilten systemen: grundlagen und lsungskonzepte
matrix eigensystem routines - eispack guide extension
conclog: a methodological approach to concurrent <phrase>logic programming</phrase>
on the shape of <phrase>mathematical</phrase> arguments
new concepts for parallel <phrase>object-relational</phrase> <phrase>query processing</phrase>
during the last few years parallel <phrase>object-relational</phrase> <phrase>database management systems</phrase> have emerged as the leading <phrase>data management</phrase> <phrase>technology</phrase> on the market place. these systems are extensible by user-defined <phrase>data</phrase> types and user-defined functionality for the <phrase>data</phrase>. this work focuses on the efficient parallel execution of user-defined functionality. the main contributions describe techniques to support <phrase>data</phrase> parallelism for user-defined scalar and aggregate functions, to support intra-<phrase>function</phrase> parallelism for the execution of a scalar <phrase>function</phrase> on a large object, and a new <phrase>technology</phrase> to provide <phrase>extensibility</phrase> with regard to new set-<phrase>oriented database</phrase> operations that can efficiently implement user-defined functionality in parallel <phrase>object-relational</phrase> <phrase>database management systems</phrase>. some of these techniques have been implemented in the <phrase>midas</phrase> <phrase>prototype</phrase> or on top of a commercial <phrase>object-relational</phrase> <phrase>database management</phrase> system.
mes premires constructions de programmes
spatio-temporal <phrase>image processing</phrase>: theory and scientific applications
modular <phrase>algorithms</phrase> in symbolic summation and symbolic integration
pascal user manual and <phrase>report</phrase>, second edition
visualizing the <phrase>semantic web</phrase>
pascal user manual and <phrase>report</phrase>, second edition
abstract compositional analysis of iterated relations, a structural approach to complex <phrase>state</phrase> transition systems.
evaluating <phrase>natural language processing</phrase> systems, an analysis and review
:this <phrase>comprehensive</phrase> <phrase>state</phrase>-of-the-<phrase>art</phrase> <phrase>book</phrase> is the first devoted to the important and timely issue of evaluating <phrase>nlp</phrase> systems. it addresses the whole <phrase>area</phrase> of <phrase>nlp</phrase> system evaluation, including aims and scope, problems and methodology. the authors provide a wide-ranging and careful analysis of evaluation concepts, <phrase>reinforced</phrase> with extensive illustrations; they relate systems to their environments and develop a framework for proper evaluation. the discussion of principles is completed by a detailed review of practice and strategies in the field, covering both systems for specific tasks, like <phrase>translation</phrase>, and core <phrase>language</phrase> processors. the methodology lessons drawn from the analysis and review are applied in a series of example cases. the <phrase>book</phrase> also refers <phrase>nlp</phrase> system evaluation to the neighbouring areas of <phrase>information</phrase> and <phrase>speech processing</phrase>, and addresses issues of tool and <phrase>data</phrase> provision for evaluation. a <phrase>comprehensive</phrase> bibliography and subject index are included as well as a term glossary. this <phrase>monograph</phrase> will be a valuable source of inspiration in <phrase>research</phrase>, practice, and teaching.
partial-<phrase>order</phrase> methods for the verification of concurrent systems - an approach to the <phrase>state</phrase>-explosion problem
<phrase>tempo</phrase>: a unified treatment of binding time and parameter passing concepts in <phrase>programming languages</phrase>
an extended entity-relationship <phrase>model</phrase> - fundamentals and <phrase>pragmatics</phrase>
<phrase>index structures</phrase> for <phrase>data</phrase> warehouses
<phrase>data</phrase> warehouses differ significantly from traditional transaction-oriented operational <phrase>database</phrase> applications. <phrase>indexing techniques</phrase> and <phrase>index structures</phrase> applied in the transaction-oriented context are not feasible for <phrase>data</phrase> warehouses. this work develops specific <phrase>heuristic</phrase> <phrase>indexing techniques</phrase>, which process <phrase>range</phrase> queries on aggregated <phrase>data</phrase> more efficiently than those traditionally used in transaction-oriented systems. the <phrase>book</phrase> presents chapters on: - the <phrase>state</phrase> of the <phrase>art</phrase> in <phrase>data warehouse</phrase> <phrase>research</phrase> - <phrase>data</phrase> storage and <phrase>index structures</phrase> - finding optimal <phrase>tree</phrase>-based <phrase>index structures</phrase> - aggregated <phrase>data</phrase> in <phrase>tree</phrase>-based <phrase>index structures</phrase> - performance models for <phrase>tree</phrase>-based <phrase>index structures</phrase> - and techniques for comparing <phrase>index structures</phrase>.
axiomatising the <phrase>logic</phrase> of computer <phrase>programming</phrase>
sprecherunabhngigkeit und sprecheradaption: lsungsanstze <phrase>fr</phrase> das problem des sprecherwechsels bei der automatischen spracherkennung
diana - an <phrase>intermediate language</phrase> for ada, revised version
gag: a practical <phrase>compiler</phrase> generator
<phrase>edinburgh</phrase> lcf
zuverlssigkeit und leistungsfhigkeit objekt-orientierter datenbanksysteme
robust adaptation to non-<phrase>native</phrase> accents in <phrase>automatic speech recognition</phrase>
conditionals in nonmonotonic reasoning and <phrase>belief revision</phrase> - considering conditionals as agents
datenbanksysteme <phrase>fr</phrase> <phrase>software</phrase>-produktionsumgebungen
personalinformationssysteme in deutschen grounternehmen: ausbaustand und rechtsprobleme
graphgrammatiken in der softwaretechnik: theorie und anwendungen
<phrase>query processing</phrase> in <phrase>database systems</phrase>
term indexing
disconnected operation in a distributed file system
:the focus of this work is on the issue of availability in distributed <phrase>file systems</phrase>. it presents the important new technique called disconnected operation, in which clients mask failures and voluntary network detachments by emulating the functionality of servers where actual server-oriented solutions are inadequate. this permits client operation even under complete isolation from the server; the clean integration of <phrase>mobile</phrase> <phrase>computers</phrase> into the system is an important side-effect of the new technique. the <phrase>design</phrase> and implementation of disconnected file service in a working system, the coda file system, is described in detail.
the problem of <phrase>incomplete information</phrase> in <phrase>relational databases</phrase>
metaclasses and their applications, <phrase>data model</phrase> tailoring and <phrase>database</phrase> integration
distributed <phrase>programming</phrase> paradigms with <phrase>cryptography</phrase> applications
<phrase>concrete</phrase> and abstract voronoi diagrams
theory of program structures: schemes, <phrase>semantics</phrase>, verification
a study in string processing languages
the <phrase>science</phrase> of <phrase>programming</phrase>.
<phrase>treewidth</phrase>, computations and approximations
a logical approach to discrete <phrase>math</phrase>.
<phrase>semantics</phrase> of <phrase>digital</phrase> circuits
lectures on the complexity of <phrase>bilinear</phrase> problems
optimal interprocedural program optimization, a new framework and its application
a perspective of constraint-based reasoning - an introductory tutorial
<phrase>axioms</phrase> and hulls
<phrase>algebraic</phrase> <phrase>semantics</phrase>
mmixware, a <phrase>risc</phrase> computer for the third <phrase>millennium</phrase>
efficient structures for geometric <phrase>data management</phrase>
benutzermodellierung in dialogsystemen
list decoding of error-correcting codes (winning <phrase>thesis</phrase> of the 2002 <phrase>acm</phrase> doctoral dissertation competition)
relationale anfragen - zerlegung und optimierung
bersetzerbau - techniken, werkzeuge, anwendungen
a unified approach to interior point <phrase>algorithms</phrase> for linear complementarity problems
prinzipien der referentialitt: untersuchungen zur propositionalen reprsentation von wissen
prosody in speech understanding systems
hyperedge replacement: grammars and languages
<phrase>model</phrase> generation for <phrase>natural language</phrase> interpretation and analysis
<phrase>communication</phrase> and cooperation in agent systems, a pragmatic theory
triggermechanismen in datenbanksystemen
representing plans under uncertainty: a <phrase>logic</phrase> of time, change, and <phrase>action</phrase>
verkehrsanalyse in endlichen zeitrumen: grundlagen und erweiterungen der operationalen analyse
lexikalisch verteiltes text-<phrase>parsing</phrase>: eine objektorientierte spezifikation eines wortexpertensystems auf der grundlage des aktorenmodells
specifying <phrase>message passing</phrase> and time-critical systems with <phrase>temporal logic</phrase>
veritying concurrent processes using <phrase>temporal logic</phrase>
distributed reason maintenance for multiagent systems
time structures - formal description and algorithmic representation
logplan '88 - <phrase>report</phrase> on the <phrase>programming language</phrase>
<phrase>management</phrase> of <phrase>telecommunication</phrase> systems and services, modelling and implementing tmn-based multi-domain <phrase>management</phrase>
:this <phrase>monograph</phrase> is the final <phrase>report</phrase> of the <phrase>european</phrase> <phrase>race</phrase> <phrase>ii</phrase> <phrase>research</phrase> project prepare, devoted to designing and implementing <phrase>prototype</phrase> <phrase>telecommunication</phrase> <phrase>management</phrase> systems over real testbeds. the excellent <phrase>results</phrase> presented are a <phrase>major</phrase> contribution to advancing the <phrase>state</phrase> of the <phrase>art</phrase> in the field and have been widely acknowledged by standards bodies, manufacturers, operators, <phrase>service providers</phrase>, and academics. prospective readers of the <phrase>book</phrase> include those concerned with the <phrase>design</phrase>, development, delivery, and maintenance of <phrase>telecommunication</phrase> services in the global service market as well as those with a <phrase>general</phrase> theoretical or practical interest in multi-domain <phrase>management</phrase> issues.
<phrase>numerical integration</phrase> on advanced computer systems
autonomous dynamic reconfiguration in <phrase>multi-agent systems</phrase>, improving the quality and efficiency of collaborative <phrase>problem solving</phrase>
situationsmodellierung in der bildfolgeauswertung
datenbanksysteme: konzepte und techniken der implementierung, 2. auflage
funktionelle analyse von kommunikationsprotokollen
datenbanksysteme: konzepte und techniken der implementierung
<phrase>cad</phrase> und arbeitssituation: untersuchungen zu den auswirkungen von <phrase>cad</phrase> sowie zur menschengerechten gestaltung von <phrase>cad</phrase>-systemen
first-<phrase>order</phrase> dynamic <phrase>logic</phrase>
semirings, <phrase>automata</phrase>, languages
adaptive modelling, estimation and <phrase>fusion</phrase> from <phrase>data</phrase>: a neurofuzzy approach
this <phrase>book</phrase> brings together for the first time the complete theory of <phrase>data</phrase>-based neurofuzzy modelling and the <phrase>linguistic</phrase> attributes of <phrase>fuzzy logic</phrase> in a <phrase>single</phrase> cohesive <phrase>mathematical</phrase> framework. after introducing the <phrase>basic</phrase> theory of <phrase>data</phrase> based modelling, new concepts including extended additive and multiplicative submodels are developed and their extensions to <phrase>state</phrase> estimation and <phrase>data fusion</phrase> are derived. all of these <phrase>algorithms</phrase> are illustrated with benchmark and real-<phrase>life</phrase> examples to demonstrate their efficiency. the <phrase>book</phrase> aims at researchers and advanced professionals in time series modelling, empirical <phrase>data</phrase> modelling, <phrase>knowledge</phrase> discovery, <phrase>data mining</phrase> and <phrase>data fusion</phrase>.
constraint <phrase>databases</phrase>
a concurrent pascal <phrase>compiler</phrase> for minicomputers
fehlererkennung und fehlerbehandlung in speicherungsstrukturen von datenbanksystemen
optimization of <phrase>sql</phrase> queries for parallel machines
parallel execution offers a method for reducing the response time of queries against large <phrase>databases</phrase>. we address the problem of parallel <phrase>query optimization</phrase>: given a declarative <phrase>sql</phrase> query, find a procedural parallel plan that delivers the query result in minimal time. we develop optimization <phrase>algorithms</phrase> using models that incorporate both sources and obstacles to speedup. we address <phrase>independent</phrase>, pipelined and partitioned parallelism. we incorporate inherent constraints on available parallelism and the extra cost of parallel execution. our models are motivated by experiments with nonstop <phrase>sql</phrase>, a commercial parallel <phrase>dbms</phrase>. we adopt a two-phase approach to parallel <phrase>query optimization</phrase>: joqr (join ordering and query rewrite), followed by parallelization. joqr minimizes total work. then, parallelization spreads work among processors to minimize response time. for joqr, we <phrase>model</phrase> <phrase>communication</phrase> costs and abstract physical characteristics of <phrase>data</phrase> as colors. we devise <phrase>tree</phrase> coloring and reordering <phrase>algorithms</phrase> that are efficient and optimal. we <phrase>model</phrase> parallelization as scheduling a <phrase>tree</phrase> whose nodes represent operators and edges represent parallel/precedence constraints. computation/<phrase>communication</phrase> costs are represented as node/edge weights. we prove worst-case bounds on the performance ratios of our <phrase>algorithms</phrase> and measure <phrase>average</phrase> cases using <phrase>simulation</phrase>. our <phrase>results</phrase> enable the <phrase>construction</phrase> of <phrase>sql</phrase> compilers that effectively exploit parallel machines.
non-standard inferences in description logics
portable methodenmonitoren: dialogsysteme zur steuerung von methodenbanken, softwaretechnischer aufbau und effizienzanalyse
bibliography on abstract <phrase>data</phrase> types, sponsored by the "sterr. fonds zur frderung der wissenschaftlichen forschung"
newcat: <phrase>parsing</phrase> <phrase>natural language</phrase> using left-associative <phrase>grammar</phrase>
semper - secure <phrase>electronic</phrase> marketplace for <phrase>europe</phrase>
berlast in rechensystemen: modellierung und verhinderung
part-whole reasoning in an object-centered framework
on the <phrase>computational geometry</phrase> of pocket <phrase>machining</phrase>
semantische reprsentation komplexer objektstrukturen: modelle <phrase>fr</phrase> nichtkonventionelle datenbankanwendungen
symbolische und konnektionistische modelle der menschlichen informationsverarbeitung - eine kritische gegenberstellung
modelling spatial <phrase>knowledge</phrase> on a <phrase>linguistic</phrase> basis: theory - <phrase>prototype</phrase> - integration
the use of <phrase>projective geometry</phrase> in computer graphics
datenbankeinsatz.
interactive <phrase>markov</phrase> chains: the quest for quantified quality
out of their minds: the lives and discoveries of 15 great computer scientists.
qualitative representation of spatial <phrase>knowledge</phrase>
directions in <phrase>human factors</phrase> for interactive systems
mehrbenutzerkontrolle in nicht-standard-datenbanksystemen
grup theoretical methods in <phrase>image processing</phrase>
uncertain <phrase>projective geometry</phrase>: statistical reasoning for <phrase>polyhedral</phrase> object <phrase>reconstruction</phrase>
petri-netz-methoden und -werkzeuge: hilfsmittel zur entwurfsspezifikation und -validation von rechensystemen
a comparative study of very large <phrase>data</phrase> bases
the nested <phrase>universal</phrase> relation <phrase>database</phrase> <phrase>model</phrase>
anaphora in <phrase>natural language understanding</phrase>: a survey
a problem that all computer-based <phrase>natural language understanding</phrase> (nlu) systems encounter is that of <phrase>linguistic</phrase> reference, and in particular anaphora (abbreviated reference). for example, in a text as simple as: \begin{quote} nadia showed sue her new <phrase>car</phrase>. the seats were day-glo orange. \end{quote} knowing that ``her'''' probably means nadia and not sue and that ``the seats'''' means the seats of nadia''s new <phrase>car</phrase> is not a simple task. .br this <phrase>thesis</phrase> is an extensive review of the reference and anaphor problem, and the approaches to it that nlu systems have taken, from early systems such as <phrase>student</phrase> through to current discourse-oriented ones such as <phrase>pal</phrase>. .br the problem is first examined in detail, and examples are given of many different types of anaphor, some of which have been ignored by previous authors. the approaches taken in traditional systems are then described and abstracted and it is shown why they were inadequate, and why discourse theme and anaphoric focus need to be taken into account. the strengths and weaknesses of current anaphora theories and approaches are evaluated. the <phrase>thesis</phrase> closes with a list of some remaining <phrase>research</phrase> problems. .br the <phrase>thesis</phrase> has been written so as to be as comprehensible as possible to both <phrase>ai</phrase> workers who know no <phrase>linguistics</phrase>, and <phrase>linguists</phrase> who have not studied <phrase>artificial intelligence</phrase>.
planen <phrase>fr</phrase> autonome montageroboter
utilizing problem structure in planning, a local search approach
call-by-push-value: a functional/imperative synthesis
group-theoretic <phrase>algorithms</phrase> and <phrase>graph isomorphism</phrase>
interaktives entwerfen groer programmsysteme: konzepte und werkzeuge
a generative theory of shape, questioning klein's <phrase>erlanger</phrase> program
elements of finite <phrase>model theory</phrase>
die konfigurierung modular aufgebauter datenbanksysteme
iterative <phrase>software engineering</phrase> for multiagent systems: the massive method
:"the <phrase>book</phrase> will serve as a valuable source of reference for <phrase>r&d</phrase> professionals active in <phrase>agent-based</phrase> <phrase>computing</phrase> as well as a gentle and systematic introduction to <phrase>agent-based</phrase> systems development and analysis for <phrase>software</phrase> engineers and advanced students."--<phrase>book</phrase> jacket.
composition of secure <phrase>multi-party</phrase> protocols, a <phrase>comprehensive</phrase> study
dynamische integritt von datenbanken: grundlagen der spezifikation und berwachung
clu <phrase>reference manual</phrase>
synthesising synchronous systems by static scheduling in space-time
foundations of <phrase>logic programming</phrase>, 1st edition
foundations of <phrase>logic programming</phrase>, <phrase>2nd edition</phrase>
datenbankhandbuch.
web-datenbanken: einsatz objekt-relationaler datenbanken <phrase>fr</phrase> web-informationssysteme
the dynamics of concepts - a <phrase>connectionist</phrase> <phrase>model</phrase>
parallele implementierung funktionaler programmiersprachen
towards a cscw framework for scientific cooperation in <phrase>europe</phrase>
anna - a <phrase>language</phrase> for annotating ada programs, <phrase>reference manual</phrase>
attributierte grammatiken und attributierungsalgorithmen
exceptionbehandlung und synchronisation - entwurf und methode
ein inhaltsadressierbares speichersystem zur untersttzung zeitkritischer prozesse der informationswiedergewinnung in datenbanksystemen
fehlertolerante dezentrale prozeautomatisierung
entwurf und realisierung eines multiprozessors
fehlerdiagnose <phrase>fr</phrase> schaltnetze aus modulen <phrase>mit</phrase> partiell injektiven pfadfunktionen
coroutines: a <phrase>programming</phrase> methodology, a <phrase>language</phrase> <phrase>design</phrase> and an implementation
coordinating plans of autonomous agents
<phrase>pisa</phrase>: a <phrase>programming</phrase> system for interactive <phrase>production</phrase> of <phrase>application software</phrase>
verteilte basisalgorithmen
an approach to <phrase>knowledge base</phrase> <phrase>management</phrase>
relevanzanalyse: eine kombination von striktheits- und datenfluanalyse zur effizienten auswertung funktionaler programme
automated deduction in equational <phrase>logic</phrase> and cubic curves
advances in cryptology 1981-1997, <phrase>electronic</phrase> proceedings and index of the crypto and eurocrypt conferences 1981-1997
<phrase>data structures</phrase> and <phrase>algorithms</phrase> 1: sorting and searching
<phrase>data structures</phrase> and <phrase>algorithms</phrase> 2: <phrase>graph</phrase> <phrase>algorithms</phrase> and <phrase>np-completeness</phrase>
<phrase>data structures</phrase> and <phrase>algorithms</phrase> 3
erweiterung relationaler datenbanksysteme <phrase>fr</phrase> technische anwendungen
modified branching programs and their computational power
www - kommunikation, <phrase>internetworking</phrase>, web-technologien
<phrase>algorithms</phrase> and <phrase>data structures</phrase> in <phrase>vlsi</phrase> <phrase>design</phrase>: obdd - foundations and applications
algorithmen und datenstrukturen im <phrase>vlsi</phrase>-<phrase>design</phrase>: obdd - grundlagen und anwendungen
<phrase>randomness</phrase> and completeness in <phrase>computational complexity</phrase>
<phrase>computational complexity</phrase> studies the inherent difficulty of computational problems and the power of the tools we may use to solve them. we focus on <phrase>randomness</phrase> and look at several properties of complete problems to investigate its role as well as issues about nondeterminism, alternation, and space versus time. we investigate the use of <phrase>randomness</phrase> in the <phrase>area</phrase> of proof checking. by derandomizing arthur-<phrase>merlin</phrase> <phrase>games</phrase>, we show that every <phrase>language</phrase> with a bounded round interactive proof system has subexponential size proofs unless the <phrase>polynomial</phrase>-time hierarchy collapses. this provides the first strong evidence that <phrase>graph</phrase> nonisomorphism has subexponential size proofs of membership. under a stronger <phrase>hypothesis</phrase> we can scale the proof size down to <phrase>polynomial</phrase>. we also show how our approach applies to several <phrase>randomized</phrase> processes other than arthur-<phrase>merlin</phrase> <phrase>games</phrase>. we develop techniques for separating complexity classes by isolating a structural difference between their complete languages. we look at several properties from this perspective: the <phrase>density</phrase> of complete languages, the redundancy in complete languages, and the <phrase>frequency</phrase> of occurrence of completeness. we show that there is no sparse hard <phrase>language</phrase> for <phrase>polynomial</phrase> time under <phrase>logarithmic</phrase> space reductions with a bounded number of queries unless <phrase>polynomial</phrase> time collapses to <phrase>logarithmic</phrase> space. the same approach works for various other complexity classes, in the deterministic as well as in the <phrase>randomized</phrase> setting. autoreducibility defines the most <phrase>general</phrase> type of efficient reduction of a problem to itself. we show that settling the question whether all complete languages for doubly exponential time are autoreducible would yield <phrase>major</phrase> separations: if yes, we have a proof that <phrase>polynomial</phrase> time differs from <phrase>polynomial</phrase> space, and <phrase>nondeterministic</phrase> <phrase>logarithmic</phrase> space from <phrase>nondeterministic</phrase> <phrase>polynomial</phrase> time; if no, we can separate the <phrase>polynomial</phrase>-time hierarchy from exponential time. resource-bounded measure formalizes the notions of scarceness and abundance within complexity classes. from the separation point of view, the theory seems particularly suited for separating <phrase>randomized</phrase> <phrase>polynomial</phrase> time from exponential time. we develop several strategies using resource-bounded measure aimed at realizing that goal: showing a difference in measure between the hard languages of these classes, a <phrase>probabilistic method</phrase> approach, and the concept of betting <phrase>games</phrase>.
generic <phrase>model</phrase> <phrase>management</phrase>: concepts and <phrase>algorithms</phrase>
a <phrase>calculus</phrase> of communicating systems
ein molekl-<phrase>atom</phrase>-datenmodell <phrase>fr</phrase> non-standard-anwendungen: anwendungsanalyse, datenmodellentwurf und implementierungskonzepte
inkonsistenzen in deduktiven datenbanken: diagnose und reparatur
prinzipien piktorieller reprsentationssysteme, untersuchungen zur bildhaften reprsentation von wissen in informationsverarbeitenden systemen
an introduction to <phrase>formal language theory</phrase>
casl <phrase>reference manual</phrase>, the complete documentation of the common <phrase>algebraic</phrase> <phrase>specification language</phrase>
<phrase>live</phrase> <phrase>data structures</phrase> in <phrase>logic</phrase> programs: derivation by means of <phrase>abstract interpretation</phrase>
modular specification and verification of <phrase>object-oriented</phrase> programs
this <phrase>book</phrase> presents new techniques for the <phrase>formal specification</phrase> and verification of <phrase>object-oriented</phrase> <phrase>software</phrase>. since modularity is of critical importance for reuse and <phrase>component-based</phrase> <phrase>programming</phrase>, special emphasis is given to the completeness of the presented specification techniques to allow module verification based on the specification of the imported modules. a formal framework developed for a <phrase>java</phrase> <phrase>subset</phrase> illustrates these new techniques.
realistische computergraphik: algorithmen, datenstrukturen und maschinen
the <phrase>design</phrase> of intelligent agents - a layered approach
modular <phrase>compiler</phrase> verification - a refinement-<phrase>algebraic</phrase> approach advocating stepwise abstraction
the complexity of simple computer architectures
handbook of networked and embedded control systems
transactional agents: towards a robust <phrase>multi-agent</phrase> system
<phrase>negation</phrase> and control in <phrase>prolog</phrase>
constraint-based agents
quality-driven query answering for integrated <phrase>information</phrase> systems
the <phrase>internet</phrase> and the <phrase>world wide web</phrase> are becoming increasingly important in our highly interconnected world. this <phrase>book</phrase> addresses the topic of querying the <phrase>data</phrase> available, with regard to its quality, in a systematic and <phrase>comprehensive</phrase> way, from a <phrase>database</phrase> point of view. first, <phrase>information</phrase> quality and <phrase>information</phrase> quality measures are systematically introduced before ranking <phrase>algorithms</phrase> are developed for selecting web sources for access. the second part is devoted to quality-driven query answering, particularly to query planning methods and <phrase>algorithms</phrase>.
automated modeling of physical systems
the <phrase>newton</phrase>-<phrase>cauchy</phrase> framework: a unified approach to unconstrained nonlinear minimization
reasoning and revision in <phrase>hybrid</phrase> representation systems
computation for <phrase>metaphors</phrase>, analogy, and agents.
as an introduction to papers in this <phrase>book</phrase> we review the notion of <phrase>metaphor</phrase> in <phrase>language</phrase>, and of <phrase>metaphor</phrase> as conceptual, and as primary to understanding. yet the view of <phrase>metaphor</phrase> here is more <phrase>general</phrase>. we propose a constructive view of <phrase>metaphor</phrase> as mapping or synthesis of meaning between domains, which need not be conceptual ones. these considerations have implications for <phrase>artificial intelligence</phrase> (<phrase>ai</phrase>), <phrase>human</phrase>-computer interaction (hci), <phrase>algebraic structure</phrase>-preservation, constructive <phrase>biology</phrase>, and agent <phrase>design</phrase>. in this larger setting for <phrase>metaphor</phrase>, contributions of the selected papers are overviewed and key aspects of computation for <phrase>metaphors</phrase>, analogy and agents highlighted.
<phrase>probability</phrase>, stocastic processes, and queuing theory - the <phrase>mathematics</phrase> of computer performance modeling.
probabilistic and <phrase>statistical methods</phrase> in cryptology, an introduction by selected topics
foundations of <phrase>inductive logic programming</phrase>
<phrase>context-free</phrase> grammars: covers, normal forms, and <phrase>parsing</phrase>
principles of <phrase>artificial intelligence</phrase>
isabelle/<phrase>hol</phrase> - a <phrase>proof assistant</phrase> for <phrase>higher-order logic</phrase>
filtering, segmentation and depth
temporally distributed symptoms in technical diagnosis
textgenerierung aus visuellen daten: beschreibungen von straenszenen
<phrase>computing</phrase> in systems described by equations
sprachkonzepte <phrase>fr</phrase> benutzergerechte systeme
<phrase>chance discovery</phrase>
coordination of <phrase>internet</phrase> agents: models, technologies, and applications
a formal <phrase>model</phrase> of visualization in computer graphics systems
reliability evaluation of some <phrase>fault-tolerant</phrase> computer architectures
co-<phrase>ordination</phrase> in <phrase>artificial</phrase> agent societies, social structure and its implications for autonomous <phrase>problem-solving</phrase> agents.
:this <phrase>monograph</phrase> provides a <phrase>comprehensive</phrase> survey of the different approaches to coordination in societies of <phrase>artificial</phrase> and <phrase>human</phrase> agents. setting out from a critical assessment of the <phrase>state</phrase> of the <phrase>art</phrase>, the <phrase>author</phrase> develops a method of structuring <phrase>multi-agent</phrase> applications with a mechanism called structural cooperation. agents are equipped with expertise about their environment in <phrase>order</phrase> to detect and overcome specific types of problem, they make use of their social <phrase>knowledge</phrase> to mutually adjust their activities, and they are coerced toward coherent <phrase>collective behavior</phrase> through normative rules.
direct methods for sparse matrices
the <phrase>design</phrase> of dynamic <phrase>data structures</phrase>
constrained <phrase>global optimization</phrase>: <phrase>algorithms</phrase> and applications
the structure of the <phrase>relational database</phrase> <phrase>model</phrase>
<phrase>software</phrase> frameworks and embedded control systems
active rules in <phrase>database systems</phrase>
learning-based <phrase>robot</phrase> vision, principles and applications
the deisgn of an extendible <phrase>graph</phrase> <phrase>editor</phrase>
isabelle - a generic theorem prover (with a contribution by t. nipkow)
synchronisation in zentralisierten datenbanksystemen: algorithmen, realisierungsmglichkeiten und quantitative analyse
<phrase>data mining</phrase> on <phrase>multimedia</phrase> <phrase>data</phrase>
computer programs for spelling correction: an experiment in program <phrase>design</phrase>
compiling natural <phrase>semantics</phrase>.
erzeugung interaktiver bildverarbeitungssysteme im dialog: konzepte, entwurf und implementierung eines dialogsystems <phrase>fr</phrase> die bildverarbeitung in der medizin
diensteintegrierende kommunikationsnetze <phrase>mit</phrase> teilnehmerberprfbarem datenschutz
<phrase>digital signature</phrase> schemes, <phrase>general</phrase> framework and fail-stop signatures
:this <phrase>book</phrase> is based on the author's ph.d. <phrase>thesis</phrase> which was selected during the 1995 gi doctoral dissertation competition as the winning <phrase>thesis</phrase> in the foundations-of-informatics <phrase>track</phrase>. birgit pfitzmann did her ph.d. work at the <phrase>university</phrase> of <phrase>hildesheim</phrase> with <phrase>professor</phrase> joachim biskup as advisor. securing integrity for <phrase>digital</phrase> communications in the age of global <phrase>electronic</phrase> <phrase>information</phrase> exchange and <phrase>electronic commerce</phrase> is of vital interest for <phrase>democratic</phrase> societies and a central technical challenge for cryptologists. as core contribution to advancing the <phrase>state</phrase> of the <phrase>art</phrase>, the <phrase>author</phrase> rigorously develops the new class of <phrase>digital</phrase> fail-stop signatures: in contrary to all previously introduced <phrase>digital signature</phrase> schemes, these new signatures enable the supposed signer to actually prove forging of the scheme in the case of a successful attack. this <phrase>monograph</phrase> is self-contained with respect to the historical background and <phrase>cryptographic</phrase> primitives used. for the first time, a <phrase>general</phrase> and sophisticated framework is introduced in which previously proposed and the innovative failstop signatures are systematically presented and evaluated, from theoretical foundations up to <phrase>engineering</phrase> aspects. thus the <phrase>book</phrase> is compulsory <phrase>reading</phrase> for anybody interested in <phrase>secure digital</phrase> <phrase>communication</phrase> at the <phrase>professional</phrase> level.
on-line <phrase>error detection</phrase> and fast recover techniques for dependable embedded processors
this <phrase>book</phrase> presents a new approach to on-line observation and concurrent checking of processors by refining and improving known techniques and introducing new ideas.the proposed on-line <phrase>error detection</phrase> and fast recover techniques support and <phrase>complement</phrase> other established methods. in combination with other on-line observation priniciples and with a combined hardware-<phrase>software</phrase> <phrase>test</phrase>, these techniques are used to fulfill a complete self-check scheme for an embedded processor.
<phrase>design</phrase> of <phrase>hashing</phrase> <phrase>algorithms</phrase>
conlan <phrase>report</phrase>
termination proofs for <phrase>logic</phrase> programs
spatial representation and <phrase>motion planning</phrase>
<phrase>compiler</phrase> specification and verification
<phrase>computational geometry</phrase> - an introduction.
diagnostisches problemlsen <phrase>mit</phrase> expertensystemen
<phrase>model-checking</phrase> based <phrase>data</phrase> retrieval, an application to semistructured and temporal <phrase>data</phrase>
multisensordatenverarbeitung in der robotik
synchronisation in mehrrechner-datenbanksystemen - konzepte, realisierungsformen und quantitative bewertung
<phrase>automatic differentiation</phrase>: techniques and applications
bounded incremental computation
audio system for technical readings.
the advent of <phrase>electronic</phrase> documents makes <phrase>information</phrase> available in more than its visual form <phrase>electronic</phrase> <phrase>information</phrase> can now be display-<phrase>independent</phrase>. we describe a <phrase>computing</phrase> system, aster, that audio formats <phrase>electronic</phrase> documents to produce audio documents. aster can speak both literary texts and highly technical documents (presently in <phrase>latex</phrase>) that contain complex <phrase>mathematics</phrase>. <phrase>visual communication</phrase> is characterized by the eye''s ability to actively access parts of a two-dimensional display. the reader is active, while the display is passive. this active-passive role is reversed by the temporal <phrase>nature</phrase> of oral <phrase>communication</phrase>: <phrase>information</phrase> flows actively past a passive listener. this prohibits <phrase>multiple views</phrase> it is impossible to first obtain a <phrase>high</phrase>-level view and then ``look'''' at details. these shortcomings become severe when presenting complex <phrase>mathematics</phrase> orally. audio formatting, which renders <phrase>information</phrase> structure in a manner attuned to an auditory display, overcomes these problems. aster is interactive, and the ability to browse <phrase>information</phrase> structure and obtain <phrase>multiple views</phrase> enables <phrase>active listening</phrase>.
<phrase>aspect</phrase>-<phrase>oriented database</phrase> systems
<phrase>design</phrase> and control of <phrase>workflow</phrase> processes: <phrase>business process management</phrase> for the service <phrase>industry</phrase>
frm: ein frame-reprsentationsmodell und <phrase>seine</phrase> formale semantik: zur integration von datenbank- und wissensreprsentationsanstzen
focusing solutions for <phrase>data mining</phrase>: analytical studies and <phrase>experimental</phrase> <phrase>results</phrase> in <phrase>real-world</phrase> domains
spielbaum-suchverfahren
by themselves, <phrase>speech recognition</phrase> and <phrase>natural language processing</phrase> have limited applications, the reason is that each only accomplishes a part of what humans are capable of when they use <phrase>language</phrase>. spoken <phrase>language</phrase> systems represent the merger of these two technologies and provide an integrated functionality that more closely approximates humans capabilities in speech <phrase>communication</phrase>.
the traveling salesman, computational solutions for tsp applications
petrinetze, <phrase>eine einfhrung</phrase>
systementwurf <phrase>mit</phrase> netzen
<phrase>petri nets</phrase>: an introduction
petrinetze, <phrase>eine einfhrung</phrase>, 2. auflage
elements of distributed <phrase>algorithms</phrase>: modeling and analysis with <phrase>petri nets</phrase>
recognizing planar objects using invariant image features
qualitative spatial reasoning with <phrase>topological</phrase> <phrase>information</phrase>
die interpretation des verhaltens mehrerer akteure in szenenfolgen
introduction to constraint <phrase>databases</phrase>
introduction to constraint <phrase>databases</phrase> comprehensively covers both constraint-<phrase>database theory</phrase> and several sample systems. the <phrase>book</phrase> reveals how constraint <phrase>databases</phrase> bring together techniques from a <phrase>variety</phrase> of fields, such as <phrase>logic</phrase> and <phrase>model</phrase> thoery, <phrase>algebraic</phrase> and <phrase>computational geometry</phrase>, and <phrase>symbolic computation</phrase>, to the <phrase>design</phrase> and analysis of <phrase>data</phrase> models and query languages. constraint <phrase>databases</phrase> are shown to be powerful and simple tools for <phrase>data modeling</phrase> and querying in application areas---such as environmental modeling, <phrase>bioinformatics</phrase>, and computer vision---that are not suitable for <phrase>relational databases</phrase>. specific applications are examined in <phrase>geographic information systems</phrase>, spatiotemporal <phrase>data management</phrase>, <phrase>linear programming</phrase>, <phrase>genome</phrase> <phrase>databases</phrase>, <phrase>model checking</phrase> of <phrase>automata</phrase>, and other areas.
coevolutionary fuzzy modeling
the <phrase>cray x-mp</phrase>/<phrase>model</phrase> 24, a <phrase>case study</phrase> in pipelined <phrase>architecture</phrase> and <phrase>vector</phrase> processing
intelligent <phrase>perceptual</phrase> systems: new directions in computational <phrase>perception</phrase>
<phrase>broadband</phrase> network teletraffic - performance evaluation and <phrase>design</phrase> of <phrase>broadband</phrase> multiservice networks: final <phrase>report</phrase> of <phrase>action</phrase> cost 242
neural nets - a theory for brains and machines
parallele systeme
<phrase>feedback</phrase> shift registers
entscheidungsorientiertes konfigurationsmagament
cyberlaw: the <phrase>law</phrase> of the <phrase>internet</phrase>
kommunikationskonzepte <phrase>fr</phrase> verteilte transaktionsorientierte systeme
a connotational theory of program structure
efficient visual recognition using the <phrase>hausdorff</phrase> distance
<phrase>operational semantics</phrase> for timed systems: a non-standard approach to uniform modeling of timed and <phrase>hybrid</phrase> systems
interacting code motion transformations: their impact and their complexity.
kryptographische verfahren in der datenverarbeitung
computational <phrase>cardiology</phrase>: modeling of <phrase>anatomy</phrase>, <phrase>electrophysiology</phrase>, and <phrase>mechanics</phrase>
darstellung und nutzung von expertenwissen <phrase>fr</phrase> ein bildanalysesystem
<phrase>data compression</phrase>: the complete reference, <phrase>3rd edition</phrase>
a <phrase>relational</phrase> theory of <phrase>computing</phrase>
using sophisticated models in resolution <phrase>theorem proving</phrase>
<phrase>workflow</phrase> <phrase>management</phrase> systems for process organisations
<phrase>view synthesis</phrase> using <phrase>stereo</phrase> vision
this <phrase>thesis</phrase> investigates the use of <phrase>stereo</phrase> vision for the application of <phrase>view synthesis</phrase>. <phrase>view synthesis</phrase> --the problem of creating images of a scene as it would appear from novel viewpoints --has traditionally been approached using methods from computer graphics. these methods, however, suffer from low rendering speed, limited achievable realism, and, most severely, their dependence on a global scene <phrase>model</phrase>, which typically needs to be constructed manually. in this <phrase>thesis</phrase>, we present a new approach to <phrase>view synthesis</phrase> that avoids the above problems by synthesizing new views from existing images of a scene. using an <phrase>image-based</phrase> representation of scene <phrase>geometry</phrase> computed by <phrase>stereo</phrase> vision methods, a global <phrase>model</phrase> can be avoided, and realistic new views can be synthesized quickly using image <phrase>warping</phrase>. the new application of <phrase>stereo</phrase> for <phrase>view synthesis</phrase> makes it necessary to re-evaluate the requirements on <phrase>stereo</phrase> <phrase>algorithms</phrase>. we compare <phrase>view synthesis</phrase> to several traditional applications of <phrase>stereo</phrase>, and conclude that <phrase>stereo</phrase> vision is better suited for <phrase>view synthesis</phrase> than for applications requiring explicit 3d <phrase>reconstruction</phrase>. we also discuss ways of dealing with partially occluded regions of unknown depth and with completely occluded regions of unknown texture, and present experiments demonstrating that it is possible to efficiently synthesize realistic new views even from inaccurate and incomplete depth <phrase>information</phrase>. this <phrase>thesis</phrase> also contributes several novel <phrase>stereo</phrase> <phrase>algorithms</phrase> that are motivated by the specific requirements imposed by <phrase>view synthesis</phrase>. we introduce a new evidence measure based on intensity gradients for establishing correspondences between images. this measure combines the notions of similarity and confidence, and allows stable matching and easy assigning of canonical depth interpretations in image regions of insufficient <phrase>information</phrase>. we also present new <phrase>diffusion</phrase>-based <phrase>stereo</phrase> <phrase>algorithms</phrase> that are motivated by the need to correctly recover object boundaries. in particular, we develop a novel <phrase>bayesian</phrase> estimation technique that significantly outperforms <phrase>area</phrase>-based <phrase>algorithms</phrase> using fixed-sized <phrase>windows</phrase>. we provide <phrase>experimental</phrase> <phrase>results</phrase> for all <phrase>algorithms</phrase> on both <phrase>synthetic and real</phrase> images.
the <phrase>automation</phrase> of reasoning with <phrase>incomplete information</phrase>, from <phrase>semantic</phrase> foundations to efficient computation.
<phrase>universal</phrase> routing strategies for interconnection networks
bildanalyse allgemeiner dokumente
migrationssteuerung und konfigurationsverwaltung <phrase>fr</phrase> verteilte objektorientierte anwendungen
nonmonotonic logics, <phrase>basic</phrase> concepts, <phrase>results</phrase>, and techniques
inductive synthesis of functional programs, <phrase>universal</phrase> planning, folding of finite programs, and schema abstraction by analogical reasoning
computational aspects of an <phrase>order</phrase>-sorted <phrase>logic</phrase> with term declarations
gpss-<phrase>fortran</phrase>, version <phrase>ii</phrase>: einfhrung in die <phrase>simulation</phrase> diskreter systeme <phrase>mit</phrase> hilfe eines <phrase>fortran</phrase>-programmpaketes
meta-level control for <phrase>deductive</phrase> <phrase>database systems</phrase>
relationen und graphen
relations and <phrase>graphs</phrase> - <phrase>discrete mathematics</phrase> for computer scientists
theorie der logischen programmierung
spatial <phrase>data</phrase> types for <phrase>database systems</phrase>, finite resolution <phrase>geometry</phrase> for <phrase>geographic information systems</phrase>
<phrase>peer-to-peer</phrase>: konomische, technische und juristische perspektiven, das aktuelle <phrase>p2p</phrase>-buch.
complexity and structure
generierung von worthypothesen in kontinuierlicher sprache
<phrase>programming</phrase> constraint services: <phrase>high</phrase>-level <phrase>programming</phrase> of standard and new constraint services
testmustergenerierung und fehlersimulation in digitalen schaltungen <phrase>mit</phrase> hoher komplexitt
objective coordination in <phrase>multi-agent</phrase> system <phrase>engineering</phrase> - <phrase>design</phrase> and implementation
<phrase>security</phrase> <phrase>engineering</phrase> with patterns - origins, theoretical models, and new applications
higher-level hardware synthesis
paragon: a <phrase>language</phrase> using type hierarchies for the specification, implementation and selection of abstract <phrase>data</phrase> types
multiagent systems - a theoretical framework for intentions, know-how, and communications
<phrase>evolution</phrase> of parallel cellular machines, the cellular <phrase>programming</phrase> approach
matrix eigensystem routines - eispack guide, second edition
from <phrase>logic</phrase> <phrase>design</phrase> to <phrase>logic programming</phrase>: <phrase>theorem proving</phrase> techniques and p-functions
grading <phrase>knowledge</phrase>, extracting <phrase>degree</phrase> <phrase>information</phrase> from texts.
"text <phrase>knowledge</phrase> extraction" maps <phrase>natural language</phrase> texts onto a formal representation of the facts contained in the texts. common text <phrase>knowledge</phrase> extraction methods show a severe lack of methods for understanding <phrase>natural language</phrase> "<phrase>degree</phrase> expressions", like "expensive <phrase>hard disk drive</phrase>" and "good monitor", which describe gradable properties like price and quality, respectively. however, without an adequate understanding of such <phrase>degree</phrase> expressions it is often impossible to grasp the central meaning of a text. this <phrase>book</phrase> shows concise and <phrase>comprehensive</phrase> concepts for extracting <phrase>degree</phrase> <phrase>information</phrase> from <phrase>natural language</phrase> texts. it researches this task with regard to the three levels of (i) analysing <phrase>natural language</phrase> <phrase>degree</phrase> expressions, (<phrase>ii</phrase>) representing them in a terminologic framework, and (iii) inferencing on them byconstrain t propagation. on each of these three levels, the <phrase>author</phrase> shows that former approaches to the <phrase>degree</phrase> understanding problem were too simplistic, since theyignored byand large the role of the background <phrase>knowledge</phrase> involved. thus, he gives a constructive verification of his central <phrase>hypothesis</phrase>, viz. that the proper extraction of grading <phrase>knowledge</phrase> relies heavilyon background grading <phrase>knowledge</phrase>. this <phrase>construction</phrase> proceeds as follows. first, the <phrase>author</phrase> gives an overview of the parsetalk <phrase>information extraction</phrase> system. then, from the review of relevant <phrase>linguistic</phrase> <phrase>literature</phrase>, the <phrase>author</phrase> derives two distinct categories of <phrase>natural language</phrase> <phrase>degree</phrase> expressions and proposes <phrase>knowledge</phrase>-intensive <phrase>algorithms</phrase> to handle their analyses in the parsetalk system. these methods are applied to two text domains, viz. a <phrase>medical diagnosis</phrase> domain and a repositoryof texts from <phrase>information</phrase> technologymagazines. moreover, for inferencing the <phrase>author</phrase> generalizes from well-known constraint propagation mechanisms. this generalization is especiallyapt for representing and reasoning with <phrase>natural language</phrase> <phrase>degree</phrase> expressions, but it is also interesting from the point of view where it originated, viz. the field of temporal reasoning. the conclusion of the <phrase>book</phrase> gives an integration of all three levels of understanding showing that their coupling leads to an even more advanced -- and more efficient -- performance of the proposed mechanisms.
handbook on <phrase>ontologies</phrase>
ausfhrbare spezifikation von directory-systemen in einer logischen sprache
<phrase>java</phrase> and the <phrase>java virtual machine</phrase>: definition, verification, validation
datenschutz bei riskanten systemen: eine konzeption entwickelt am beispiel eines medizinischen informationssystems
stateless core: a scalable approach for quality of service in the <phrase>internet</phrase>, winning <phrase>thesis</phrase> of the 2001 <phrase>acm</phrase> doctoral dissertation competition
<phrase>test</phrase> von <phrase>osi</phrase>-protokollen
fehlertoleranz in verteilten realzeitsystemen: anwendungsorientierte techniken
maschinen-unabhngige code-erzeugung als semantikerhaltende beweisbare programmtransformation
konzepte <phrase>fr</phrase> eine verteilte wissensbasierte softwareproduktionsumgebung
a hierarchical associative processing system
<phrase>multimedia</phrase> <phrase>database</phrase> system: issues and <phrase>research</phrase> direction
efficient checking of <phrase>polynomials</phrase> and proofs anf the hardness of approximation problems
<phrase>turing machines</phrase> with sublogarithmic space
ada 95 <phrase>reference manual</phrase>, <phrase>language</phrase> and standard <phrase>libraries</phrase>, international standard <phrase>iso</phrase>/<phrase>iec</phrase> 8652: 1995(<phrase>e</phrase>)
consolidated ada <phrase>reference manual</phrase>. <phrase>language</phrase> and standard <phrase>libraries</phrase>, international standard <phrase>iso</phrase>/<phrase>iec</phrase> 8652/1995(<phrase>e</phrase>) with technical corrigendum 1
artificail <phrase>perception</phrase> and <phrase>music</phrase> recognition
finite representations of ccs and tcsp programs by <phrase>automata</phrase> and <phrase>petri nets</phrase>
flagorientierte assoziativspeicher und -prozessoren
towards dynamic <phrase>randomized</phrase> <phrase>algorithms</phrase> in <phrase>computational geometry</phrase>
<phrase>boolean</phrase> caclulus of differences
p-functions and <phrase>boolean</phrase> matrix factorization: a unified approach for <phrase>wired</phrase>, programmed and microprogrammed implementations of discrete <phrase>algorithms</phrase>
<phrase>algorithms</phrase> for parallel <phrase>polygon</phrase> rendering
challenges for <phrase>action</phrase> theories
the <phrase>computational complexity</phrase> of equivalence and <phrase>isomorphism</phrase> problems
visualization of scientific parallel programs
<phrase>global optimization</phrase>
current trends in concurrency, overviews and tutorials
extraction and exploitation of intensional <phrase>knowledge</phrase> from heterogeneous <phrase>information</phrase> sources: <phrase>semi-automatic</phrase> approaches and tools
<phrase>algorithms</phrase> on <phrase>trees</phrase> and <phrase>graphs</phrase>
uncertainty handling and quality assessment in <phrase>data mining</phrase>
:"uncertainty handling and quality assessment in <phrase>data mining</phrase> provides an introduction to the application of these concepts in <phrase>knowledge</phrase> discovery and <phrase>data mining</phrase>. it reviews the <phrase>state</phrase>-of-the-<phrase>art</phrase> in uncertainty handling and discusses a framework for unveiling and handling uncertainty. coverage of quality assessment begins with an introduction to <phrase>cluster analysis</phrase> and a comparison of the methods and approaches that may be used. the techniques and <phrase>algorithms</phrase> involved in other essential <phrase>data mining</phrase> tasks, such as classification and extraction of association rules, are also discussed together with a review of the quality criteria and techniques for evaluating the <phrase>data mining</phrase> <phrase>results</phrase>." "this <phrase>book</phrase> presents a <phrase>general</phrase> framework for assessing quality and handling uncertainty, which is based on tested concepts and theories. this framework forms the basis of an implementation tool, 'uminer' which is introduced to the reader for the first time." aimed at it professionals involved with <phrase>data mining</phrase> and <phrase>knowledge</phrase> discovery, the work is supported with case studies from <phrase>epidemiology</phrase> that illustrate how the tool works in '<phrase>real-world</phrase>' <phrase>data mining</phrase> projects. the <phrase>book</phrase> would also be of interest to final year undergraduates or post-graduate students looking at <phrase>databases</phrase>, <phrase>algorithms</phrase>, <phrase>artificial intelligence</phrase> and <phrase>information</phrase> systems particularly with regard to uncertainty and quality assessment.
interactive <phrase>multimedia</phrase> documents: modeling, authoring, and implementation experiences
<phrase>matchmaking</phrase> in <phrase>electronic</phrase> markets - an <phrase>agent-based</phrase> approach towards <phrase>matchmaking</phrase> in <phrase>electronic</phrase> negotiations
planning and learning by analogical reasoning
closed object boundaries from scattered points
intelligent <phrase>information</phrase> integration for the <phrase>semantic web</phrase>
signaturanalyse: theoretische grundlagen und probleme; ausblick auf anwendungen
<phrase>software</phrase>-diversitt und ihre modellierung: <phrase>software</phrase>-fehlertoleranz und ihre bewertung durch fehler- und kostenmodelle
modular <phrase>construction</phrase> and partial <phrase>order</phrase> <phrase>semantics</phrase> of <phrase>petri nets</phrase>
<phrase>relational</phrase> matching
vivid <phrase>logic</phrase>: <phrase>knowledge</phrase>-based reasoning with two kinds of <phrase>negation</phrase>
natrlichsprachliche argumentation in dialogsystemen: ki-verfahren zur rekonstruktion und erklrung approximativer inferenzprozesse
alternating sequential/<phrase>parallel processing</phrase>
<phrase>integer</phrase> optimization by local search
datenbankgesttzte reprsentation und extraktion von episodenbeschreibungen aus bildfolgen
<phrase>data mining</phrase> in <phrase>bioinformatics</phrase>
the <phrase>logic</phrase> of <phrase>information</phrase> structures
the generic development <phrase>language</phrase> deva: presentation and case studies
komplexittstheorie: grenzen der effizienz von algorithmen
a methodology for uncertainty in <phrase>knowledge-based systems</phrase>
revisions- und konsistenzkontrolle in einer integrierten softwareentwicklungsumgebung
models and tools for managing development processes
bersetzerbau - theorie, konstruktion, generierung
bersetzerbau - theorie, konstruktion, generierung, 2. auflage
instantiation theory - on the foundations of automated deduction
systematische <phrase>software</phrase>-qualittssicherung anhand von qualitts- und produktmodellen
<phrase>programming</phrase> in <phrase>modula 2</phrase>
programmieren in <phrase>modula-2</phrase>
grammars and l forms: an introduction
<phrase>artificial intelligence</phrase> today: recent trends and developments
probabilistische verfahren <phrase>fr</phrase> den <phrase>test</phrase> hochintegrierter schaltungen
<phrase>multicast</phrase>-kommunikation in verteilten systemen
<phrase>attribute grammar</phrase> inversion and source-to-source <phrase>translation</phrase>
<phrase>high</phrase>-dimensional indexing: transformational approaches to <phrase>high</phrase>-dimensional <phrase>range</phrase> and similarity searches
association rule <phrase>mining</phrase>, models and <phrase>algorithms</phrase>
due to the popularity of <phrase>knowledge</phrase> discovery and <phrase>data mining</phrase>, in practice as well as among <phrase>academic</phrase> and corporate <phrase>r&d</phrase> professionals, association rule <phrase>mining</phrase> is receiving increasing attention. the authors present the recent progress achieved in <phrase>mining</phrase> quantitative association rules, causal rules, exceptional rules, negative association rules, association rules in multi-<phrase>databases</phrase>, and association rules in small <phrase>databases</phrase>. this <phrase>book</phrase> is written for researchers, professionals, and students working in the fields of <phrase>data mining</phrase>, <phrase>data analysis</phrase>, <phrase>machine learning</phrase>, and <phrase>knowledge</phrase> discovery in <phrase>databases</phrase>, and for anyone who is interested in association rule <phrase>mining</phrase>.
<phrase>agent-based</phrase> <phrase>hybrid</phrase> <phrase>intelligent systems</phrase>: an <phrase>agent-based</phrase> framework for complex <phrase>problem solving</phrase>
kopplung von rechnernetzen: techniken zu planung, entwurf, vermessung und leistungsoptimierung
automatische komplexittsanalyse funktionaler programme
y12m - <phrase>solution</phrase> of large and sparse systems of linear <phrase>algebraic</phrase> equations
john zukowski's definitive guide to swing for <phrase>java</phrase> 2
compositionality, concurrency and partial correctness - proof theories for networks of processes, and their relationship
<phrase>multi-agent</phrase> <phrase>programming</phrase>: languages, platforms and applications
web dynamics - adapting to change in content, size, <phrase>topology</phrase> and use
understanding planning tasks: domain complexity and <phrase>heuristic</phrase> decomposition.
metrics for process models: empirical foundations of verification, error prediction, and guidelines for correctness.
omdoc - an open <phrase>markup</phrase> format for <phrase>mathematical</phrase> documents [version 1.2].
<phrase>software</phrase> visualization - visualizing the structure, behaviour, and <phrase>evolution</phrase> of <phrase>software</phrase>.
do-all <phrase>computing</phrase> in <phrase>distributed systems</phrase>: cooperation in the presence of adversity.
<phrase>resource allocation</phrase> in <phrase>wireless networks</phrase>: theory and <phrase>algorithms</phrase>.
case-based approximate reasoning
concurrent zero-<phrase>knowledge</phrase> - with additional background by oded goldreich
dissemination of <phrase>information</phrase> in <phrase>communication</phrase> networks - <phrase>broadcasting</phrase>, gossiping, leader <phrase>election</phrase>, and <phrase>fault-tolerance</phrase>
<phrase>algorithms</phrase> and <phrase>data structures</phrase>: the <phrase>basic</phrase> toolbox.
formal models of communicating systems - languages, <phrase>automata</phrase>, and monadic <phrase>second-order logic</phrase>
web <phrase>data mining</phrase>: exploring <phrase>hyperlinks</phrase>, contents, and usage <phrase>data</phrase>
web <phrase>mining</phrase> aims to discover useful <phrase>information</phrase> and <phrase>knowledge</phrase> from the web <phrase>hyperlink</phrase> structure, page contents, and usage <phrase>data</phrase>. although web <phrase>mining</phrase> uses many conventional <phrase>data mining</phrase> techniques, it is not purely an application of traditional <phrase>data mining</phrase> due to the semistructured and unstructured <phrase>nature</phrase> of the web <phrase>data</phrase> and its heterogeneity. it has also developed many of its own <phrase>algorithms</phrase> and techniques. liu has written a <phrase>comprehensive</phrase> text on web <phrase>data mining</phrase>. key topics of structure <phrase>mining</phrase>, content <phrase>mining</phrase>, and usage <phrase>mining</phrase> are <phrase>covered</phrase> both in breadth and in depth. his <phrase>book</phrase> brings together all the essential concepts and <phrase>algorithms</phrase> from related areas such as <phrase>data mining</phrase>, <phrase>machine learning</phrase>, and text processing to form an authoritative and coherent text. the <phrase>book</phrase> offers a rich blend of theory and practice, addressing seminal <phrase>research</phrase> ideas, as well as examining the <phrase>technology</phrase> from a practical point of view. it is suitable for students, researchers and practitioners interested in web <phrase>mining</phrase> both as a learning text and a reference <phrase>book</phrase>. lecturers can readily use it for classes on <phrase>data mining</phrase>, web <phrase>mining</phrase>, and <phrase>web search</phrase>. additional teaching materials such as lecture slides, datasets, and implemented <phrase>algorithms</phrase> are available online. the <phrase>acm</phrase> portal is published by the <phrase>association for computing machinery</phrase>. <phrase>copyright</phrase> 2010 <phrase>acm</phrase>, inc. terms of usage <phrase>privacy policy</phrase> code of <phrase>ethics</phrase> contact us useful downloads: <phrase>adobe acrobat</phrase> <phrase>quicktime</phrase> <phrase>windows media player</phrase> real player
<phrase>data quality</phrase>: concepts, methodologies and techniques
<phrase>web services</phrase> - concepts, architectures and applications
ada 2005 rationale: the <phrase>language</phrase>, the standard <phrase>libraries</phrase>
ada 2005 <phrase>reference manual</phrase>. <phrase>language</phrase> and standard <phrase>libraries</phrase> - international standard <phrase>iso</phrase>/<phrase>iec</phrase> 8652/1995 (<phrase>e</phrase>) with technical corrigendum 1 and amendment 1
<phrase>cooperative</phrase> bug isolation (winning <phrase>thesis</phrase> of the 2005 <phrase>acm</phrase> doctoral dissertation competition).
secure transaction protocol analysis: models and applications
<phrase>neural networks</phrase> - a systematic introduction
<phrase>grid computing</phrase>, experiment <phrase>management</phrase>, tool integration, and scientific workflows
<phrase>business process management</phrase>: concepts, languages, architectures
pedagogically founded courseware generation for <phrase>web-based</phrase> learning, an htn-planning-<phrase>based approach</phrase> implemented in paigos
kleeme-algebren formaler ausdrcke
skriptum informatik: eine konventionelle einfhrung, 5. aufl.
skriptum informatik: eine konventionelle einfhrung
skriptum informatik: eine konventionelle einfhrung, 2., durchges. aufl.
skriptum informatik: eine konventionelle einfhrung, 3., durchges. und erw. aufl.
skriptum informatik: eine konventionelle einfhrung, 4., durchges. aufl.
starthilfe informatik, 2., durchges. aufl.
starthilfe informatik
algorithmischen konzepte der informatik - berechenbarkeit, komplexittstheorie, algorithmik, kryptographie
grundlagen der programmiersprachen
effiziente algorithmen
mathematische grundlagen der informatik - mathematisches denken und beweisen, <phrase>eine einfhrung</phrase>
mathematische grundlagen der informatik - mathematisches denken und beweisen, <phrase>eine einfhrung</phrase>, 2. auflage
transaktionssysteme
einfhrung in die komplexittstheorie
archivierung in datenbanksystemen - konzept und sprache
datenbanksysteme: konzepte und modelle
arithmetik in rechananlagen
aufgaben zum skriptum informatik
aufgaben zum skriptum informatik, 2., durchges. aufl.
backup und recovery in datenbanksystemen
semantik und programmverifikation
the complexity of <phrase>boolean</phrase> functions
kompendium theoretische informatik - eine ideensammlung
effiziente algorithmen <phrase>fr</phrase> grundlegende funktionen (2. auflage)
theoretische informatik - eine algorithmenorientierte einfhrung (2. auflage)
compilerbau - <phrase>eine einfhrung</phrase>
didaktik der informatik, <phrase>mit</phrase> praxiserprobtem unterrichtsmaterial.
didaktik der informatik, <phrase>mit</phrase> praxiserprobtem unterrichtsmaterial, 2. auflage.
expertensysteme <phrase>fr</phrase> die planung der produktion
grundlagen von informationssystemen
deduktive datenbanken: <phrase>eine einfhrung</phrase> aus der sicht der logischen programmierung
anfrageverarbeitung in datenbanksystemen - entwurfs- und implementierungskonzepte
datenbank-<phrase>engineering</phrase>: analyse, entwurf und implementierung objektrelationaler datenbanken <phrase>mit</phrase> <phrase>uml</phrase>, <phrase>db2</phrase>-<phrase>sql</phrase> und <phrase>java</phrase>
hochleistungs-transaktionssysteme. konzepte und entwicklungen moderner datenbankarchitekturen.
bilddatenkompression: grundlagen, codierung, <phrase>mpeg</phrase>, <phrase>jpeg</phrase>
automatisierung von terminierungsbeweisen.
classification and <phrase>regression</phrase> <phrase>trees</phrase>.
the <phrase>probabilistic method</phrase>
environmental systems <phrase>research</phrase> institute: understanding <phrase>gis</phrase> - the arc/info method, 3rd ed.
modeling the <phrase>internet</phrase> and the web: <phrase>probabilistic method</phrase> and <phrase>algorithms</phrase>
sampling techniques
sampling techniques
sampling techniques, <phrase>3rd edition</phrase>.
exploratory <phrase>data mining</phrase> and <phrase>data</phrase> cleaning
<phrase>compiler</phrase> <phrase>construction</phrase> for <phrase>digital</phrase> <phrase>computers</phrase>
<phrase>modern compiler</phrase> <phrase>design</phrase>
rdb/<phrase>vms</phrase>: developing the <phrase>data warehouse</phrase>
classification <phrase>algorithms</phrase>
<phrase>garbage collection</phrase>: <phrase>algorithms</phrase> for automatic dynamic <phrase>memory management</phrase>.
finding groups in <phrase>data</phrase>: an introduction to <phrase>cluster analysis</phrase>.
the <phrase>data warehouse</phrase> toolkit: practical techniques for building dimensional <phrase>data</phrase> warehouses.
foundations of <phrase>programming languages</phrase>
the essential distributed objects survival guide.
<phrase>corba</phrase> fundamentals and <phrase>programming</phrase>.
applied operating system concepts, first edition
operating system concepts, sixth edition
statistical decision funtions.
theory of modeling and <phrase>simulation</phrase>
<phrase>high</phrase> performance parallel <phrase>database</phrase> processing and grid <phrase>databases</phrase>
this <phrase>book</phrase> targets the theoretical/conceptual details needed to form a base of understanding and then delivers <phrase>information</phrase> on development, implementations, and analytical modeling of parallel <phrase>databases</phrase>. it includes key <phrase>information</phrase> on new developments with grid <phrase>databases</phrase>. also uses a theoretical and practical balance to support in-depth study of parallel <phrase>query processing</phrase> offered by modern <phrase>dbms</phrase> as well as hands on experience of parallel query <phrase>algorithms</phrase> development, implementation, and analysis.
creating <phrase>metabolic</phrase> network models using <phrase>text mining</phrase> and expert <phrase>knowledge</phrase>.
<phrase>graph theoretic</phrase> <phrase>sequence</phrase> clustering <phrase>algorithms</phrase> and their applications to <phrase>genome</phrase> comparison.
introduction to self-assembling <phrase>dna</phrase> <phrase>nanostructures</phrase> for computation and nanofabrication.
phyloinformatics and <phrase>tree</phrase> networks.
<phrase>high</phrase>-grade <phrase>ore</phrase> for <phrase>data mining</phrase> in 3d structures.
exploring <phrase>rna</phrase> intermediate conformations with the massively parallel <phrase>genetic algorithm</phrase>.
mapping <phrase>sequence</phrase> to <phrase>rice</phrase> fpc.
interrelated clustering: an approach for <phrase>gene expression</phrase> <phrase>data analysis</phrase>.
<phrase>protein</phrase> classification: a geometric <phrase>hashing</phrase> approach.
the <phrase>protein</phrase> <phrase>information</phrase> resource for <phrase>functional genomics</phrase> and <phrase>proteomics</phrase>.
<phrase>conservative</phrase> extension in structural <phrase>operational semantics</phrase>.
some pointed questions concerning asymptotic <phrase>lower</phrase> bounds, and <phrase>news</phrase> from the <phrase>isomorphism</phrase> front.
theoretical and <phrase>experimental</phrase> <phrase>dna</phrase> computation.
theory of <phrase>genetic algorithms</phrase>.
<phrase>propositional</phrase> proof complexity: past, present, and future.
the underlying <phrase>logic</phrase> of <phrase>hoare logic</phrase>.
<phrase>security</phrase> analysis using flow logics.
<phrase>quantum computing</phrase> and <phrase>communication</phrase> complexity.
more infinite <phrase>results</phrase>.
a machine <phrase>model</phrase> for the complexity of np-approximation problems.
functions versus <phrase>algorithms</phrase>.
characterizations of regular languages in low level complexity classes.
getgrats and appligraph: theory and applications of <phrase>graph</phrase> transformation.
networks of <phrase>language</phrase> processors.
networks of <phrase>language</phrase> processors: parallel communicating systems.
herbrand's theorem and equational reasoning: problems and solutions.
on the role of <phrase>formal specification</phrase> techniques: from tapsoft 1985 to etaps 2000.
on formal <phrase>semantics</phrase> and integration of <phrase>object-oriented</phrase> modeling languages.
theory and practice of <phrase>software development</phrase>: a review of driving forces and expectations of tapsoft from 1985 to 1997.
<phrase>algebraic</phrase> techniques in <phrase>software development</phrase>: a review of progress up to the mid nineties.
dynamic abstract <phrase>data</phrase> types: an informal proposal in 1994.
integration <phrase>paradigm</phrase> for <phrase>data type</phrase> and process specification techniques.
from <phrase>basic</phrase> views and aspects to integration of specification formalisms.
why <phrase>evolutionary algorithms</phrase>?
diagonalization.
what is branching time <phrase>semantics</phrase> and why to use it?
why are modal logics so robustly <phrase>decidable</phrase>?
on a reference <phrase>model</phrase> for the formalization and integration of <phrase>software</phrase> specification languages.
<phrase>homotopy</phrase> and concurrency.
amast'91 banquet <phrase>talk</phrase>.
the value, if any, of decidability.
<phrase>platonism</phrase>, <phrase>constructivism</phrase>, and computer proofs vs. proofs by hand.
from invariants to <phrase>canonization</phrase>.
the sequential asm <phrase>thesis</phrase>.
an introduction to <phrase>quantum computing</phrase>.
on slender languages.
the d0l problem revisited.
progress in descriptive complexity.
<phrase>dna</phrase> <phrase>computers</phrase>: tomorrow's <phrase>reality</phrase>.
the <phrase>genomics</phrase> <phrase>revolution</phrase> and its challenges for algorithmic <phrase>research</phrase>.
natural <phrase>data mining</phrase> techniques.
simple words in equality sets.
it is well known that equality sets between two morphisms possess a remarkable generative capacity: an arbitrary <phrase>recursively enumerable set</phrase> is obtained from an equality set by certain simple operations. interconnections between simplicity of computations and structural primitivity of words in equality sets have also been observed. the <phrase>paper</phrase> discusses recent work in this <phrase>area</phrase>, pointing out certain open problems and emphasizing new directions for <phrase>research</phrase>.
twelve problems in resource-bounded measure.
neural computation: a <phrase>research</phrase> topic for theoretical computer <phrase>science</phrase>? some thoughts and pointers.
words on trajectories.
many-valued truth functions, cernys' conjecture, and <phrase>road</phrase> coloring.
lindenmayer and <phrase>dna</phrase>: watson-<phrase>crick</phrase> d0l systems.
cofi: the common framework <phrase>initiative</phrase> for <phrase>algebraic</phrase> specification and development.
classification of <phrase>petri nets</phrase> using <phrase>adjoint functors</phrase>.
does concurrency theory have anything to say about <phrase>parallel programming</phrase>?
splicing: a challenge for <phrase>formal language</phrase> theorists.
<phrase>computing</phrase> with membranes (p systems): an introduction.
<phrase>logic</phrase> on words.
towards global computations guided by concurrency theory.
the complexity of <phrase>propositional</phrase> proofs.
current trends in theoretical computer <phrase>science</phrase>, entering the 21th century
string searching <phrase>algorithms</phrase>
<phrase>computational biology</phrase> and <phrase>genome</phrase> informatics
analysis of biological <phrase>data</phrase>: a <phrase>soft computing</phrase> approach
in silico <phrase>design</phrase> of <phrase>ligands</phrase> using properties of <phrase>target</phrase> active sites.
sophisticated methods for <phrase>cancer</phrase> classification using <phrase>microarray</phrase> <phrase>data</phrase>.
a reliable classification of <phrase>gene</phrase> clusters for <phrase>cancer</phrase> samples using a <phrase>hybrid</phrase> multi-objective <phrase>evolutionary</phrase> procedure.
reconstructing phylogenies with memetic <phrase>algorithms</phrase> and branch-and-bound.
an introduction to <phrase>soft computing</phrase>.
beyond string <phrase>algorithms</phrase>: <phrase>protein</phrase> <phrase>sequence analysis</phrase> using <phrase>wavelet</phrase> transforms.
multiobjective <phrase>evolutionary</phrase> approach to fuzzy clustering of <phrase>microarray</phrase> <phrase>data</phrase>.
inferring regulations in a <phrase>genomic</phrase> network from <phrase>gene expression</phrase> profiles.
distill: a <phrase>machine learning</phrase> approach to <phrase>ab initio</phrase> <phrase>protein structure prediction</phrase>.
filtering <phrase>protein</phrase> surface motifs using negative instances of active sites candidates.
<phrase>bioinformatics</phrase>: <phrase>mining</phrase> the massive <phrase>data</phrase> from <phrase>high</phrase> throughput <phrase>genomics</phrase> experiments.
classification of <phrase>rna</phrase> sequences with <phrase>support vector machines</phrase>.
<phrase>feature selection</phrase> for <phrase>cancer</phrase> classification using <phrase>ant colony</phrase> optimization and <phrase>support vector machines</phrase>.
six degrees: the <phrase>science</phrase> of a connected age.
<phrase>information retrieval</phrase> interaction.
standard codecs: <phrase>image compression</phrase> to advanced <phrase>video</phrase> coding.
neuronale netze - <phrase>eine einfhrung</phrase> in die grundlagen, anwendungen und datenauswertung
the <phrase>datacenter</phrase> as a computer: an introduction to the <phrase>design</phrase> of <phrase>warehouse</phrase>-scale machines
real-time active <phrase>range</phrase> finder using <phrase>light</phrase> intensity <phrase>modulation</phrase>.
strategies for registering <phrase>range</phrase> images from unknown <phrase>camera</phrase> positions.
<phrase>high</phrase>-resolution ultrafast 3d imaging.
<phrase>optoelectronic</phrase> dimensional <phrase>integral</phrase> inspection of hollow cylinder-type articles for <phrase>conveyor</phrase> line.
development of a 3d digitizer for <phrase>breast</phrase> <phrase>surgery</phrase> procedures.
automatic <phrase>reconstruction</phrase> of large 3d models of real environments from unregistered <phrase>data</phrase>-sets.
three-line <phrase>high</phrase>-power <phrase>three-dimensional</phrase> <phrase>sensor</phrase>.
multispectral pattern projection <phrase>range</phrase> finder.
<phrase>high</phrase>-speed <phrase>three-dimensional</phrase> <phrase>laser</phrase> <phrase>sensor</phrase>.
3d <phrase>range</phrase> optical <phrase>sensor</phrase>: analysis of the measurement errors and development of procedures for their compensation.
3d profilometry using a dynamically configurable <phrase>confocal</phrase> <phrase>microscope</phrase>.
examining <phrase>laser</phrase> <phrase>triangulation</phrase> system performance using a <phrase>software</phrase> <phrase>simulation</phrase>.
effect of sway on image fidelity in whole-body <phrase>digitizing</phrase>.
moly: a <phrase>prototype</phrase> handheld 3d digitizer with <phrase>diffraction</phrase> <phrase>optics</phrase>.
<phrase>reconstruction</phrase> of the surface of the <phrase>human</phrase> body from 3d scanner <phrase>data</phrase> using 13-splines.
<phrase>reverse engineering</phrase> using optical 3d sensors.
wrapping 3d scanning <phrase>data</phrase>.
<phrase>spherical harmonic</phrase> surface representation with <phrase>feedback</phrase> control.
efficient <phrase>free</phrase>-form surface representation with application in <phrase>orthodontics</phrase>.
real-time 3d shape measurement with <phrase>digital</phrase> stripe projection by <phrase>texas instruments</phrase> micro <phrase>mirror</phrase> devices dmd.
multiscale analysis of 3d surface image: application to <phrase>clam</phrase> shell characterization.
multiple <phrase>structured light</phrase> system for the 3d measurement of feet.
pose and <phrase>motion estimation</phrase> using dual <phrase>quaternion</phrase>-based extended kalman filtering.
direct estimation of 3d motion parameters and relative depth using the <phrase>minimum description length</phrase> principle.
depth-based selective image <phrase>reconstruction</phrase> using spatiotemporal <phrase>image analysis</phrase>.
toward a handheld <phrase>laser</phrase> <phrase>range</phrase> scanner: integrating observation-based <phrase>motion compensation</phrase>.
slicing, fitting, and linking (<phrase>sfl</phrase>): a modular <phrase>triangulation</phrase> approach.
3d object <phrase>reconstruction</phrase> from a <phrase>sequence</phrase> of images using <phrase>voxel</phrase> coloring.
3d profiling by optical <phrase>demodulation</phrase> with an <phrase>image intensifier</phrase>.
<phrase>restoration</phrase> of broken <phrase>earthenware</phrase> using close <phrase>range</phrase> <phrase>photogrammetry</phrase> and a <phrase>cad</phrase> system.
real-time 3d <phrase>reconstruction</phrase> system using cam-based highly <phrase>parallel processing</phrase> board.
novel fully integrated computer system for custom <phrase>footwear</phrase>: from 3d digitization to <phrase>manufacturing</phrase>.
<phrase>reconstruction</phrase> of complete 3d object <phrase>model</phrase> from multiview <phrase>range</phrase> images.
color <phrase>digitizing</phrase> and modeling of <phrase>free</phrase>-form 3d objects.
object modeling in multiple-object 3d scene using deformable <phrase>simplex</phrase> meshes.
robust 3d <phrase>reconstruction</phrase> system for <phrase>human</phrase> jaw modeling.
acquisition of 3d image representation in <phrase>multimedia</phrase> ambiance <phrase>communication</phrase> using 3d <phrase>laser</phrase> scanner and <phrase>digital camera</phrase>.
new approach for the modeling and smoothing of scattered 3d <phrase>data</phrase>.
fast shape from focus using <phrase>dynamic programming</phrase>.
axi-vision <phrase>camera</phrase>: a <phrase>three-dimensional</phrase> <phrase>camera</phrase>.
real-time <phrase>structured light</phrase> depth extraction.
impact of intensity edge map on segmentation of noisy <phrase>range</phrase> images.
<phrase>interpolation</phrase> of ray-space <phrase>data</phrase> by adaptive filtering.
three steps to make <phrase>shape from shading</phrase> work consistently on real scenes.
robust cooperation concept for low-level vision modules.
error sensitivity of rotation angles in the <phrase>icp</phrase> <phrase>algorithm</phrase>.
3d surface real-time measurement using phase-shifted interference fringe technique for <phrase>craniofacial</phrase> identification.
automated fudicial labeling on <phrase>human</phrase> body <phrase>data</phrase>.
segmenting 3d surface scan <phrase>data</phrase> of the <phrase>human</phrase> body by 2d projection.
extracting surface <phrase>area</phrase> coverage by superimposing 3d scan <phrase>data</phrase>.
<phrase>entropy</phrase> of profile sections to estimate the next <phrase>sensor</phrase> position.
optimizing triangular mesh generation from <phrase>range</phrase> images.
cybermodeler: a compact 3d scanner based on monoscopic <phrase>camera</phrase>.
adaptive <phrase>area</phrase>-based <phrase>stereo</phrase> matching.
gel tomography for 3d acquisition of <phrase>plant</phrase> <phrase>root</phrase> systems.
real-time monitoring of <phrase>icebreaker</phrase> <phrase>propeller</phrase> blades' <phrase>ice</phrase> load using underwater <phrase>laser</phrase> ranging system.
scanning projection <phrase>grating</phrase> moire <phrase>topography</phrase>.
shapegrabber footscanner: a low cost <phrase>high</phrase> accuracy 3d system for the acquisition of <phrase>human</phrase> feet.
3d <phrase>reconstruction</phrase>, visualization, and measurement of <phrase>mri</phrase> images.
generating <phrase>animated</phrase> sequences from 3d whole-body scans.
tilted planes in 3d <phrase>image analysis</phrase>.
monitoring and measurement of movement of objects by fringe projection method.
parameters matching of objects in <phrase>video</phrase> sequences.
optimizing random patterns for invariants-based identification.
<phrase>three-dimensional</phrase> scene <phrase>reconstruction</phrase> from images.
3d shape initialization of objects in multiview <phrase>image sequences</phrase>.
3d textured models of indoor scenes from composite <phrase>range</phrase> and <phrase>video</phrase> images.
<phrase>three-dimensional</phrase> shape <phrase>reconstruction</phrase> from two-dimensional images using symmetric <phrase>camera</phrase> location.
new class <phrase>library</phrase> for <phrase>animation</phrase> of <phrase>voxel</phrase> humans.
opto-<phrase>numerical methods</phrase> of <phrase>data acquisition</phrase> for computer graphics and <phrase>animation</phrase> systems.
viro 3d: tast <phrase>three-dimensional</phrase> full-body scanning for humans and other living objects.
<phrase>high</phrase>-resolution <phrase>triangulation</phrase> of arbitrary shaped surfaces based on coordinate curves.
differential motions for recovering 3d structure and motions from an unstructured environment.
recreation of <phrase>three-dimensional</phrase> objects in a real-time simulated environment by means of a panoramic <phrase>single</phrase> <phrase>lens</phrase> <phrase>stereoscopic</phrase> image-capturing device.
volumetric apparel for visible female.
gaussian <phrase>scale-space</phrase> dense disparity estimation with <phrase>anisotropic</phrase> disparity-field <phrase>diffusion</phrase>.
we present a new reliable dense disparity estimation <phrase>algorithm</phrase> which employs gaussian <phrase>scale-space</phrase> with <phrase>anisotropic</phrase> disparity-field <phrase>diffusion</phrase>. this <phrase>algorithm</phrase> estimates edge-preserving dense disparity vectors using a <phrase>diffusive</phrase> method on iteratively gaussian-filtered images with a scale, i.e. the gaussian scalespace. while a gaussian filter kernel generates a coarser resolution from <phrase>stereo</phrase> image pairs, only strong and meaningful boundaries are adaptively selected on the resolution of the filtered images. then, coarse global disparity vectors are initialized using the boundary constraint. the per-<phrase>pixel</phrase> disparity vectors are iteratively obtained by the local adjustment of the global disparity vectors using an <phrase>energy</phrase>-minimization framework. the proposed <phrase>algorithm</phrase> preserves the boundaries while inner regions are smoothed using <phrase>anisotropic</phrase> disparity-field <phrase>diffusion</phrase>. in this work, the gaussian <phrase>scale-space</phrase> efficiently avoids illegal matching on a large baseline by the restriction of the <phrase>range</phrase>. moreover, it prevents the computation from iterating into local minima of ill-posed <phrase>diffusion</phrase> on large <phrase>gradient</phrase> areas e.g. shadow and texture <phrase>region</phrase>, etc. the <phrase>experimental</phrase> <phrase>results</phrase> prove the excellent localization performance preserving the disparity discontinuity of each object.
projection-based registration using a <phrase>multi-view</phrase> <phrase>camera</phrase> for indoor scene <phrase>reconstruction</phrase>.
a registration method is proposed for 3d <phrase>reconstruction</phrase> of an indoor environment using a <phrase>multi-view</phrase> <phrase>camera</phrase>. in <phrase>general</phrase>, previous methods have a <phrase>high</phrase> <phrase>computational complexity</phrase> and are not robust for 3d <phrase>point cloud</phrase> with low precision. thus, a projection-based registration is presented. first, depth are refined based on temporal <phrase>property</phrase> by excluding 3d points with a large variation, and spatial <phrase>property</phrase> by filling holes referring neighboring 3d points. second, 3d point clouds acquired at two views are projected onto the same image plane, and two-step <phrase>integer</phrase> mapping enables the modified klt to find correspondences. then, fine registration is carried out by minimizing distance errors. finally, a final color is evaluated using colors of corresponding points and an indoor environment is reconstructed by applying the above procedure to consecutive scenes. <phrase>the proposed method</phrase> reduces <phrase>computational complexity</phrase> by searching for correspondences within an image plane. it not only enables an effective registration even for 3d <phrase>point cloud</phrase> with low precision, but also need only a few views. the generated <phrase>model</phrase> can be adopted for interaction with as well as <phrase>navigation</phrase> in a <phrase>virtual environment</phrase>.
registration of <phrase>multiple range</phrase> scans as a location recognition problem: <phrase>hypothesis</phrase> generation, refinement and verification.
this <phrase>paper</phrase> addresses the following version of the <phrase>multiple range</phrase> scan registration problem. a scanner with an associated intensity <phrase>camera</phrase> is placed at a series of locations throughout a large environment; scans are acquired at each location. the problem is to decide automatically which scans overlap and to estimate the parameters of the transformations aligning these scans. our technique is based on (1) detecting and matching keypoints distinctive locations in <phrase>range</phrase> and intensity images, (2) generating andrefining a transformation estimate from each keypoint match, and (3) deciding if a given refined estimate is correct. while these steps are familiar, we present novel approaches to each. a new <phrase>range</phrase> keypoint technique is presented that uses <phrase>spin</phrase> images to describe holes in smooth surfaces. intensity keypoints are detected using multiscale filters, described using intensity <phrase>gradient</phrase> histograms, and backprojected to form 3d keypoints. a hypothesized transformation is generated by matching a <phrase>single</phrase> keypoint from one scan to a <phrase>single</phrase> keypoint from another, and is refined using a robust form of the <phrase>icp</phrase> <phrase>algorithm</phrase> in combination with controlled <phrase>region</phrase> growing. deciding whether a refined transformation is correct is based on three criteria: alignment accuracy, visibility, and a novel <phrase>randomness</phrase> measure. together these three steps produce good <phrase>results</phrase> in <phrase>test</phrase> scans of the rensselaer <phrase>campus</phrase>.
in process 3d-sensing for <phrase>laser</phrase> material processing.
from coarse to fine correspondence of 3-d facial images and its application to facial caricaturing.
automatic class selection and prototyping for 3-d object classification.
most <phrase>research</phrase> on 3-d object classification and recognition focuses on recognition of objects in 3-d scenes from a small <phrase>database</phrase> of known 3-d models. such an approach does not scale well to large <phrase>databases</phrase> of objects and does not generalize well to unknown (but similar) object classification. this <phrase>paper</phrase> presents two ideas to address these problems (i) class selection, i.e., grouping similar objects into classes (<phrase>ii</phrase>) class prototyping, i.e., exploiting common structure within classes to represent the classes. at run time matching a query against the <phrase>prototypes</phrase> is sufficient for classification. this approach will not only reduce the retrieval time but also will help increase the generalizing power of theclassification <phrase>algorithm</phrase>. objects are segmented into classes automatically using an agglomerative clustering <phrase>algorithm</phrase>. <phrase>prototypes</phrase> from these classes are extracted using one of three class prototyping <phrase>algorithms</phrase>. <phrase>experimental</phrase> <phrase>results</phrase> demonstrate the effectiveness of the two steps in speeding up the classification process without sacrificing accuracy.
weighted cone-<phrase>curvature</phrase>: applications for 3d shapes similarity.
capturing 2d depth and texture of time-varying scenes using structured <phrase>infrared light</phrase>.
accurate principal directions estimation in discrete surfaces.
accurate local surface <phrase>geometry</phrase> estimation in discrete surfaces is an important problem with numerous applications. principal curvatures and principal directions can be used in applications such as shape analysis and recognition, object segmentation, adaptive smoothing, <phrase>anisotropic</phrase> fairing of irregular meshes, and <phrase>anisotropic</phrase> <phrase>texture mapping</phrase>. in this <phrase>paper</phrase>, a novel approach for accurate principal direction estimation in discrete surfaces is described. the proposed approach is based on local directional curve sampling of the surface where the sampling <phrase>frequency</phrase> can be controlled. this local <phrase>model</phrase> has a large number of degrees of freedoms compared with known techniques and so can better represent the local <phrase>geometry</phrase>. the proposed approach is quantitatively evaluated and compared with known techniques for principal direction estimation. in <phrase>order</phrase> to perform an unbiased evaluation in which smoothing effects are factored out, we use a set of randomly generated bezier surface patches for which the principal directions can be computed analytically.
3d modeling system of <phrase>human</phrase> face and full 3d facial caricaturing.
this <phrase>paper</phrase> proposes a method for modeling 3d face from the 2d facial images captured from the surrounding 2d cameras by which the color texture and surface shape <phrase>information</phrase> of the face are synchronously measured. and 3d facial caricaturing method is proposed by using the 3d(the <phrase>polygon</phrase> <phrase>data</phrase>) face <phrase>model</phrase>. automatic method for extracting regions of the facial parts is technically proposed, and the <phrase>feature points</phrase> are extracted from those regions. we propose the <phrase>mesh model</phrase> composed of 44 <phrase>feature points</phrase> and 82 meshes to <phrase>cover</phrase> a head. to generate the <phrase>caricature</phrase> from this <phrase>polygon</phrase> <phrase>data</phrase>, the individuality feature is defined in value by the difference of the <phrase>feature points</phrase> between the input face and the mean face, which was defined from the <phrase>average</phrase> of many input faces.
colour texture <phrase>fusion</phrase> of <phrase>multiple range</phrase> images.
on the detection of <phrase>feature points</phrase> of 3d facial image and its application to 3d facial <phrase>caricature</phrase>.
3d <phrase>animation</phrase> of cerebral activity using both spatial and temporal <phrase>fmri</phrase> <phrase>information</phrase>.
interactive shape acquisition using marker attached <phrase>laser</phrase> projecto.
<phrase>cad</phrase> and vision in <phrase>rangefinder</phrase>-based dimensional <phrase>metrology</phrase>.
a concept of <phrase>cad</phrase> <phrase>model</phrase>-based automated 3d measurement and the evaluation of its feasibility in cases drawn from <phrase>industrial</phrase> needs. automated 3d measurement can be seen as comprising two steps: measurement planning and measurement execution. the concept was evaluated by implementing a <phrase>graphical</phrase> measurement planning tool and two automatic optical measurement systems equipped with vision systems for sensory <phrase>feedback</phrase>. the operation chain from <phrase>cad</phrase> <phrase>model</phrase>-based measurement planning to comparison between the measured and designed geometries was demonstrated. successful experiments with automatic operation controlled by the measurement plan and vision guidance were carried out and the key performance criteria were met.
uncalibrated multiple image <phrase>stereo</phrase> system with arbitrarily movable <phrase>camera</phrase> and projector for wide <phrase>range</phrase> scanning.
in this <phrase>paper</phrase>, we propose an uncalibrated, multi-image 3d <phrase>reconstruction</phrase>, using coded <phrase>structured light</phrase>. normally, a conventional coded <phrase>structured light</phrase> system consists of a <phrase>camera</phrase> and a projector and needs precalibration before scanning. since the <phrase>camera</phrase> and the projector have to be fixed after calibration, <phrase>reconstruction</phrase> of a wide <phrase>area</phrase> of the scene or reducing occlusions by multiple scanning are difficult and sometimes impossible. in <phrase>the proposed method</phrase>, multiple scanning while moving the <phrase>camera</phrase> or the projector is possible by applying the uncalibrated <phrase>stereo</phrase> method, thereby achieving a multi-image 3d <phrase>reconstruction</phrase>. as compared to the conventional coded <phrase>structured light</phrase> method, our system does not require calibration of extrinsic <phrase>camera</phrase> parameters, occlusions are reduced, and a wide <phrase>area</phrase> of the scene can be acquired. as compared to <phrase>image-based</phrase> multi-image <phrase>reconstruction</phrase>, the proposed system can obtain dense shape <phrase>data</phrase> with higher precision. as a result of these advantages, users can freely move either the cameras or projectors to scan a wide <phrase>range</phrase> of objects, but not if both the <phrase>camera</phrase> and the projector are moved at the same time.
<phrase>three-dimensional</phrase> <phrase>reconstruction</phrase> of the bony structures involved in the articular complex of the <phrase>human</phrase> shoulder using shape-based <phrase>interpolation</phrase> and contour-based <phrase>extrapolation</phrase>.
<phrase>virtual environment</phrase> modeling by integrated optical and <phrase>acoustic</phrase> sensing.
avenue: automated site modeling in <phrase>urban</phrase> environments.
<phrase>oscar</phrase>: object segmentation using correspondence and relaxation.
efficient discovery service for a <phrase>digital library</phrase> of 3d models.
many geographically distributed experts in different areas such as <phrase>medical imaging</phrase>, <phrase>e-commerce</phrase>, and <phrase>digital</phrase> <phrase>museums</phrase>, are in need of 3d models. although 3d models are becoming widely available due to the recent technological advancement and modeling tools, we lack a <phrase>digital library</phrase> system where they can be searched and retrieved efficiently. in this <phrase>paper</phrase> we focus on an efficient discovery service consisting of multi-level hierarchical browsing service that enables users to navigate large sets of 3d models. for this purpose, we use shape based clustering to abstract a large set of 3d models to a small set of representative models (key models). our service applies clustering recursively to limit the number of key models that a user views at a time. clustering is derived from metrics that are based on a concept of compression and similarity computation using surface signatures. signatures are the two-dimensional representations of a 3d <phrase>model</phrase> and they can be used to define similarity between 3d models. we integrated the proposed browsing capability with 3dlib,(a <phrase>digital library</phrase> for 3-d models that we are building at old <phrase>dominion</phrase> <phrase>university</phrase>), and evaluated the proposed browsing service using the <phrase>princeton</phrase> shape benchmark (psb). our evaluation shows significant better precision and recall as compared to other approaches.
<phrase>industrial</phrase> <phrase>painting</phrase> inspection using <phrase>specular</phrase> sharpness.
<phrase>digital 3d</phrase> plus color imaging is a growing field as it has many <phrase>industrial</phrase> applications. one of them is the precise understanding of optical properties of objects, which is possible to obtain from 3d plus brightness or color <phrase>data</phrase>. those properties are usually important inspection criteria; for example, the <phrase>specular</phrase> sharpness of a coatted surface is usually related to the coating quality. this <phrase>paper</phrase> is related to a procedure for recovering the <phrase>specular</phrase> sharpness and other surface-related properties from objects. the current approach works on 3d plus brightness or color <phrase>data</phrase>, which are retrieved in two steps using a common <phrase>structured-light</phrase> <phrase>triangulation</phrase>-based technique and an ordinary non-structured spot <phrase>light</phrase>. some <phrase>experimental</phrase> <phrase>results</phrase> obtained on <phrase>industrial</phrase> parts are also shown.
automatic 3d modeling of <phrase>palatal</phrase> <phrase>plaster</phrase> casts.
geometrically stable sampling for the <phrase>icp</phrase> <phrase>algorithm</phrase>.
on-line hand-eye calibration.
multiresolution interactive modeling with efficient visualization.
3d interactive modeling from <phrase>range</phrase> <phrase>data</phrase> aims at simultaneously producing and visualizing the surface <phrase>model</phrase> of an object while <phrase>data</phrase> is collected. the current <phrase>research</phrase> challenge is producing the final result in real-time. using a recently proposed framework, a surface <phrase>model</phrase> is built in a volumetric structure encoding a <phrase>vector field</phrase> in the <phrase>neighborhood</phrase> of the object surface. in this <phrase>paper</phrase>, it is shown that the framework allows one to locally control the <phrase>model</phrase> resolution during acquisition. using <phrase>ray tracing</phrase>, efficient visualization approaches of the multiresolution <phrase>vector field</phrase> are described and compared. more precisely, it is shown that volume traversal can be optimized while preventing holes and reducing <phrase>aliasing</phrase> in the rendered image.
a robust <phrase>image-based</phrase> method for 3d registration.
today modeling from <phrase>reality</phrase> receives more and more attention. in this <phrase>paper</phrase>, we present a novel <phrase>image-based</phrase> 3d registration method. compared with a previous one it does not require accurate starting position, <phrase>albedo</phrase> and geometric invariants, but has to address the more apparent mismatch problems. first, distinctive corner points, which <phrase>act</phrase> as salient features for subsequent image matching, are detected via the minimal <phrase>eigenvalue</phrase> of the auto-correlation matrix. then, a verification scheme discards the potential mismatches by thresholds of the correlation coefficients and point-to-point distances. <phrase>experimental</phrase> <phrase>results</phrase> demonstrate the superiority of our proposed verification scheme to the previous one using only correlation coefficients under the relaxed conditions.
a 3d <phrase>laser</phrase> micro-<phrase>sensor</phrase> integrating control and <phrase>data processing</phrase> in an <phrase>fpga</phrase>-based <phrase>calculator</phrase>.
hierarchical coarse to fine depth estimation for realistic view <phrase>interpolation</phrase>.
this <phrase>paper</phrase> presents a novel approach for <phrase>view synthesis</phrase> and image <phrase>interpolation</phrase>. the <phrase>algorithm</phrase> is build up in a hierarchical way, and this on different structural levels instead of using a classic image <phrase>pyramid</phrase>. first coarse matching is done on a shape basis only. a background-foreground segmentation yields a fairly accurate contour for every incoming <phrase>video</phrase> <phrase>stream</phrase>. inter-relating these contours is a 1d problem and as such very fast. this step is then used to compute small position dependent bounding-boxes in 3d space which enclose the underlying object. the next step is a more expensive window based matching, within the volume of these bounding-boxes. this is limited to a number of regions around promising <phrase>feature points</phrase>. global regularisation is obtained by a <phrase>graph</phrase> cut. speed <phrase>results</phrase> here from limiting the number of <phrase>feature points</phrase>. in a third step the <phrase>interpolation</phrase> is pre-rendered and simultaneously evaluated on a per <phrase>pixel</phrase> basis. this is done by <phrase>computing</phrase> a birchfield dissimilarity measure on the <phrase>gpu</phrase>. per <phrase>pixel</phrase> parallelised operations keep computational cost low. finally the bad interpolated parts are patched. this per <phrase>pixel</phrase> correction yields the final interpolated view at the finest level. here we will also deal explicitly with opacity at the <phrase>borders</phrase> of the foreground object.
3d modeling of outdoor environments by integrating <phrase>omnidirectional</phrase> <phrase>range</phrase> and <phrase>color images</phrase>.
this <phrase>paper</phrase> describes a 3d modeling method for wide <phrase>area</phrase> outdoor environments which is based on integrating <phrase>omnidirectional</phrase> <phrase>range</phrase> and <phrase>color images</phrase>. in <phrase>the proposed method</phrase>, outdoor scenes can be efficiently digitized by an <phrase>omnidirectional</phrase> <phrase>laser rangefinder</phrase> which can obtain a 3d shape with <phrase>high</phrase>-accuracy and by an <phrase>omnidirectional</phrase> <phrase>multi-camera</phrase> system (oms) which can capture a <phrase>high</phrase>-resolution color image. <phrase>multiple range</phrase> images are registered by minimizing the distances between corresponding points in the different <phrase>range</phrase> images. in <phrase>order</phrase> to register <phrase>multiple range</phrase> images stably, points on plane portions detected from the <phrase>range</phrase> <phrase>data</phrase> are used in registration process. the position and orientation acquired by rtk-<phrase>gps</phrase> and <phrase>gyroscope</phrase> are used as initial values of simultaneous registration. the 3d <phrase>model</phrase> obtained by registration of <phrase>range</phrase> <phrase>data</phrase> is mapped by textures selected from <phrase>omnidirectional</phrase> images in consideration of the resolution of texture and occlusions of the <phrase>model</phrase>. in experiments, we have carried out 3d modeling of our <phrase>campus</phrase> with <phrase>the proposed method</phrase>.
detecting cylinders in 3d <phrase>range</phrase> <phrase>data</phrase> using <phrase>model selection</phrase> criteria.
in this <phrase>paper</phrase>, we use a <phrase>model selection</phrase> criterion to decide whether two cylinders should be merged as a <phrase>single</phrase> cylinder or they should be left separated. we compare and evaluate an extensive number of different <phrase>model selection</phrase> criteria for this purpose and examine which factors can affect their performance. we conclude that ssc, gic, mcaic and caic have a better performance (for this particular application) compared to the other criteria.
constructing models of articulating objects: <phrase>range</phrase> <phrase>data</phrase> partitioning.
in this <phrase>paper</phrase> we consider one <phrase>aspect</phrase> of the problem of automatically building shape models of articulating objects from example <phrase>range</phrase> images. central to the <phrase>model</phrase> <phrase>construction</phrase> problem is the registration of <phrase>range</phrase> <phrase>data</phrase>, taken from different vantage points, into a common coordinate frame. this involves determining a transformation for each set of <phrase>range</phrase> <phrase>data</phrase> which aligns overlapping surface points in the common frame. current registration <phrase>algorithms</phrase> have been developed specifically for rigid objects, but it is not obvious how these can be extended to articulated or more generally deformable objects. here, we propose that <phrase>range</phrase> images of articulated objects are first segmented into their rigid subcomponents. each subcomponent can then be registered in isolation using the existing <phrase>algorithms</phrase> designed specifically for rigid parts and the final <phrase>model</phrase> formed by reassembling all of the submodels. this has motivated the development of a rigid part segmentation <phrase>algorithm</phrase> which is described and demonstrated here. the <phrase>algorithm</phrase> is currently limited to non-umbilic surfaces, but in this more restricted domain is shown to work well.
<phrase>human</phrase> identification from <phrase>body shape</phrase>.
extracting main modes of <phrase>human body</phrase> shape variation from 3-d <phrase>anthropometric</phrase> <phrase>data</phrase>.
characterizing the variations of the <phrase>human body</phrase> shape is fundamentally important to many applications ranging from <phrase>animation</phrase> to <phrase>product design</phrase>. 3-d scanning <phrase>technology</phrase> makes it possible to digitize the complete surfaces of a large number of <phrase>human</phrase> bodies, providing much richer <phrase>information</phrase> about the <phrase>body shape</phrase> than the traditional <phrase>anthropometric</phrase> measurements. this <phrase>technology</phrase> opens up opportunities to extract new measurements for quantifying the <phrase>body shape</phrase>. using the <phrase>data</phrase> from the first <phrase>large scale</phrase> 3-d <phrase>anthropometric</phrase> survey, the <phrase>caesar</phrase> project, we demonstrate that the <phrase>human body</phrase> shape can be represented by a small number of principal components. <phrase>principal component analysis</phrase> extracts orthogonal basis vectors, called eigenpersons, from the space of body shapes. the shape of any individual person can then be expressed by the <phrase>linear combination</phrase> of the basis vectors. we demonstrate that some of these components correspond to the commonly used body measurements like height and weight and others indicate new ways of charactering <phrase>body shape</phrase> variations. we develop tools to visualize the changes of the <phrase>body shape</phrase> along the main components. these tools help understand the meaningful components of the <phrase>human body</phrase> shape.
a method for the registration of attributed <phrase>range</phrase> images.
<phrase>data acquisition</phrase> and representation of mechanical parts and interfaces to <phrase>manufacturing</phrase> devices.
in this <phrase>paper</phrase> we address <phrase>data acquisition</phrase>, <phrase>data</phrase> represen- tation and interfaces to <phrase>manufacturing</phrase> devices as they relate to automatic creation of <phrase>electronic</phrase> files for representing the complete <phrase>geometry</phrase> of an arbitrarily shaped part.
accuracy of 3d <phrase>range</phrase> scanners by measurement of the slanted edge <phrase>modulation</phrase> <phrase>transfer function</phrase>.
<phrase>design</phrase> considerations for a <phrase>range</phrase> <phrase>image sensor</phrase> containing a psd-array and an on-chip <phrase>multiplexer</phrase>.
in the present <phrase>paper</phrase> we introduce a <phrase>range</phrase> <phrase>image sensor</phrase>, the psd-chip, designed for sheet of <phrase>light</phrase> <phrase>range</phrase> imaging. the <phrase>image sensor</phrase> consists of an array of 128 position sensitive detector (psd)-<phrase>strips</phrase> with <phrase>a 20</phrase> mm length and a 28 /<phrase>spl</phrase> mu/m pitch. <phrase>design</phrase> considerations for the <phrase>image sensor</phrase> are discussed, as well as the on-chip <phrase>electronics</phrase>. the on-chip <phrase>electronics</phrase> consists of an analog part and a <phrase>digital</phrase> part. the analog preamplifiers deal with the <phrase>low-pass</phrase> filtering of the <phrase>sensor</phrase>-signals in <phrase>order</phrase> to reduce the noise bandwidth. the <phrase>digital</phrase> part consists of an analog current <phrase>multiplexer</phrase>, implemented in <phrase>ecl</phrase> <phrase>technology</phrase>. our goal is to achieve a 2 <phrase>mhz</phrase> <phrase>range</phrase>/<phrase>frequency</phrase> at 12 <phrase>bits</phrase> resolution.
image-<phrase>gradient</phrase>-guided real-time <phrase>stereo</phrase> on <phrase>graphics hardware</phrase>.
we present a real-time correlation-based <phrase>stereo</phrase> <phrase>algorithm</phrase> with improved accuracy. encouraged by the success of recent <phrase>stereo</phrase> <phrase>algorithms</phrase> that aggregate the matching cost based on color segmentation, a novel image-<phrase>gradient</phrase>-guided cost aggregation scheme is presented in this <phrase>paper</phrase>. the new scheme is designed to fit the <phrase>architecture</phrase> of recent graphics processing units (<phrase>gpus</phrase>). as a result, our <phrase>stereo</phrase> <phrase>algorithm</phrase> can run completely on the graphics board: from rectification, matching cost computation, cost aggregation, to the final disparity selection. compared with many real-time <phrase>stereo</phrase> <phrase>algorithms</phrase> that use fixed <phrase>windows</phrase>, noticeable accuracy improvement has been obtained without sacrificing realtime performance. in addition, existing <phrase>global optimization</phrase> <phrase>algorithms</phrase> can also benefit from the new cost aggregation scheme. the effectiveness of our approach is demonstrated with several widely used <phrase>stereo</phrase> datasets and <phrase>live</phrase> <phrase>data</phrase> captured from a <phrase>stereo camera</phrase>.
structure and motion from two uncalibrated views using points on planes.
automatic <phrase>cad</phrase> modeling of <phrase>industrial</phrase> pipes from <phrase>range</phrase> images.
we present in this <phrase>paper</phrase> a method to obtain automatically <phrase>cad</phrase> models of <phrase>industrial</phrase> pipes from <phrase>range</phrase> images. the models are based on two geometric primitives, cylinders and <phrase>torii</phrase>, which are enough to represent most parts of the pipes. the images are obtained with an accurate <phrase>long</phrase>-distance <phrase>laser</phrase> <phrase>range</phrase> <phrase>sensor</phrase> developed for <phrase>reverse engineering</phrase> in <phrase>industrial</phrase> structures. the key issue for automatic <phrase>cad</phrase> modeling is the automatic segmentation of the <phrase>data</phrase> into subsets of points corresponding to the desired primitives. to do so, we use <phrase>differential geometry</phrase> to segment lines of centers of <phrase>curvature</phrase> into straight and curved parts, corresponding to the cylinder and <phrase>torus</phrase> parts of the original image. <phrase>differential geometry</phrase> <phrase>results</phrase> being noisy and biased, we use an optimal approach for the computation of centers of <phrase>curvature</phrase>.
fast global registration of 3d sampled surfaces using a multi-z-buffer technique.
we present a new method for the global registration of several overlapping 3d surfaces sampled on an object. the <phrase>method is based</phrase> on the <phrase>icp</phrase> (<phrase>iterative closest</phrase> point) <phrase>algorithm</phrase> and on a segmentation of the sampled points in an optimized set of z-buffers. this multi-z-buffer technique provides a 3d space partitioning which greatly accelerates the search of the nearest <phrase>neighbours</phrase> in the establishment of the point-to-point correspondence between overlapping surfaces. then a <phrase>randomized</phrase> iterative registration is processed on the surface set. we have tested an implementation of this technique on real sampled surfaces. it appears to be rapid accurate and robust, especially in the case of highly curved objects.
3-d vision <phrase>technology</phrase> for occupant detection and classification.
this <phrase>paper</phrase> describes a 3-d vision system based on a new 3-d <phrase>sensor</phrase> <phrase>technology</phrase> for the detection and classification of occupants in a <phrase>car</phrase>. new generation of so-called "smart <phrase>airbags</phrase>" require the <phrase>information</phrase> about the occupancy type and position of the occupant. this <phrase>information</phrase> allows a distinct control of the <phrase>airbag</phrase> <phrase>inflation</phrase>. in <phrase>order</phrase> to reduce the <phrase>risk</phrase> of injuries due to <phrase>airbag</phrase> deployment, the <phrase>airbag</phrase> can be suppressed completely in case of a child seat oriented in reward direction. in this <phrase>paper</phrase> we propose a 3-d vision system based on a 3-d optical time-of-flight (tof) <phrase>sensor</phrase>, for the detection and classification of the occupancy on the passenger seat. geometrical shape features are extracted from the 3-d <phrase>image sequences</phrase>. polynomialclassifier is considered for the classification task. a comparison of classifier performance with principle components (eigenimages) is presented. this <phrase>paper</phrase> also discuss the robustness of the features with variation of the <phrase>data</phrase>. the full scale <phrase>tests</phrase> have been conducted on a wide <phrase>range</phrase> of realistic situations (adults/children/child seats etc.) which may occur in a vehicle.
<phrase>digital 3d</phrase> imaging system for rapid response on remote sites.
is appearance-based <phrase>structure from motion</phrase> viable?
using k-d <phrase>trees</phrase> for robust 3d point <phrase>pattern matching</phrase>.
optimized position sensors for flying-spot active <phrase>triangulation</phrase> systems.
registration and <phrase>fusion</phrase> of intensity and <phrase>range</phrase> <phrase>data</phrase> for 3d modelling of <phrase>real world</phrase> scenes.
automatic <phrase>editing</phrase> and <phrase>curve-fitting</phrase> of 3-d surface scan <phrase>data</phrase> of the <phrase>human</phrase> body.
this <phrase>paper</phrase> presents an automatic method to trim <phrase>arms</phrase> from the surface scan <phrase>data</phrase> of the <phrase>human</phrase> body. curve and surface approximation are employed to refill the <phrase>data</phrase> gap after the trimming process. this delivers a more realistic torso <phrase>model</phrase> for further applications.
object <phrase>model</phrase> creation from <phrase>multiple range</phrase> images: acquisition, calibration, <phrase>model</phrase> building and verification.
this <phrase>paper</phrase> demonstrates the accuracy of a <phrase>prototype</phrase> <phrase>laser</phrase> <phrase>range</phrase> <phrase>camera</phrase> (lrc) developed at the <phrase>national research council of canada</phrase> for the creation of models of real objects. a <phrase>laser</phrase> survey performed in collaboration with the <phrase>canadian space agency</phrase> and <phrase>nasa</phrase> is used as a <phrase>test case</phrase>. the object selected for this particular <phrase>test case</phrase> is the orbiter docking system (ods) located at the <phrase>kennedy space center</phrase>, <phrase>florida</phrase>. during the <phrase>laser</phrase> survey, 128 <phrase>range</phrase> (and registered intensity) images were acquired all around the ods. these images were then processed in our <phrase>laboratory</phrase>. a full <phrase>model</phrase> of the top portion of the ods was created along with an almost complete <phrase>model</phrase> of the ods. the ods has a <phrase>diameter</phrase> of 1.6 m and a height of 3.9 m. targets mounted on the top portion of the ods were used to assess the accuracy of the calibration and of the <phrase>image registration</phrase> process. these targets were measured with a network of theodolites a day prior to the <phrase>laser</phrase> survey and used as a reference. with the current calibration and <phrase>range</phrase> <phrase>image registration</phrase> techniques, an accuracy better than 0.25 mm in x and y, and, 0.80 mm in z was achieved. these <phrase>results</phrase> compare favorably with the <phrase>single</phrase> point accuracy obtained after calibration, i.e., about 0.25 mm in x and y, and, 0.50 mm in z. these figures and others should testify on the usefulness of a lrc for accurate <phrase>model</phrase> building.
a scene analysis system for the generation of 3-d models.
a scene analysis system for the 3-d modeling of objects is presented. it combines <phrase>surface reconstruction</phrase> techniques with object recognition for the generation of 3-d models for computer <phrase>graphic</phrase> applications. the system permits the insertion of highlevel constraints, like a specific angle between two <phrase>house</phrase> walls, in an <phrase>explicit knowledge</phrase> base implemented as a <phrase>semantic</phrase> net. the applicability of those constraints is proved by asserting and testing hypotheses in an interpretation phase. in the case of rejection a more <phrase>general</phrase> constraint or <phrase>model</phrase> is selected. the capabilities of the system were shown for the modeling of buildings using depth from <phrase>stereo</phrase> and contour <phrase>information</phrase>. the system reconstructs the surface of the scene objects using the constraints selected in the prior interpretation.
real-time geometrical tracking and <phrase>pose estimation</phrase> using <phrase>laser</phrase> <phrase>triangulation</phrase> and <phrase>photogrammetry</phrase>.
efficient and reliable template set matching for 3d object recognition.
recursive <phrase>model</phrase> optimization using <phrase>icp</phrase> and <phrase>free</phrase> moving 3d <phrase>data acquisition</phrase>.
a nearest neighbor method for efficient <phrase>icp</phrase>.
a geometric approach to the segmentation of <phrase>range</phrase> images.
approximate k-d <phrase>tree</phrase> search for efficient <phrase>icp</phrase>.
accuracy of 3d scanning technologies in a face scanning scenario.
in this <phrase>paper</phrase>, we will review several different 3d scanning devices. we will present a method for empirical accuracy analysis, and apply it to several scanners providing an overview of their technologies. the scanners include both <phrase>general</phrase> purpose and face specific scanning devices. we will focus on face scanning technique, although the technique should be applicable to other domains as well. <phrase>the proposed method</phrase> involves several different calibration faces of known shape and comparisons of their scans to investigate both absolute accuracy and <phrase>repeatability</phrase>.
hand posture estimation from 2d <phrase>monocular</phrase> image.
hierarchical segmentation of <phrase>range</phrase> images with contour constraints.
this <phrase>paper</phrase> describes a new <phrase>algorithm</phrase> to segment in continuous parametric regions <phrase>range</phrase> images. the <phrase>algorithm</phrase> starts with an initial <phrase>partition</phrase> of small first <phrase>order</phrase> regions using a robust fitting <phrase>algorithm</phrase> constrained by the detection of depth and orientation discontinuities. the <phrase>algorithm</phrase> then optimally group these regions into larger and larger regions using parametric functions until an approximation limit is reached. the <phrase>algorithm</phrase> uses <phrase>bayesian</phrase> <phrase>decision theory</phrase> to determine the local optimal grouping and the complexity of the parametric <phrase>model</phrase> used to represent the <phrase>range</phrase> signal. after the segmentation process an exact description of the boundary of each <phrase>region</phrase> is computed from the mutual intersections of the extracted surfaces. <phrase>experimental</phrase> <phrase>results</phrase> show significant improvement of <phrase>region</phrase> boundary localization. a systematic comparison of our <phrase>algorithm</phrase> to the most well known <phrase>algorithm</phrase> in the <phrase>literature</phrase> is presented to highlight the contributions of this <phrase>paper</phrase>.
reliable 3d surface acquisition, registration and validation using statistical error models.
non-parametric 3d surface completion.
we consider the completion of the hidden or missing portions of 3d objects after the visible portions have been acquired with 2d (or 3d) <phrase>range</phrase> capture. our approach uses a combination of global surface fitting, to derive the underlying geometric surface completion, together with an extension, from 2d to 3d, of non-parametric texture synthesis in <phrase>order</phrase> to complete localised surface texture <phrase>relief</phrase> and structure. through this combination and adaptation of existing completion techniques we are able to achieve realistic, plausible completion of 2d <phrase>range</phrase> captures.
robust surface matching for registration.
micro-<phrase>stereoscopic</phrase> vision system for the determination of air bubbles and aqueous droplets content within <phrase>oil</phrase> drops in simulated processes of multiphase fermentations.
<phrase>industrial</phrase> <phrase>fermentation</phrase> procedures involve the <phrase>mixing</phrase> of multiple phases (solid, <phrase>liquid</phrase>, gaseous), where the interfacial <phrase>area</phrase> between the phases (air bubbles, <phrase>oil</phrase> drops and aqueous medium) determines the <phrase>nutrients</phrase> transfer and hence the performance of the <phrase>culture</phrase>. interactions between phases occur, giving rise to the formation of complex structures containing air bubbles and small drops from the aqueous phase, trapped in <phrase>oil</phrase> drops (<phrase>water</phrase>-in-<phrase>oil</phrase>-in-<phrase>water</phrase>). a two-dimensional observation of this phenomenon may <phrase>lead</phrase> to an erroneous determination of the phenomena occurring since bubbles and droplets coming from different focal planes may appear overlapped. in the present work, an original strategy to solve this problem is described. micro-<phrase>stereoscopic</phrase> on-line image acquisition techniques have been used, so as to obtain accurate images from the cultures for further <phrase>three-dimensional</phrase> analysis. using this methodology, the <phrase>three-dimensional</phrase> spatial position of the trapped bubbles and droplets moving at <phrase>high</phrase> speed can be calculated in <phrase>order</phrase> to determine their relative concentration. to evaluate the accuracy of this technique, the <phrase>results</phrase> obtained with our system have been compared with those obtained by an expert. an agreement of 95% was achieved. also, this technique was able to evaluate 14% more bubbles and droplets corresponding to overlaps that the expert was not able to discern in non-<phrase>stereoscopic</phrase> images.
3d optical scanning diagnostics for <phrase>leonardo</phrase> da vinci's "adorazione dei <phrase>magi</phrase>" conservation.
unsupervised 3d object recognition and <phrase>reconstruction</phrase> in unordered datasets.
this <phrase>paper</phrase> presents a system for fully automatic recognition and <phrase>reconstruction</phrase> of 3d objects in image <phrase>databases</phrase>. we pose the object recognition problem as one of finding consistent matches between all images, subject to the constraint that the images were taken from a perspective <phrase>camera</phrase>. we assume that the objects or scenes are rigid. for each image we associate a <phrase>camera</phrase> matrix, which is parameterised by rotation, <phrase>translation</phrase> and <phrase>focal length</phrase>. we use invariant local features to find matches between all images, and the ransac <phrase>algorithm</phrase> to find those that are consistent with the fundamental matrix. objects are recognised as subsets of matching images. we then solve for the structure and motion of each object, using a sparse bundle adjustment <phrase>algorithm</phrase>. our <phrase>results</phrase> demonstrate that it is possible to recognise and reconstruct 3d objects from an unordered image <phrase>database</phrase> with no user input at all.
accuracy verification and enhancement in 3d modeling: application to donatello's maddalena.
optimal postures and positioning for <phrase>human</phrase> body scanning.
advancements in <phrase>technology</phrase> for <phrase>digitizing</phrase> the surface of the <phrase>human</phrase> body are providing new opportunities for <phrase>research</phrase> in <phrase>engineering</phrase> <phrase>anthropometry</phrase>, the study of <phrase>human</phrase> body measurement for <phrase>design</phrase> and evaluation purposes. the availability of the <phrase>technology</phrase> is just the first step in applying surface scanning to <phrase>engineering</phrase> <phrase>anthropometry</phrase>; several issues remain to be resolved to make these tools useful for <phrase>engineering</phrase> applications. one important issue is the standardization of positioning and the posture of the subject for scanning. in <phrase>engineering</phrase> it is not enough to be able to measure one individual one time in one posture, but it is also necessary to measure the individual in different postures and compare the individual with many other people who have been comparably measured. not surprisingly, people can be more difficult to measure precisely than fixed stationary objects. in the process of developing standardized procedures for <phrase>surveying</phrase> the <phrase>civilian</phrase> populations of <phrase>north america</phrase> and <phrase>europe</phrase>, an experiment was conducted to determine optimal scanning positions. while this experiment used just one type of scanning <phrase>technology</phrase>, many of the methods are transferable to other methods as well. this <phrase>paper</phrase> discusses the <phrase>results</phrase> from that investigation.
3d digitization of a large <phrase>model</phrase> of <phrase>imperial rome</phrase>.
this <phrase>paper</phrase> describes 3d acquisition and modeling of the "plastico di <phrase>roma</phrase> antica", a large <phrase>plaster</phrase>-of-<phrase>paris</phrase> <phrase>model</phrase> of <phrase>imperial rome</phrase> (16x17 meters) created in the last century. its overall size demands an acquisition approach typical of large structures, but it is also characterized by extremely tiny details, typical of small objects: houses are a few centimeters <phrase>high</phrase>; their doors, <phrase>windows</phrase>, etc. are smaller than 1 cm. the approach followed to resolve this "contradiction" is described. the result is a huge but precise 3d <phrase>model</phrase> created by using a special <phrase>metrology</phrase> <phrase>laser</phrase> <phrase>radar</phrase>. we give an account of the procedures of reorienting the large point clouds obtained after each acquisition step (50-60 million points) into a <phrase>single</phrase> reference system by means of measuring fixed redundant reference points. finally we show how the <phrase>data set</phrase> can be properly divided into 2x2 meters sub-areas for allowing <phrase>data</phrase> merging and mesh <phrase>editing</phrase>..
3d registration by textured <phrase>spin</phrase>-images.
this work is motivated by the desire of exploiting for 3d registration purposes the <phrase>photometric</phrase> <phrase>information</phrase> current <phrase>range</phrase> cameras typically associate to <phrase>range</phrase> <phrase>data</phrase>. automatic pairwise 3d registration procedures are two steps procedures with the first step performing an automatic crude estimate of the rigid motion parameters and the second step refining them by the <phrase>icp</phrase> <phrase>algorithm</phrase> or some of its variations. methods for efficiently implementing the first crude automatic estimate are still an open <phrase>research</phrase> <phrase>area</phrase>. <phrase>spin</phrase>-images are a 3d matching tecnique very effective in this task. since <phrase>spin</phrase>-images solely exploit <phrase>geometry</phrase> <phrase>information</phrase> it appears natural to extend their original definition to include texture <phrase>information</phrase>. such an operation can clearly be made in many ways. this work introduces one particular extension of <phrase>spin</phrase>-images, called textured <phrase>spin</phrase>-images, and demostrates its performance for 3d registration. it will be seen that textured <phrase>spin</phrase>-images enjoy remarkable properties since they can give rigid motion estimates more robust, more precise, more resilient to noise than standard <phrase>spin</phrase>-images at a <phrase>lower</phrase> computational cost.
calibration of a zooming <phrase>camera</phrase> using the normalized image of the absolute <phrase>conic</phrase>.
3-d landmark detection and identification in the <phrase>caesar</phrase> project.
<phrase>euclidean</phrase> <phrase>reconstruction</phrase> from translational motion using multiple cameras.
we investigate the possibility of <phrase>euclidean</phrase> <phrase>reconstruction</phrase> from translational motion, using multiple uncalibrated cameras. we show that in the case of multiple cameras viewing a translating scene, no additional constraints are given by the translational motion compared to the more <phrase>general</phrase> case with one <phrase>camera</phrase> viewing a scene undergoing a <phrase>general</phrase> motion. however, the <phrase>knowledge</phrase> of translational motion allows an intermediate affine <phrase>reconstruction</phrase> from each <phrase>camera</phrase>, and <phrase>aids</phrase> in the <phrase>reconstruction</phrase> process by simplifying several steps, resulting in a more reliable <phrase>algorithm</phrase> for 3d <phrase>reconstruction</phrase>. we also identify the critical directions of <phrase>translation</phrase>, for which no affine <phrase>reconstruction</phrase> is possible. experiments on real and simulated <phrase>data</phrase> are performed to illustrate that the method works in practice.
automated pavement distress collection and analysis: a 3-d approach.
compact and portable 3d <phrase>camera</phrase> for space applications.
comparison of hk and sc <phrase>curvature</phrase> description methods.
generalized cylinders extraction in a <phrase>range</phrase> image.
we deal with 3d object modeling using generalized cylinders from a <phrase>single</phrase> <phrase>range</phrase> image. we focus on right generalized cylinders, a class of generalized cylinders for which the cross-section is at right angle with the <phrase>axis</phrase>. no homogeneity or straightness constraints are imposed. the crucial part of this work is the extraction of <phrase>axis</phrase> points and the representation of the <phrase>axis</phrase> curve in 3d space. interesting <phrase>results</phrase> are obtained with a broad <phrase>variety</phrase> of <phrase>range</phrase> images.
realistic <phrase>human</phrase> head modeling with <phrase>multi-view</phrase> hairstyle <phrase>reconstruction</phrase>.
we present a method for constructing <phrase>photorealistic</phrase> 3d head models from <phrase>color images</phrase> and a geometric head <phrase>model</phrase> of a specific person. with a simple <phrase>experimental</phrase> setup, we employ a user-assisted technique to register the uncalibrated images with the geometric <phrase>model</phrase>. a weighted averaging method is then used to extract a panoramic texture map from the input images. to recover the hairstyle of the specified person, a virtual photo plane is defined, according to a corresponding true photo plane, on which one input image is recorded. it provides hints to compute the 3d positions of the visual contour points of the <phrase>hair</phrase> from the images taken at different viewpoints. finally, more hairs are grown to <phrase>cover</phrase> the whole <phrase>region</phrase> of the <phrase>scalp</phrase> using an <phrase>interpolation</phrase> method on the 3d <phrase>scalp</phrase> mesh surfaces.
large <phrase>data</phrase> sets and confusing scenes in 3-d surface matching and recognition.
how much 3d-<phrase>information</phrase> can we acquire? optical <phrase>range</phrase> sensors at the physical limit, and where to apply them.
automatic body measurement for <phrase>mass customization</phrase> of garments.
automatic registration of <phrase>range</phrase> images based on correspondence of complete plane patches.
one of the difficulties in registering two <phrase>range</phrase> images scanned by 3d <phrase>laser</phrase> scanners is how to get a correct correspondence over the two images automatically. in this <phrase>paper</phrase>, we propose an automatic registration method based on matching of extracted planes. first, we introduce a new class of features: complete plane patches (cpp) on the basis of analysis of properties of real scenes. then we generate a compact interpretation <phrase>tree</phrase> for these features. finally, the <phrase>image registration</phrase> is accomplished automatically by searching the interpretation <phrase>tree</phrase>.
<phrase>semi-automatic</phrase> <phrase>range</phrase> to <phrase>range</phrase> registration: a <phrase>feature-based</phrase> method.
a self-referenced hand-held <phrase>range</phrase> <phrase>sensor</phrase>.
segmentation of <phrase>range</phrase> images into planar regions.
this <phrase>paper</phrase> presents a <phrase>hybrid</phrase> approach to the segmentation of <phrase>range</phrase> images into planar regions. the term <phrase>hybrid</phrase> refers to a combination of edge- and <phrase>region</phrase>-based considerations. a reliable computational procedure which takes the <phrase>range</phrase> image discontinuities into account is presented for <phrase>computing</phrase> the pixel's normal. the segmentation <phrase>algorithm</phrase> consists of two parts. in the first one, the <phrase>pixels</phrase> are aggregated according to local properties derived from the input <phrase>data</phrase> and are represented by a <phrase>region</phrase> adjacency <phrase>graph</phrase> (rag). at this stage, the image is still over-segmented. in the second part, the segmentation is refined thanks to the <phrase>construction</phrase> of an irregular <phrase>pyramid</phrase>. the base of the <phrase>pyramid</phrase> is the rag previously extracted. the over-segmented regions are merged using a surface-based description. this <phrase>algorithm</phrase> has been evaluated on 80 real images acquired by two different <phrase>range</phrase> sensors using the methodology proposed in (<phrase>hoover</phrase> et al., 1996). <phrase>experimental</phrase> <phrase>results</phrase> are presented and compared to others obtained by four <phrase>research</phrase> groups.
locking onto 3d-structure by a combined vergence and <phrase>fusion</phrase> system.
a 3d scanning system based on low-occlusion approach.
<phrase>multi-resolution</phrase> geometric <phrase>fusion</phrase>.
geometric <phrase>fusion</phrase> of multiple sets of overlapping surface measurements is an important problem for complete 3d object or environment modelling. <phrase>fusion</phrase> based on a discrete implicit surface representation enables fast <phrase>reconstruction</phrase> for complex object modelling. however, surfaces are represented at a <phrase>single</phrase> resolution resulting in impractical storage costs for accurate <phrase>reconstruction</phrase> of large objects. this <phrase>paper</phrase> addresses accurate <phrase>reconstruction</phrase> of surface models <phrase>independent</phrase> of object size. an incremental <phrase>algorithm</phrase> is presented for implicit surface representation of an arbitrary triangulated mesh in a volumetric envelope around the surface. a hierarchical volumetric structure is introduced for efficient representation by local approximation of the surface within a fixed error bound using the maximum <phrase>voxel</phrase> size. <phrase>multi-resolution</phrase> geometric <phrase>fusion</phrase> is achieved by incrementally constructing a hierarchical surface representation with bounded error. <phrase>results</phrase> are presented for validation of the <phrase>multi-resolution</phrase> representation accuracy and <phrase>reconstruction</phrase> of real objects. <phrase>multi-resolution</phrase> geometric <phrase>fusion</phrase> achieves a significant reduction in representation cost for the same level of geometric accuracy.
a <phrase>light</phrase> <phrase>modulation</phrase>/<phrase>demodulation</phrase> method for real-time 3d imaging.
this <phrase>paper</phrase> describes a novel method for <phrase>digitizing</phrase> the 3d shape of an object in real-time, which can be used for capturing <phrase>live</phrase> <phrase>sequence</phrase> of the 3d shape of moving or deformable objects such as faces. two dmd (= <phrase>digital</phrase> micro <phrase>mirror</phrase>) devices are used as <phrase>high</phrase> speed switches for modulating and demodulating <phrase>light</phrase> <phrase>rays</phrase>. one dmd is used to generate <phrase>rays</phrase> of <phrase>light</phrase> pulses, which are projected onto the object to be measured. another dmd is used to demodulate the <phrase>light</phrase> reflected from the object <phrase>illuminated</phrase> by the <phrase>light</phrase> pulses into intensity image that describes the disparity. a <phrase>prototype</phrase> <phrase>range</phrase> finder implementing <phrase>the proposed method</phrase> has been built. the <phrase>experimental</phrase> <phrase>results</phrase> showed that <phrase>the proposed method</phrase> works and <phrase>video</phrase> sequences of disparity images can be captured in real time.
toward optimal <phrase>structured light</phrase> patterns.
a methodology for the <phrase>optimal design</phrase> of projection patterns for stereometric <phrase>structured light</phrase> systems is presented. the similarity as well as the difference between the <phrase>design</phrase> of projection patterns and the <phrase>design</phrase> of optimal signals for <phrase>digital</phrase> <phrase>communication</phrase> are discussed. the <phrase>design</phrase> of k projection patterns for a <phrase>structured light</phrase> system with l distinct planes of <phrase>light</phrase> is shown to be equivalent to the placement of l points in a k dimensional space subject to certain constraints. <phrase>optimal design</phrase> in the mse sense is defined, but shown to <phrase>lead</phrase> to an intractable multi-parameter <phrase>global optimization</phrase> problem. intuitively appealing suboptimal solutions derived from the <phrase>family</phrase> of k dimensional space-filling <phrase>hilbert</phrase> curves are obtained. preliminary <phrase>experimental</phrase> <phrase>results</phrase> are presented.
edge-<phrase>based approach</phrase> to mesh simplification.
a complete u-v-disparity study for stereovision based 3d driving environment analysis.
reliable understanding of the 3d driving environment is vital for obstacle detection and adaptive <phrase>cruise control</phrase> (<phrase>acc</phrase>) applications. <phrase>laser</phrase> or millimeter <phrase>wave</phrase> <phrase>radars</phrase> have shown good performance in measuring relative speed and distance in a <phrase>highway</phrase> driving environment. however the accuracy of these systems decreases in an <phrase>urban</phrase> traffic environment as more confusion occurs due to factors such as parked vehicles, guardrails, <phrase>poles</phrase> and <phrase>motorcycles</phrase>. a stereovision based sensing system provides an effective supplement to <phrase>radar</phrase>-based <phrase>road</phrase> scene analysis with its much wider field of view and more accurate lateral <phrase>information</phrase>. this <phrase>paper</phrase> presents an efficient <phrase>solution</phrase> using a stereovision based <phrase>road</phrase> scene analysis <phrase>algorithm</phrase> which employs the "u-v-disparity" concept. this concept is used to classify a 3d <phrase>road</phrase> scene into relative surface planes and characterize the features of <phrase>road</phrase> pavement surfaces, roadside structures and obstacles. real-time implementation of the disparity map calculation and the "u-v-disparity" classification is also presented.
calibration-<phrase>free</phrase> approach to 3d <phrase>reconstruction</phrase> using <phrase>light</phrase> stripe projections on a <phrase>cube</phrase> frame.
<phrase>moving objects</phrase> detection from time-varied background: an application of <phrase>camera</phrase> 3d motion analysis.
<phrase>palm</phrase>: portable <phrase>sensor</phrase>-augmented vision system for large-scene modeling.
automatic <phrase>burr</phrase> detection on surfaces of <phrase>revolution</phrase> based on adaptive 3d scanning.
this <phrase>paper</phrase> describes how to automatically extract the presence and location of geometrical irregularities on a surface of <phrase>revolution</phrase>. to this end a partial 3d scan of the workpiece under consideration is acquired by <phrase>structured light</phrase> ranging. the application we focus on is the detection and removal of burrs on <phrase>industrial</phrase> workpieces. cylindrical metallic objects will cause a strong <phrase>specular reflection</phrase> in every direction. these highlights are compensated for in the projected patterns, hence adaptive 3d scanning. the triangular mesh <phrase>produced</phrase> is then used to identify the <phrase>axis</phrase> and generatrix of the corresponding surface of <phrase>revolution</phrase>. the search space for finding this <phrase>axis</phrase> is four dimensional: a valid choice of parameters is two orientation angles (as in <phrase>spherical coordinates</phrase>) and the 2d intersection point with the plane spanned by two out of three <phrase>axis</phrase> of the local coordinate system. for finding the <phrase>axis</phrase> we <phrase>test</phrase> the circularity of the planar intersections of the mesh in different directions, using statistical estimation methods to deal with noise. finally the ideal generatrix derived from the scan <phrase>data</phrase> is compared to the real surface <phrase>topology</phrase>. the difference will identify the <phrase>burr</phrase>. the <phrase>algorithm</phrase> is demonstrated on a <phrase>metal</phrase> wheel that has burrs on both sides. visual servoing of a <phrase>robotic arm</phrase> based on this detection is work in progress.
automatic 3d modeling using <phrase>range</phrase> images obtained from unknown viewpoints.
partial surface integration based on variational implicit functions and surfaces for 3d <phrase>model</phrase> building.
most <phrase>three-dimensional</phrase> acquisition systems generate several partial reconstructions that have to be registered and integrated for building a complete 3d <phrase>model</phrase>. in this <phrase>paper</phrase>, we propose a volumetric shape integration method, consisting of weighted signed distance functions represented as variational implicit functions (vif) or surfaces (vis). texture integration is solved similarly by using three weighted color functions also based on vifs. using these continuous (not grid-based) representations solves current limitations of volumetric methods: no <phrase>memory</phrase> inefficient and resolution limiting grid representation is required. the built-in smoothing properties of the vis representations also improve the robustness of the final integration against noise in the input <phrase>data</phrase>. <phrase>experimental</phrase> <phrase>results</phrase> are performed on real-<phrase>live</phrase>, noiseless and noisy synthetic <phrase>data</phrase> of <phrase>human</phrase> faces in <phrase>order</phrase> to show the robustness and accuracy of the integration <phrase>algorithm</phrase>.
geometric matching of 3-d objects: assessing the <phrase>range</phrase> of successful initial configurations.
robust and accurate partial surface registration based on variational implicit surfaces for automatic 3d <phrase>model</phrase> building.
<phrase>three-dimensional</phrase> models are often assembled from several partial reconstructions from unknown viewpoints. in <phrase>order</phrase> to provide a fully automatic, robust and accurate method for aligning and integrating partial reconstructions without any prior <phrase>knowledge</phrase> of the relative viewpoints of the <phrase>sensor</phrase> or the <phrase>geometry</phrase> of the imaging process, we propose <phrase>a 4</phrase>-step registration and integration <phrase>algorithm</phrase> based on a common variational implicit surface (vis) representation of the partial surface reconstructions. first, a global crude registration without a priori <phrase>knowledge</phrase> is performed followed by a pose refinement of partial <phrase>reconstruction</phrase> pairs. pair-wise registrations are converted into a <phrase>multi-view</phrase> registration, before a final integration of the reconstructions into one entity or <phrase>model</phrase> occurs. furthermore, making use of the smoothing properties of the vis representations, the <phrase>algorithm</phrase> proves to be robust against noise in the <phrase>reconstruction</phrase> <phrase>data</phrase>. <phrase>experimental</phrase> <phrase>results</phrase> on real-<phrase>live</phrase>, as well as noiseless and noisy simulated <phrase>data</phrase> are presented to show the feasibility, the accuracy and robustness of our registration scheme.
<phrase>stereo</phrase> by multiperspective imaging under 6 dof <phrase>camera</phrase> motion.
multiperspective imaging has been used to recover the structure of a scene. although several <phrase>algorithms</phrase> for structure recovery have been developed as typified by <phrase>stereo</phrase> panoramas, there exists no common framework which subsumes various <phrase>camera</phrase> motions to capture <phrase>stereo</phrase> images. this <phrase>paper</phrase> presents a framework for <phrase>stereo</phrase> by multiperspective imaging, which is <phrase>general</phrase> in that it can handle 6 <phrase>degree</phrase>-of-freedom (dof) <phrase>camera</phrase> motion. we derive geometric constraints, equation for structure recovery and that for an epipolar curve by modeling the acquisition of <phrase>stereo</phrase> images using push-broom cameras (line sensors). we consider a class of <phrase>camera</phrase> motion called a vertical view plane class and demonstrate that several previous <phrase>results</phrase> are really special cases of our <phrase>results</phrase>. numerical examples are given to show the correctness of the derived equations.
planar patch extraction with noisy depth <phrase>data</phrase>.
a hierarchical method for aligning warped meshes.
joined segmentation of <phrase>cortical</phrase> surface and <phrase>brain</phrase> volume in <phrase>mri</phrase> using a <phrase>homotopic</phrase> deformable cellular <phrase>model</phrase>.
modeling from <phrase>reality</phrase>.
spatio-temporal <phrase>fusion</phrase> of <phrase>multiple view</phrase> <phrase>video</phrase> rate 3d surfaces.
we consider the problem of geometric integration and representation of <phrase>multiple views</phrase> of non-rigidly deforming 3d surface <phrase>geometry</phrase> captured at <phrase>video</phrase> rate. instead of treating each frame as a separate mesh we present a representation which takes into consideration temporal and spatial coherence in the <phrase>data</phrase> where possible. we first segment gross base transformations using correspondence based on a <phrase>closest point</phrase> metric and represent these motions as <phrase>piecewise</phrase> rigid transformations. the remaining residual is encoded as displacement maps at each frame giving a displacement <phrase>video</phrase>. at both these stages occlusions and missing <phrase>data</phrase> are interpolated to give a representation which is continuous in space and time. we demonstrate the integration of <phrase>multiple views</phrase> for four different non-rigidly deforming scenes: hand, face, cloth and a composite scene. the approach achieves the integration of <phrase>multiple-view</phrase> <phrase>data</phrase> at different times into one representation which can processed and edited.
fast alignment of 3d geometrical models and 2d <phrase>color images</phrase> using 2d distance maps.
this <phrase>paper</phrase> presents a fast <phrase>pose estimation</phrase> <phrase>algorithm</phrase> of a 3d <phrase>free</phrase> form object in 2d images using 2d distance maps. one of the popular techniques of the <phrase>pose estimation</phrase> of 3d object in 2d image is the point-based method such as the <phrase>icp</phrase> <phrase>algorithm</phrase>. however, the calculation cost for determining point correspondences is expensive. to overcome this problem, <phrase>the proposed method</phrase> utilizes a distance map on the 2d image plane, which is constructed quite rapidly by the fast <phrase>marching</phrase> method. for <phrase>pose estimation</phrase> of the object, contour lines of the 2d image and the projection of the 3d object are aligned using the distance map iteratively by the robust m-<phrase>estimator</phrase>. some <phrase>experimental</phrase> <phrase>results</phrase> with simulated models and actual images of the <phrase>endoscopic</phrase> operation are successfully carried out.
acquisition of view-based 3-d object models using supervised, <phrase>unstructured data</phrase>.
existing techniques for view-based 3-d object recognition using computer vision rely on training the system on a particular object before it is introduced into an environment. this training often consists of taking over 100 images at predetermined points around the viewing <phrase>sphere</phrase> in an attempt to account for most angles for viewing the object. however, in many circumstances, the environment is well known and we only expect to see a small <phrase>subset</phrase> of all possible appearances. in this <phrase>paper</phrase>, we will <phrase>test</phrase> the idea that under these conditions, it is possible to <phrase>train</phrase> an object recognition system on-the-<phrase>fly</phrase> using images of an object as it appears in its environment, with supervision from the user. furthermore, because some views of an object are much more likely than others, the number of training images required can be greatly reduced.
fusing and guiding <phrase>range</phrase> measurements with colour <phrase>video</phrase> images.
combining <phrase>data</phrase> from different sensors provides richer <phrase>data</phrase> for visualisation and helps in automatic detection and recognition of objects. this <phrase>paper</phrase> presents two methods relating two dimensional images from colour <phrase>ccd</phrase> cameras with <phrase>three dimensional</phrase> <phrase>data</phrase> from <phrase>range</phrase> scanners. the first method is applicable in a static case and provides the geometrically correct <phrase>solution</phrase>. the second one offers an approximate <phrase>solution</phrase> but can be used in a dynamic environment for guiding selective <phrase>range</phrase> measurements and simplifies the calibration process. the error introduced by this approximation has been derived as a <phrase>function</phrase> of the distance to the object and <phrase>geometry</phrase> of the set-up. <phrase>results</phrase> of combining <phrase>range</phrase> <phrase>information</phrase> from a <phrase>laser</phrase> <phrase>camera</phrase> and colour <phrase>camera</phrase> are presented.
using <phrase>pca</phrase> to <phrase>model</phrase> shape for <phrase>process control</phrase>.
before <phrase>surface mount</phrase> components can be placed on a <phrase>circuit board</phrase>, it is necessary to print <phrase>solder</phrase> paste onto pads. the paste is then melted to make an electrical connection (reflow). a <phrase>screen printing</phrase> process is used to print the <phrase>solder</phrase> paste onto the board. this is a complicated process with a large number of input parameters. some of these parameters can be controlled and it is the purpose of this work to investigate control of the process based on measurement of the output shape of the printed paste. the shape is measured using a <phrase>laser</phrase> <phrase>range</phrase> scanner <phrase>principal component analysis</phrase> (<phrase>pca</phrase>) is proposed as a tool for describing <phrase>solder</phrase> paste shape with a small number of parameters. this <phrase>paper</phrase> discusses the use of <phrase>pca</phrase> for shape analysis in <phrase>range</phrase> images as well as explaining how such a description can be incorporated into a <phrase>process control</phrase> loop.
fitting of 3d circles and ellipses using a parameter decomposition approach.
many optimization processes encounter a problem inefficiently reaching a global minimum or a near global minimum. traditional methods such as levenberg-marquardt <phrase>algorithm</phrase> and trust-<phrase>region</phrase> method face the problems of dropping into local minima as well. on the other hand, some <phrase>algorithms</phrase> such as <phrase>simulated annealing</phrase> and <phrase>genetic algorithm</phrase> try to find a global minimum but they are mostly time-consuming. without a good initialization, many optimization methods are unable to guarantee a global minimum result. we address a novel method in 3d <phrase>circle</phrase> and <phrase>ellipse</phrase> fitting, which alleviates the <phrase>optimization problem</phrase>. it can not only increase the <phrase>probability</phrase> of getting in global minima but also reduce the computation time. based on our previous work, we decompose the parameters into two parts: one part of parameters can be solved by an analytic or a direct method and another part has to be solved by an iterative procedure. via this scheme, the <phrase>topography</phrase> of optimization space is simplified and therefore we reduce the number of local minima and the computation time. we experimentally compare our method with the traditional ones and show <phrase>superior</phrase> performance.
reducing movement artifacts in whole body scanning.
movement artifacts during whole body scanning are a <phrase>major</phrase> concern when reproducible <phrase>results</phrase> are required. to determine the <phrase>magnitude</phrase> of the artifacts, 11 subjects were scanned with and without a positioning device on the head. the sway of the body was determined by a force plate. it was shown that the pointer on the head reduced the <phrase>magnitude</phrase> of the <phrase>forward</phrase>/backward sway by over 50%. the resulting <phrase>standard deviation</phrase> is less than the resolution of the scanner. movement artifacts within the body, like head rotation, are hard to control. again, the pointer on the head assists in reducing the artifacts. the ventilation depth during the scan determines the shape of the chest and should be standardized to get reproducible <phrase>results</phrase>.
extraction and tracking of surfaces in <phrase>range image</phrase> sequences.
<phrase>range</phrase> <phrase>image registration</phrase>: a <phrase>software</phrase> platform and empirical evaluation.
surface registration by matching oriented points.
for registration of 3-d <phrase>free</phrase>-form surfaces we have developed a representation which requires no <phrase>knowledge</phrase> of the transformation between views. the representation comprises descriptive images associated with oriented points on the surface of an object. constructed using <phrase>single</phrase> point bases, these images are <phrase>data</phrase> level shape descriptions that are used for efficient matching of oriented points. correlation of images is used to establish point correspondences between two views; from these correspondences a rigid transformation that aligns the views is calculated. the transformation is then refined and verified using a modified <phrase>iterative closest</phrase> point <phrase>algorithm</phrase>. to demonstrate the generality of our approach, we present <phrase>results</phrase> from multiple sensing domains.
a <phrase>laser</phrase> <phrase>range</phrase> scanner designed for minimum calibration complexity.
a system for <phrase>semi-automatic</phrase> modeling of complex environments.
we present a <phrase>perception</phrase> system, called <phrase>artisan</phrase>, that semi-automatically builds 3-d models of a robot's workspace. <phrase>range</phrase> images are acquired with a scanning <phrase>laser rangefinder</phrase> and then processed, based an a systematic <phrase>sensor</phrase> characterization, to remove noise and artifacts. complex 3-d objects represented as surface meshes are subsequently recognized in the <phrase>range</phrase> images and inserted into a virtual workspace. this <phrase>graphical</phrase> virtual workspace is then used to by <phrase>human</phrase> operators to plan and execute remote robotic operations.
<phrase>image-based</phrase> techniques for <phrase>digitizing</phrase> environments and artifacts.
registration and integration of textured 3-d <phrase>data</phrase>.
in <phrase>general</phrase>, <phrase>multiple views</phrase> are required to create a complete 3-d <phrase>model</phrase> of an object or of a multi-roomed indoor scene. in this work, we address the problem of merging multiple textured 3-d <phrase>data</phrase> sets, each of which corresponds to a different view of a scene or object. there are two steps to the merging process: registration and integration. to register, or align, <phrase>data</phrase> sets we use a modified version of the <phrase>iterative closest</phrase> point <phrase>algorithm</phrase>; our version, which we call color <phrase>icp</phrase>, considers not only 3-d <phrase>information</phrase>, but color as well. we show that the use of color decreases registration error by an <phrase>order</phrase> of <phrase>magnitude</phrase>. once the 3-d <phrase>data</phrase> sets have been registered we integrate them to produce a seamless, composite 3-d textured <phrase>model</phrase>. our approach to integration uses a 3-d occupancy grid to represent likelihood of spatial occupancy through voting. in addition to occupancy <phrase>information</phrase>, we store <phrase>surface normal</phrase> in each <phrase>voxel</phrase> of the occupancy grid. <phrase>surface normal</phrase> is used to robustly extract a surface from the occupancy grid; on that surface we blend textures from <phrase>multiple views</phrase>.
building symbolic <phrase>information</phrase> for 3d <phrase>human</phrase> body modeling from <phrase>range</phrase> <phrase>data</phrase>.
automatic 3 -- d digitization using a <phrase>laser rangefinder</phrase> with a small field of view.
we address the problem of the <phrase>automation</phrase> of surface digitization using a precision 3-d <phrase>laser rangefinder</phrase>. because of their small field of view, such sensors navigate closely to the digitized object and are subject to collisions. unlike previous techniques that addressed only the exhaustiveness of digitization, this work focuses on collision avoidance. to safely identify empty space, shadow and occlusion phenomena are carefully analyzed, and so is the effect of sampling. then, the planning problem is solved using a hierarchical approach. at low level, we digitize a <phrase>single</phrase> view and address collision avoidance using path planning techniques. at <phrase>high</phrase> level, the problem becomes the choice of the next best view. some implementation details and <phrase>experimental</phrase> <phrase>results</phrase> are presented.
<phrase>three-dimensional</phrase> modelling and rendering of the <phrase>human</phrase> skeletal trunk from 2d radiographic images.
compact 3d profilometer with <phrase>grazing</phrase> incidence <phrase>diffraction</phrase> <phrase>optics</phrase>.
fast multiple-baseline <phrase>stereo</phrase> with occlusion.
this <phrase>paper</phrase> presents a new and fast <phrase>algorithm</phrase> for multi-baseline <phrase>stereo</phrase> designed to handle the occlusion problem. the <phrase>algorithm</phrase> is a <phrase>hybrid</phrase> between fast <phrase>heuristic</phrase> occlusion overcoming <phrase>algorithms</phrase> that precompute an approximate visibility and slower methods that use correct visibility handling. our approach is based on iterative <phrase>dynamic programming</phrase> and computes simultaneously disparity and <phrase>camera</phrase> visibility. interestingly, <phrase>dynamic programming</phrase> makes it possible to compute exactly part of the visibility <phrase>information</phrase>. the remainder is obtained through heuristics. the validity of our scheme is established using real imagery with <phrase>ground truth</phrase> and compares favorably with otherstate-of-the-<phrase>art</phrase> multi-baseline <phrase>stereo</phrase> <phrase>algorithms</phrase>.
building 3-d <phrase>city</phrase> models from multiple unregistered profile maps.
the <phrase>paper</phrase> presents an approach for building 3-d <phrase>city</phrase> models for <phrase>virtual environments</phrase> from multiple 3-d <phrase>data</phrase> sets acquired from different viewpoints by <phrase>light</phrase> striping. the raw <phrase>data</phrase> sets are represented as <phrase>single</phrase> valued parametric surfaces called the 3-d profile maps. the profile maps are registered to the same coordinate system by an iterative surface matching <phrase>algorithm</phrase> developed previously. the registration proceeds hierarchically from low to <phrase>high</phrase> resolution and all the <phrase>data</phrase> sets are matched simultaneously but an initial registration is assumed to be known. after having segmented each map by a <phrase>region</phrase> growing <phrase>algorithm</phrase>, the maps are integrated into a <phrase>piecewise</phrase> planar surface <phrase>model</phrase> by merging compatible segments in the overlapping areas. the <phrase>borders</phrase> of the segments are also traced an the parametric domains of the maps as a step for building a wireframe <phrase>model</phrase>. <phrase>test</phrase> <phrase>results</phrase> are shown in the case of a <phrase>scale model</phrase> of an <phrase>urban area</phrase> digitized in <phrase>laboratory</phrase> conditions.
an improved calibration technique for coupled <phrase>single</phrase>-row telemeter and <phrase>ccd camera</phrase>.
toward a successful 3d and textural <phrase>reconstruction</phrase> of <phrase>urban</phrase> scenes, the use of both <phrase>single</phrase>-row based telemetric and photographic <phrase>data</phrase> in a same framework has proved to be a powerful technique. a necessary condition to obtain good <phrase>results</phrase> is to accurately calibrate the telemetric and photographic sensors together. we present a study of this calibration process and propose an improved extrinsic calibration technique. it is based on an existing technique which consists in scanning a planar pattern in several poses, giving a set of relative position and orientation constraints. the <phrase>innovation</phrase> is the use of a more appropriate <phrase>laser beam</phrase> distance between telemetric points and the planar <phrase>target</phrase>. moreover, we use robust methods to manage <phrase>outliers</phrase> at several steps of the <phrase>algorithm</phrase>. improved <phrase>results</phrase> on both theoretical and <phrase>experimental</phrase> <phrase>data</phrase> are given.
self-calibration of a <phrase>light</phrase> striping system by matching multiple 3-d profile maps.
transform-based methods for indexing and retrieval of 3d objects.
we compare two transform-based indexing methods for retrieval of 3d objects. we apply 3d <phrase>discrete fourier transform</phrase> (<phrase>dft</phrase>) and 3d <phrase>radial</phrase> cosine transform (rct) to the voxelized <phrase>data</phrase> of 3d objects. rotation invariant features are derived from the coefficients of these transforms. furthermore we compare two different <phrase>voxel</phrase> representations, namely, <phrase>binary</phrase> denoting object and background space, and continuous after distance transformation. in the <phrase>binary</phrase> <phrase>voxel</phrase> representation the <phrase>voxel</phrase> values are simply set to 1 on the surface of the object and 0 elsewhere. in the continuous-valued representation the space is filled with a <phrase>function</phrase> of distance transform. the rotation invariance properties of the <phrase>dft</phrase> and rct schemes are analyzed. we have conducted retrieval experiments on the <phrase>princeton</phrase> shape benchmark and investigated the retrieval performance of the methods using several quality measures.
a <phrase>multi-resolution</phrase> <phrase>icp</phrase> with <phrase>heuristic</phrase> <phrase>closest point</phrase> search for fast and robust 3d registration of <phrase>range</phrase> images.
effective 3d modeling of heritage sites.
identifying the interface between two <phrase>sand</phrase> materials.
to study the behavior of <phrase>water</phrase> flow at interfaces between different <phrase>soil</phrase> materials we made <phrase>computed tomography</phrase> scans of <phrase>sand</phrase> samples using <phrase>synchrotron light</phrase>. the samples were prepared with an interface between two <phrase>sand</phrase> materials. the contact points between grains at the interface between the sands were identified using a combination of <phrase>watershed</phrase> segmentation and a classifier that used the <phrase>grain-size</phrase> and -location. the process from a bilevel image to a classified image is described. in the classified image five classes are represented; two for the grains and three for the contact points to represent intra- and inter-class contact points.
<phrase>silhouette</phrase> and <phrase>stereo</phrase> <phrase>fusion</phrase> for 3d object modeling.
in this <phrase>paper</phrase>, we present a new approach to <phrase>high</phrase> quality 3d object <phrase>reconstruction</phrase>. starting from a calibrated <phrase>sequence</phrase> of <phrase>color images</phrase>, the <phrase>algorithm</phrase> is able to reconstruct both the 3d <phrase>geometry</phrase> and the texture. the core of the <phrase>method is based</phrase> on a deformable <phrase>model</phrase>, which defines the framework where texture and <phrase>silhouette</phrase> <phrase>information</phrase> can be fused. this is achieved by defining two external forces based on the images: a texture driven force and a <phrase>silhouette</phrase> driven force. the texture force is computed in two steps: a multi-<phrase>stereo</phrase> correlation voting approach and a <phrase>gradient</phrase> <phrase>vector</phrase> flow <phrase>diffusion</phrase>. due to the <phrase>high</phrase> resolution of the voting approach, a multi-grid version of the <phrase>gradient</phrase> <phrase>vector</phrase> flow has been developed. concerning the <phrase>silhouette</phrase> force, a new formulation of the <phrase>silhouette</phrase> constraint is derived. it provides a robust way to integrate the silhouettes in the <phrase>evolution</phrase> <phrase>algorithm</phrase>. as a consequence, we are able to recover the contour generators of the <phrase>model</phrase> at the end of the iteration process. finally, a texture map is computed from the original images for the reconstructed 3d <phrase>model</phrase>.
extracting surface patches from complete <phrase>range</phrase> descriptions.
constructing a full <phrase>cad</phrase> <phrase>model</phrase> of a part requires feature descriptions from all sides; in this case we consider surface patches as the geometric primitives. most previous <phrase>research</phrase> in surface patch extraction has concentrated on extracting patches from a <phrase>single</phrase> view. this leads to several problems with aligning and combining partial patch fragments in <phrase>order</phrase> to produce complete part models. we have avoided these problems by adapting our <phrase>single</phrase> view, <phrase>range</phrase> <phrase>data</phrase> segmentation program to extract patches, and thus models, directly from fully merged <phrase>range</phrase> datasets.
foreword.
scanning and processing 3d objects for web display.
coordination of appearance and motion <phrase>data</phrase> for virtual view generation of traditional dances.
a novel method is proposed for virtual view generation of traditional dances. in the proposed framework, a traditional <phrase>dance</phrase> is captured separately for appearance registration and motion registration. by coordinating the appearance and motion <phrase>data</phrase>, we can easily control virtual <phrase>camera</phrase> motion within a <phrase>dancer</phrase>-centered coordinate system. for this purpose, a coordination problem should be solved between the appearance and motion <phrase>data</phrase>, since they are captured separately and the <phrase>dancer</phrase> moves freely in the room. the present <phrase>paper</phrase> shows a practical <phrase>algorithm</phrase> to solve it. a set of <phrase>algorithms</phrase> are also provided for appearance and motion registration, and virtual view generation from archived <phrase>data</phrase>. in the appearance registration, a 3d <phrase>human</phrase> shape is recovered in each time from a set of input images after suppressing their backgrounds. by combining the recovered 3d shape and a set of images for each time, we can <phrase>compose</phrase> archived <phrase>dance</phrase> <phrase>data</phrase>. in the motion registration, <phrase>stereoscopic</phrase> tracking is accomplished for color markers placed on the <phrase>dancer</phrase>. a virtual view generation is formalized as a color blending among <phrase>multiple views</phrase>, and a novel and efficient <phrase>algorithm</phrase> is proposed for the composition of a natural virtual view from a set of images. in <phrase>the proposed method</phrase>, weightings of the <phrase>linear combination</phrase> are calculated from both an assumed viewpoint and a <phrase>surface normal</phrase>.
exploiting mirrors for <phrase>laser</phrase> stripe 3d scanning.
virtual <phrase>reconstruction</phrase> of broken and unbroken <phrase>pottery</phrase>.
multisensor <phrase>fusion</phrase> for volumetric <phrase>reconstruction</phrase> of large outdoor areas.
this <phrase>paper</phrase> presents techniques for the merging of 3d <phrase>data</phrase> coming from different sensors, such as ground and aerial <phrase>laser</phrase> <phrase>range</phrase> scans. the 3d models created are reconstructed to give a photo-realistic scene enabling interactive virtual walkthroughs, measurements and scene change analysis. the reconstructed <phrase>model</phrase> is based on a weighted integration of all available <phrase>data</phrase> based on <phrase>sensor</phrase>-specific parameters such as noise level, accuracy, <phrase>inclination</phrase> and reflectivity of the <phrase>target</phrase>, spatial distribution of points. the <phrase>geometry</phrase> is robustly reconstructed with a volumetric approach. once registered and weighed, all <phrase>data</phrase> is re-sampled in a <phrase>multi-resolution</phrase> distance field usingout-of-core techniques. the final mesh is extracted by contouring the <phrase>iso</phrase>-surface with a feature preserving dual contouring <phrase>algorithm</phrase>. the <phrase>paper</phrase> shows <phrase>results</phrase> of the above technique applied to <phrase>verona</phrase> (<phrase>italy</phrase>) <phrase>city</phrase> centre.
further improving geometric fitting.
we give a formal definition of geometric fitting in a way that suits computer vision applications. we point out that the performance of geometric fitting should be evaluated in the limit of small noise rather than in the limit of a large number of <phrase>data</phrase> as recommended in the statistical <phrase>literature</phrase>. taking the kcr <phrase>lower</phrase> bound as an optimality requirement and focusing on the linearized constraint case, we compare the accuracy of kanatanis <phrase>renormalization</phrase> with <phrase>maximum likelihood</phrase> (ml) approaches including the fns of chojnacki et al. and the heiv of leedan and meer. our analysis reveals the existence of a method <phrase>superior</phrase> to all these.
solving <phrase>architectural</phrase> modelling problems using <phrase>knowledge</phrase>.
processing <phrase>range</phrase> <phrase>data</phrase> for <phrase>reverse engineering</phrase> and <phrase>virtual reality</phrase>.
a low-cost <phrase>range</phrase> finder using a visually located, <phrase>structured light</phrase> source.
refining <phrase>triangle</phrase> meshes by non-linear subdivision.
real-time <phrase>range</phrase> scanning of deformable surfaces by adaptively coded <phrase>structured light</phrase>.
deformable <phrase>model</phrase> with adaptive mesh and automated <phrase>topology</phrase> changes.
scale selection for classification of point-sampled 3-d surfaces.
<phrase>three-dimensional</phrase> ladar <phrase>data</phrase> are commonly used to perform scene understanding for outdoor <phrase>mobile</phrase> <phrase>robots</phrase>, specifically in natural terrain. one effective method is to classify points using features based on local <phrase>point cloud</phrase> distribution into surfaces, linear structures or clutter volumes. but the local features are computed using 3-d points within a support-volume. local and global point <phrase>density</phrase> variations and the presence of multiple <phrase>manifolds</phrase> make the problem of selecting the size of this support volume, or scale, challenging. in this <phrase>paper</phrase> we adopt an approach inspired by recent developments in <phrase>computational geometry</phrase> [5] and investigate the problem of automatic <phrase>data</phrase>-driven scale selection to improve <phrase>point cloud</phrase> classification. the approach is validated with <phrase>results</phrase> using <phrase>data</phrase> from different sensors in various environments classified into different terrain types (<phrase>vegetation</phrase>, solid surface and linear structure).
an <phrase>automation</phrase> system for <phrase>industrial</phrase> 3-d <phrase>laser</phrase> <phrase>digitizing</phrase>.
estimation of elastic constants from 3d <phrase>range</phrase>-flow.
<phrase>bayesian</phrase> estimation of distance and <phrase>surface normal</phrase> with a time-of-flight <phrase>laser rangefinder</phrase>.
advances in the cooperation of <phrase>shape from shading</phrase> and <phrase>stereo</phrase> vision.
the parallel <phrase>iterative closest</phrase> point <phrase>algorithm</phrase>.
building 3d facial models and detecting face pose in 3d space.
the mapping of texture on vr polygonal models.
<phrase>road</phrase> surface inspection using <phrase>laser</phrase> scanners adapted for the <phrase>high</phrase> precision measurements of large flat surfaces.
toward a near optimal quad/<phrase>triangle</phrase> subdivision surface fitting.
in this <phrase>paper</phrase> we present a new framework for subdivision surface fitting of arbitrary surfaces (not closed objects) represented by polygonal meshes. our approach is particularly suited for output surfaces from a mechanical or <phrase>cad</phrase> object segmentation for a <phrase>piecewise</phrase> subdivision surface approximation. our <phrase>algorithm</phrase> produces a mixed quadrangle-<phrase>triangle</phrase> control mesh, near optimal in terms of face and vertex numbers while remaining <phrase>independent</phrase> of the connectivity of the input mesh. the first step approximates the boundaries with subdivision curves and creates an initial subdivision surface by optimally linking the boundary control points with respect to the lines of <phrase>curvature</phrase> of the <phrase>target</phrase> surface. then, a second step optimizes the initial control <phrase>polyhedron</phrase> by iteratively moving control points and enriching regions according to the error distribution. experiments conducted on several surfaces and on a whole segmented mechanical object, have <phrase>proven</phrase> the coherency and theefficiency of our <phrase>algorithm</phrase>, compared with existing methods.
an efficient scattered <phrase>data</phrase> approximation using multilevel b-splines based on quasi-interpolants.
in this <phrase>paper</phrase>, we propose an efficient <phrase>approximation algorithm</phrase> using multilevel b-splines based on quasi-interpolants. multilevel technique uses a coarse to fine hierarchy to generate a <phrase>sequence</phrase> of bicubic <phrase>b-spline</phrase> functions whose sum approaches the desired <phrase>interpolation</phrase> <phrase>function</phrase>. to compute a set of control points, quasi-interpolants gives a procedure for deriving local spline approximation methods where a <phrase>b-spline</phrase> coefficient only depends on <phrase>data</phrase> points taken from the <phrase>neighborhood</phrase> of the support corresponding the <phrase>b-spline</phrase>. <phrase>experimental</phrase> <phrase>results</phrase> show that the smooth <phrase>surface reconstruction</phrase> with <phrase>high</phrase> accuracy can be obtained from a selected set of scattered or dense irregular samples.
the <phrase>digital</phrase> <phrase>michelangelo</phrase> project.
evaluating collinearity constraint for automatic <phrase>range</phrase> <phrase>image registration</phrase>.
while most of the existing <phrase>range</phrase> <phrase>image registration</phrase> <phrase>algorithms</phrase> either have to extract and match structural (geometric or optical) features or have to estimate the motion parameters of interest from <phrase>outliers</phrase> corrupted point correspondence <phrase>data</phrase> for the elimination of false matches in the process of <phrase>image registration</phrase>, the registration error and the collinearity error derived directly from the traditional <phrase>closest point</phrase> criterion are also capable of doing the same job. however, the latter has an advantage of easy implementation. the purpose of this <phrase>paper</phrase> is to investigate which definition of collinearity is more accurate and stable in eliminating false matches inevitably introduced by the <phrase>closest point</phrase> criterion. the experiments based on real images show the advantages and disadvantages of different definitions of collinearity.
<phrase>multi-resolution</phrase> modeling and locally refined <phrase>collision detection</phrase> for <phrase>haptic</phrase> interaction.
the computational cost of a <phrase>collision detection</phrase> (<phrase>cd</phrase>) <phrase>algorithm</phrase> on polygonal surfaces depends highly on the complexity of the models. a novel "locally refined" approach is introduced in this <phrase>paper</phrase> for fast <phrase>cd</phrase> in <phrase>haptic</phrase> rendering applications, e.g. <phrase>haptic</phrase> <phrase>surgery</phrase> and <phrase>haptic</phrase> <phrase>sculpture</phrase> simulations. exact interference detections are performed on proposed locally refined meshes, which are in <phrase>multi-resolution</phrase> representation. the meshes are generated using mesh simplification and space <phrase>partition</phrase>. a new bvh <phrase>algorithm</phrase> called "active bounding <phrase>tree</phrase>", or <phrase>ab</phrase>-<phrase>tree</phrase>, handling collision queries is introduced. at runtime the meshes are dynamically refined to higher resolution in areas that are most likely to collide with other objects. the <phrase>algorithms</phrase> are successfully demonstrated in an interactive <phrase>haptic</phrase> environment. compared to existing <phrase>cd</phrase> <phrase>algorithms</phrase> on <phrase>single</phrase> resolution models, noticeable <phrase>performance improvement</phrase> has been observed in terms of the precision of collision queries, <phrase>frame rate</phrase>, and <phrase>memory</phrase> usage.
evaluating structural constraints for accurate <phrase>range</phrase> <phrase>image registration</phrase>.
an adaptive dandelion <phrase>model</phrase> for reconstructing spherical terrain-like <phrase>visual hull</phrase> surfaces.
in this <phrase>paper</phrase> we present an adaptive dandelion <phrase>model</phrase> for reconstructing spherical terrain-like <phrase>visual hull</phrase> (vh) surfaces. the dandelion <phrase>model</phrase> represents a solid by a pencil of organized line segments emitted from a common point. the directions and the <phrase>topology</phrase> of the line segments are derived from the <phrase>triangle</phrase> facets of a <phrase>geodesic</phrase> <phrase>sphere</phrase>, which are recursively subdivided until the desired precision is achieved. the initial lines are cut by silhouettes in 2d and then lifted back to 3d to determine the ending points of the line segments defining sampling points on the spherical terrain-like vh surface. a <phrase>mesh model</phrase> can be easily constructed from the dandelion <phrase>model</phrase>. our <phrase>algorithm</phrase> has the advantages of controllable precision, adaptive resolution, simplicity and speediness. we validate our <phrase>algorithm</phrase> by theories and experiments.
a fast and accurate 3-d <phrase>rangefinder</phrase> using the biris <phrase>technology</phrase>: the trid <phrase>sensor</phrase>.
in many <phrase>industrial</phrase> applications such as autonomous vehicle guidance and robotic manipulations, reliable 3-d informations must be extracted from the scene in <phrase>order</phrase> to provide a robust description of the environment in which the system evolves. this <phrase>paper</phrase> presents the trid <phrase>sensor</phrase>, a simple, fast, robust and accurate 3-d sensing device based on the biris <phrase>technology</phrase>. trid is limited to a lateral resolution of one point. <phrase>experimental</phrase> <phrase>results</phrase> show that an accuracy better than 0.15% can be obtained for the depth component (z) of 3-d points at a distance of 1.3 meter from a wooden <phrase>black</phrase> painted <phrase>beam</phrase> surface with an acquisition rate of 8.5 points/s.
3d statistical shape models for <phrase>medical</phrase> <phrase>image segmentation</phrase>.
reliable and rapidly-converging <phrase>icp</phrase> <phrase>algorithm</phrase> using multiresolution smoothing.
<phrase>frequency domain</phrase> estimation of 3-d rigid motion based on <phrase>range</phrase> and intensity <phrase>data</phrase>.
<phrase>video</phrase>-rate registered <phrase>range</phrase> and intensity <phrase>data</phrase> are at reach of current <phrase>sensor</phrase> <phrase>technology</phrase>. this wealth of <phrase>data</phrase> can be profitably exploited in <phrase>order</phrase> to estimate rigid motion parameters as the approaches to 3-d <phrase>motion estimation</phrase>, based on the <phrase>optical flow</phrase> of both types of <phrase>data</phrase>, indicate. this work introduces an <phrase>alternative</phrase> for 3-d <phrase>motion estimation</phrase> based on the <phrase>fourier transform</phrase> of the 3-d intensity <phrase>function</phrase> implicitly described by the registered time-sequences of <phrase>range</phrase> and intensity <phrase>data</phrase>. the proposed procedure can <phrase>lead</phrase> to an unsupervised method for 3-d rigid <phrase>motion estimation</phrase>. this method has several advantages related to the fact that it uses the total available <phrase>information</phrase> and not sets of features. with respect to <phrase>memory</phrase> occupancy the use of a time-<phrase>sequence</phrase> of a 3-d intensity <phrase>function</phrase> represents a considerable <phrase>data</phrase> reduction with respect to a pair of time-sequences of 2-d functions. the proposed technique, which extends to the 3-d case previous <phrase>frequency domain</phrase> estimation <phrase>algorithms</phrase> developed for the planar case, retain their robustness.
automatic modeling of animatable virtual humans - a survey.
<phrase>rangefinder</phrase> using time correlated <phrase>single</phrase> <phrase>photon</phrase> counting.
this <phrase>paper</phrase> describes a method for acquiring <phrase>range</phrase> <phrase>data</phrase> based on time-correlated <phrase>single</phrase> <phrase>photon</phrase> counting (tcspc) and details the <phrase>data analysis</phrase> techniques used to compute the <phrase>range</phrase> measurements. the <phrase>prototype</phrase> <phrase>sensor</phrase> being built in our <phrase>laboratory</phrase> is capable of measuring up to 100 points per second with an accuracy of about 15 /<phrase>spl</phrase> mu/m.
generation of geometric <phrase>model</phrase> by registration and integration of <phrase>multiple range</phrase> images.
surface <phrase>curvature</phrase> estimation from the signed distance field.
simultaneous determination of registration and deformation parameters among 3d <phrase>range</phrase> images.
conventional registration <phrase>algorithms</phrase> are mostly concerned with <phrase>rigid-body</phrase> transformation parameters between a pair of 3d <phrase>range</phrase> images. our proposed framework aims to determine, in a unified manner, not only such rigid transformation parameters but also various deformation parameters, assuming that the deformation we handle here is strictly defined by some parameterized formulation derived from the deformation mechanism. while conventional registration <phrase>algorithms</phrase> usually calcurate six parameters (three <phrase>translation</phrase> and three rotation parameters), our proposed <phrase>algorithm</phrase> estimates deformation parameters as well. in this <phrase>paper</phrase>, we describe how we formulated such an <phrase>algorithm</phrase>, implemented it, and evaluated its performance.
a method of style <phrase>discrimination</phrase> of <phrase>oil painting</phrase> based on 3d <phrase>range</phrase> <phrase>data</phrase>.
a portable <phrase>three-dimensional</phrase> digitizer.
a portable <phrase>three-dimensional</phrase> digitizer using a <phrase>monocular</phrase> <phrase>camera</phrase> is presented in this <phrase>paper</phrase>. the digitizer automatically acquires the shape of a <phrase>target</phrase> object as well as its texture. the digitizer has the following advantages: 1) compact and inexpensive, 2) skill-<phrase>free</phrase> 3d image acquisition, and 3) handles a wide <phrase>range</phrase> of objects of various materials. the <phrase>digitizing</phrase> <phrase>algorithm</phrase> is based on the "<phrase>shape-from-silhouette</phrase>" framework, where several novel techniques are embedded as follows. in the <phrase>silhouette</phrase> extraction, not only <phrase>pixel</phrase>-level <phrase>subtraction</phrase> between images but also <phrase>region</phrase>-level <phrase>subtraction</phrase> are embedded so as to achieve precise extraction. the texture acquisition is treated as a labeling problem in an <phrase>energy</phrase> minimization framework, which enables us to get realistic textures with a simple operation. our experiments showed that the <phrase>digitizing</phrase> speed of the digitizer was practical.
calibration of a <phrase>laser</phrase> stripe profiler.
3-d motion and shape from multiple <phrase>image sequences</phrase>.
streaming transmission of point-sampled <phrase>geometry</phrase> based on view-dependent level-of-detail.
active balloon <phrase>model</phrase> based on 3d <phrase>skeleton</phrase> extraction by competitive learning.
computations on a spherical view space for efficient planning of viewpoints in 3-d object modeling.
<phrase>reconstruction</phrase> of complex environments by robust pre-aligned <phrase>icp</phrase>.
stroboscopic <phrase>stereo</phrase> <phrase>rangefinder</phrase>.
fast and robust registration of 3d surfaces using low <phrase>curvature</phrase> patche.
automatic <phrase>reconstruction</phrase> of 3d objects using a <phrase>mobile</phrase> monoscopic <phrase>camera</phrase>.
a method for the automatic <phrase>reconstruction</phrase> of 3d objects from multiple <phrase>camera</phrase> views for 3d <phrase>multimedia</phrase> applications is presented. conventional 3d <phrase>reconstruction</phrase> techniques use equipment that restrict the flexibility of the user. in <phrase>order</phrase> to increase this flexibility, the presented method is characterized by a simple measurement environment, that consists of a new calibration pattern placed below the object allowing object and pattern acquisition simultaneously. this ensures, that each view can be calibrated individually. from these obtained calibrated <phrase>camera</phrase> views, a textured 3d wireframe <phrase>model</phrase> is estimated using a <phrase>shape-from-silhouette</phrase> approach and <phrase>texture mapping</phrase> of the original <phrase>camera</phrase> views. experiments with this system have confirmed a significant gain of flexibility for the user and a drastic reduction of costs for technical equipment while ensuring comparable <phrase>model</phrase> quality as conventional <phrase>reconstruction</phrase> techniques at the same time.
3d <phrase>reconstruction</phrase> from two orthogonal views using <phrase>simulated annealing</phrase> approach.
automatic <phrase>model</phrase> refinement for 3d <phrase>reconstruction</phrase> with <phrase>mobile</phrase> <phrase>robots</phrase>.
locating landmarks on <phrase>human</phrase> body scan <phrase>data</phrase>.
<phrase>software</phrase> for locating <phrase>anthropometric</phrase> landmarks from a <phrase>cloud</phrase> of more than 100000 <phrase>three dimensional</phrase> <phrase>data</phrase> points, captured from a <phrase>human</phrase> subject, is presented. the <phrase>software</phrase> is part of an incremental approach that progressively refines the identification of <phrase>data</phrase> points. the first phase of identification is to <phrase>orient</phrase> and segment the <phrase>human</phrase> body <phrase>data</phrase> points. <phrase>algorithms</phrase> for these tasks are presented, with a description of their use. one of the <phrase>algorithms</phrase>, a discrete point cusp detector is believed to be unique. the <phrase>software</phrase> has been tested on twenty different body scan <phrase>data</phrase> sets and shown to be robust.
fast simultaneous alignment of <phrase>multiple range</phrase> images using index images.
this <phrase>paper</phrase> describes a fast and easy-to-use simultaneous alignment method of <phrase>multiple range</phrase> images. the most time consuming part of alignment process is searching corresponding points. although "inverse calibration" method quickly searches corresponding points in complexity o(n), where n is the number of vertices, the method requires some look-up tables or precise sensors parameters. then, we propose an easy-to-use method that uses "index image": "index image" can be rapidly created using <phrase>graphics hardware</phrase> without precise sensors parameters. for fast computation of rigid transformation matrices of a large number of <phrase>range</phrase> images, we utilized linearized <phrase>error function</phrase> and applied incomplete cholesky conjugate <phrase>gradient</phrase> (iccg) method for solving linear equations. some <phrase>experimental</phrase> <phrase>results</phrase> that aligned a large number of <phrase>range</phrase> images measured with <phrase>laser</phrase> <phrase>range</phrase> sensors show the effectiveness of our method.
parallel alignment of a large number of <phrase>range</phrase> images.
contour point tracking by enforcement of rigidity constraints.
the <phrase>aperture</phrase> problem is one of the omnipresent issues in computer vision. its local character constrains point matching to <phrase>high</phrase> textured areas, so that points ingradient- oriented regions (such as straight lines) can not be reliably matched. we propose a new method to overcome this problem by devising a global matching strategy under the factorization framework. we solve the n-frame correspondence problem under this context by assuming the rigidity of the scene. to this end, a geometric contraint is used that selects the matching <phrase>solution</phrase> resulting in a rank-4 observation matrix. the rank of the observation matrix is a <phrase>function</phrase> of the matching solutions associated to each image and as such a simulteaneous <phrase>solution</phrase> for all frames has to be found. an optimization procedure is used in this text in <phrase>order</phrase> to find the <phrase>solution</phrase>.
dense disparity estimation using gabor filters and image derivatives.
incremental catmull-clark subdivision.
in this <phrase>paper</phrase>, a new adaptive method for catmull-clark subdivision is introduced. adaptive subdivision refines specific areas of a <phrase>model</phrase> according to user or application needs. naive adaptive subdivision <phrase>algorithm</phrase> change the connectivity of the mesh, causing geometrical inconsistencies that alter the limit surface. our method expands the specified <phrase>region</phrase> of the mesh such that when it is adaptively subdivided, it produces a smooth surface whose selected <phrase>area</phrase> is identical to when the entire mesh is refined. this technique also produces a surface with an increasing level of detail from coarse to fine areas of the surface. we compare our adaptive subdivision with other schemes and present some example applications.
a contour-<phrase>based approach</phrase> to 3d text labeling on triangulated surfaces.
this <phrase>paper</phrase> presents a simple and efficient method of forming a 3d text <phrase>label</phrase> on a 3d triangulated surface. the <phrase>label</phrase> is formed by projecting the 2d contours that define the text <phrase>silhouette</phrase> onto the triangulated surface, forming 3d contour paths. surface <phrase>polygons</phrase> upon which the 3d contour paths lie are retriangulated using a novel approach that forms a polyline defining the <phrase>region</phrase> outside the contour. this <phrase>algorithm</phrase> produces labeled 3d surfaces that conform to the specifications of the <phrase>stl</phrase> format, making them suitable for fabrication by a <phrase>rapid prototyping</phrase> machine. we demonstrate the effectiveness of the <phrase>algorithm</phrase> in forming flat and extruded <phrase>labels</phrase> on non-trivial surfaces.
<phrase>nefertiti</phrase>: a query by content <phrase>software</phrase> for <phrase>three-dimensional</phrase> models <phrase>databases</phrase> <phrase>management</phrase>.
this <phrase>paper</phrase> presents a new approach for the classification and retrieval of <phrase>three-dimensional</phrase> images and models from <phrase>databases</phrase>. a set of retrieval <phrase>algorithms</phrase> is introduced. these <phrase>algorithms</phrase> are <phrase>content-based</phrase>, meaning that the input is not made out of keywords but of <phrase>three-dimensional</phrase> models. <phrase>tensor</phrase> of <phrase>inertia</phrase>, distribution of normals, distribution of cords and multiresolution analysis are used to describe each <phrase>model</phrase>. the <phrase>database</phrase> can be searched by scale, shape or color or any combination of these parameters. a user <phrase>friendly</phrase> interface makes the retrieval operation simple and intuitive and allows to edit reference models according to the specifications of the user. <phrase>experimental</phrase> <phrase>results</phrase> using a <phrase>database</phrase> of more than 400 <phrase>range</phrase> images and 1000 <phrase>vrml</phrase> models are presented.
automatic feature correspondence for scene <phrase>reconstruction</phrase> from <phrase>multiple views</phrase>.
dual-<phrase>beam</phrase> <phrase>structured-light</phrase> scanning for 3-d object modeling.
estimating pose through local <phrase>geometry</phrase>.
constructing nurbs surface <phrase>model</phrase> from scattered and unorganized <phrase>range</phrase> <phrase>data</phrase>.
a point-and-shoot color 3d <phrase>camera</phrase>.
a <phrase>range</phrase> image refinement technique for <phrase>multi-view</phrase> 3d <phrase>model</phrase> <phrase>reconstruction</phrase>.
<phrase>reconstruction</phrase> of surfaces behind occlusions in <phrase>range</phrase> images.
a fast point-to-<phrase>tangent</phrase> plane technique for <phrase>multi-view</phrase> registration.
<phrase>bayesian</phrase> modelling of <phrase>camera</phrase> calibration and <phrase>reconstruction</phrase>.
<phrase>camera</phrase> calibration methods, whether implicit or explicit, are a critical part of most 3d vision systems. these methods involve estimation of a <phrase>model</phrase> for the <phrase>camera</phrase> that <phrase>produced</phrase> the visual input, and subsequently to infer the 3d structure that gave rise to the input. however, in these systems the error in calibration is typically unknown, or if known, the effect of calibration error on subsequent processing (e.g. 3d <phrase>reconstruction</phrase>) is not accounted for. in this <phrase>paper</phrase>, we propose a <phrase>bayesian</phrase> <phrase>camera</phrase> calibration method that explicitly computes calibration error, and we show how <phrase>knowledge</phrase> of this error can be used to improve the accuracy of subsequent processing. what distinguishes the work is the explicit computation of a posterior distribution on unknown <phrase>camera</phrase> parameters, rather than just a best estimate. marginalizing (averaging) subsequent estimates by this posterior is shown to reduce <phrase>reconstruction</phrase> error over calibration approaches that rely on a <phrase>single</phrase> best estimate. the method is made practical using sampling techniques, that require only the evaluation of the calibration <phrase>error function</phrase> and the specification of priors. samples with their corresponding <phrase>probability</phrase> weights can be used to produce better estimates of the <phrase>camera</phrase> parameters. moreover, these samples can be directly used to improve estimates that rely on calibration <phrase>information</phrase>, like 3d <phrase>reconstruction</phrase>. we evaluate our method using simulated <phrase>data</phrase> for a <phrase>structure from motion</phrase> problem, in which the same point matches are used to calibrate the <phrase>camera</phrase>, estimate the motion, and reconstruct the 3d <phrase>geometry</phrase>. our <phrase>results</phrase> show improved <phrase>reconstruction</phrase> over non-linear <phrase>camera</phrase> calibration methods like the <phrase>maximum likelihood</phrase> estimate. additionally, this approach scales much better in the face of increasingly noisy point matches.
affine transformations of 3d objects represented with <phrase>neural networks</phrase>.
acquisition of <phrase>three-dimensional</phrase> <phrase>information</phrase> in a real environment by using the <phrase>stereo</phrase> omni-directional system (<phrase>sos</phrase>).
a registration aid.
although the <phrase>iterative closest</phrase> point <phrase>algorithm</phrase> is very effective in registering <phrase>range</phrase> <phrase>data</phrase>, the quality of registration depends on the <phrase>geometry</phrase> of the sampled surfaces. this work presents a registration aid which can greatly reduce the possibility of catastrophic registration failure, increase the quality of registration and register <phrase>range</phrase> <phrase>data</phrase> which does not overlap. by placing the aid into the scene along with the object(s) of interest, <phrase>range</phrase> images taken from any vantage point within the <phrase>range</phrase> scanner's workspace can be registered onto a <phrase>model</phrase> of the aid and thus into a common <phrase>reference frame</phrase>. by considering the causes of registration failure, the surface of the aid is designed so that any <phrase>range</phrase> image taken of it from any point in the scanner's workspace will properly register with a <phrase>model</phrase> of the aid. the reliability of the aid is demonstrated with a <phrase>monte carlo</phrase> experiment and its utility is demonstrated with examples from an automated surface acquisition system.
<phrase>free</phrase>-form <phrase>surface reconstruction</phrase> from multiple images .
hand-held acquisition of 3d models with a <phrase>video camera</phrase>.
a mrf formulation for coded <phrase>structured light</phrase>.
<phrase>multimedia</phrase> projectors and cameras make possible the use of <phrase>structured light</phrase> to solve problems such as 3d <phrase>reconstruction</phrase>, disparity map computation and <phrase>camera</phrase> or projector calibration. each projector displays patterns over a scene viewed by a <phrase>camera</phrase>, thereby allowing automatic computation of <phrase>camera</phrase>-projector <phrase>pixel</phrase> correspondences. this <phrase>paper</phrase> introduces a new <phrase>algorithm</phrase> to establish this correspondence in difficult cases of image acquisition. a probabilistic <phrase>model</phrase> formulated as a <phrase>markov random field</phrase> uses the stripe images to find the most likely correspondences in the presence of noise. our <phrase>model</phrase> is specially tailored to handle the unfavorable projector-<phrase>camera</phrase> <phrase>pixel</phrase> ratios that occur in multiple-projector <phrase>single</phrase>-<phrase>camera</phrase> setups. for the case where more than one <phrase>camera</phrase> is used, we propose a robust approach to establish correspondences between the cameras and compute an accurate disparity map. to conduct experiments, a <phrase>ground truth</phrase> was first reconstructed from a <phrase>high</phrase> quality acquisition. various degradations were applied to the pattern images which were then solved using our method. the <phrase>results</phrase> were compared to the <phrase>ground truth</phrase> for error analysis and showed very good performances, even near depth discontinuities.
the modelcamera: a hand-held device for interactive modeling.
multi-projectors for arbitrary surfaces without explicit calibration nor reconstructio.
<phrase>virtual environments</phrase> for critical intervention support: modeling, <phrase>design</phrase> and implementation issues.
cramer-rao bounds for nonparametric <phrase>surface reconstruction</phrase> from <phrase>range</phrase> <phrase>data</phrase>.
tolerance control with <phrase>high</phrase> resolution 3d measurement.
<phrase>anisotropic</phrase> <phrase>diffusion</phrase> of <phrase>surface normals</phrase> for feature preserving <phrase>surface reconstruction</phrase>.
<phrase>cad</phrase>-based <phrase>range</phrase> <phrase>sensor</phrase> placement for optimum 3d <phrase>data</phrase> acquisitio.
a flexible 3d modeling system based on combining <phrase>shape-from-silhouette</phrase> with <phrase>light</phrase>-sectioning <phrase>algorithm</phrase>.
in this <phrase>paper</phrase> we present a flexible modeling system for obtaining the texture-mapped 3d geometric <phrase>model</phrase>. the modeling system uses an <phrase>algorithm</phrase> combining <phrase>shape-from-silhouette</phrase> with <phrase>light</phrase>-sectioning. in the <phrase>algorithm</phrase>, at first, a rough shape <phrase>model</phrase> is obtained by <phrase>shape-from-silhouette</phrase> method almost automatically. next, concavities and complex parts on the object surface are obtained by <phrase>light</phrase>-sectioning method with manual scanning. for applying <phrase>light</phrase>-sectioning method to volume <phrase>data</phrase>, we propose volumetric <phrase>light</phrase>-sectioning <phrase>algorithm</phrase>. then our modeling system can realize easy and accurate generation of 3d geometric <phrase>model</phrase>.
multiview registration for large <phrase>data</phrase> sets.
<phrase>physics</phrase>-based models for <phrase>image analysis</phrase>/synthesis and geometric <phrase>design</phrase>.
this <phrase>paper</phrase> reviews recently developed <phrase>physics</phrase>-based surface modeling techniques for geometric <phrase>design</phrase>, <phrase>medical</phrase> <phrase>image analysis</phrase>, and <phrase>human</phrase> facial modeling. it briefly motivates the problems of interest in each application <phrase>area</phrase>, describes the models that the authors have developed to address them, presents sample <phrase>results</phrase>, and provides references to technical papers containing the full details.
robust meshes from <phrase>multiple range</phrase> maps.
this <phrase>paper</phrase> presents a method for modeling the surface of an object from a <phrase>sequence</phrase> of <phrase>range</phrase> maps. our <phrase>method is based</phrase> on a volumetric approach that produces a compact surface without boundary. it provides robustness through the use of interval analysis techniques and computational efficiency through hierarchical processing using octrees.
determining characteristic views of a 3d object by visual hulls and <phrase>hausdorff</phrase> distance.
nowadays, with the exponential growing of 3d object representations in <phrase>private</phrase> <phrase>databases</phrase> or on the web, it is all the more required to match these objects from some views. to improve the <phrase>results</phrase> of their matching, we work on the characteristic views of an object. the aim of this study is to find how many characteristic views are required and what relative positions are optimal. this is the reason why the visual hulls are used. from some 2d masks, the nearest possible 3d mesh from the original object is computed. <phrase>opengl</phrase> views are used to build the visual hulls of 3d models from a given collection and then the distance between the visual hulls and the models are measured thanks to the <phrase>hausdorff</phrase> distance. then the best view parameters are deduced to reduce the distance. these shots show that three orthogonal views give <phrase>results</phrase> very close to the ones given by twelve views on a isocahedron. some other <phrase>results</phrase> on the view resolution and the field of view are discussed.
<phrase>projective</phrase> surface matching of colored 3d scans.
we present a new method for registering multiple 3d scans of a colored object. each scan is regarded as a color and <phrase>range</phrase> image of the object recorded by a <phrase>pinhole camera</phrase>. consider a pair of cameras that see overlapping parts of the objects. for correct <phrase>camera</phrase> poses, the actual image of the overlap <phrase>area</phrase> in one <phrase>camera</phrase> matches the rendition of the overlap <phrase>area</phrase> as seen by the other <phrase>camera</phrase>. we define a mismatch score summarizing discrepancies in color, <phrase>range</phrase>, and <phrase>silhouette</phrase> between pairs of images, and we present an <phrase>algorithm</phrase> to efficiently minimize this mismatch score over <phrase>camera</phrase> poses.
3d modeling of <phrase>archaeological</phrase> vessels using <phrase>shape from silhouette</phrase>.
adaptive enhancement of 3d scenes using hierarchical registration of texture-mapped 3d models.
relighting acquired models of outdoor scenes.
in this <phrase>paper</phrase> we introduce a relighting <phrase>algorithm</phrase> for diffuse outdoor scenes that enables us to create geometrically correct and illumination consistent models from a series of <phrase>range</phrase> scans and a set of overlapping photographs that have been taken under different illumination conditions. to perform the relighting we compute a set of mappings from the overlap <phrase>region</phrase> of two images. we call these mappings <phrase>irradiance</phrase> ratio maps (irms). our <phrase>algorithm</phrase> handles cast shadows, being able to relight shadowed regions into non-shadowed regions and vice-versa. we solve these cases by <phrase>computing</phrase> four different irms, to handle all four combinations of shadowed vs. non-shadowed surfaces. to relight the non-overlapping <phrase>region</phrase> of an image, we look into the appropriate irm which we index on <phrase>surface normal</phrase>, and apply its value to the corresponding <phrase>pixels</phrase>. the result is an illumination consistent set of images.
approaches to a color scannerless <phrase>range</phrase> imaging system.
efficient <phrase>surface reconstruction</phrase> from <phrase>range</phrase> curves.
3-d modeling from <phrase>range</phrase> imagery: an incremental method with a planning component.
in this <phrase>paper</phrase> we present a method for automatically constructing a <phrase>cad</phrase> <phrase>model</phrase> of an unknown object from <phrase>range</phrase> images. the method is an incremental one that interleaves a sensing operation that acquires and merges <phrase>information</phrase> into the <phrase>model</phrase> with a planning phase to determine the next <phrase>sensor</phrase> position or "view". this is accomplished by integrating a system for 3-d <phrase>model</phrase> acquisition with a <phrase>sensor</phrase> planner. the <phrase>model</phrase> acquisition system provides facilities for <phrase>range</phrase> image acquisition, solid <phrase>model</phrase> <phrase>construction</phrase> and <phrase>model</phrase> merging: both mesh surface and solid representations are used to build a <phrase>model</phrase> of the <phrase>range</phrase> <phrase>data</phrase> from each view, which is then merged with the <phrase>model</phrase> built from previous sensing operations. the planning system utilizes the resulting incomplete <phrase>model</phrase> to plan the next sensing operation by finding a <phrase>sensor</phrase> viewpoint that will improve the fidelity of the <phrase>model</phrase>. <phrase>experimental</phrase> <phrase>results</phrase> are presented for a complex part that includes polygonal faces, curved surfaces, and large self-occlusions.
virtual <phrase>clay</phrase> modeling system using multi-viewpoint images.
this <phrase>paper</phrase> proposes a "non-contact virtual <phrase>clay</phrase> modeling system." we developed a <phrase>prototype</phrase> of <phrase>three-dimensional</phrase> modeling system that allows users to shape "virtual <phrase>clay</phrase>" with their hand movements. in our proposed method, the users hand movements are observed with multiple cameras to estimate their positions. the <phrase>human</phrase> hand surface and virtual <phrase>clay</phrase> are modeled by using subdivision surface. using these estimated hand positions, virtual <phrase>clay</phrase> is shaped based on a direct <phrase>free</phrase>-form deformation technique. to improve processing speed, we implemented the proposed system on a <phrase>pc</phrase> cluster. this system proves the feasibility of an intuitive virtual <phrase>clay</phrase> modeling system.
<phrase>human</phrase> figure <phrase>reconstruction</phrase> and modeling from <phrase>single</phrase> image or <phrase>monocular</phrase> <phrase>video</phrase> <phrase>sequence</phrase>.
generating smooth surfaces with bicubic splines over triangular meshes: toward automatic <phrase>model</phrase> building from unorganized 3d points.
3d models from extended uncalibrated <phrase>video</phrase> sequences: addressing <phrase>key-frame</phrase> selection and <phrase>projective</phrase> drift.
in this <phrase>paper</phrase>; we present an approach that is able to reconstruct 3d models from extended <phrase>video</phrase> sequences captured with an uncalibrated <phrase>hand-held camera</phrase>. we focus on two specific issues: (1) <phrase>key-frame</phrase> selection, and ( 2 ) <phrase>projective</phrase> drift. given a <phrase>long</phrase> <phrase>video</phrase> <phrase>sequence</phrase> it is often not practical to work with all videoframes. in addition, to allow for effective <phrase>outlier</phrase> rejection and <phrase>motion estimation</phrase> it is necessary to have a sufficient baseline between frames. for this purpose, we propose a <phrase>key-frame</phrase> selection procedure based on a robust <phrase>model selection</phrase> criterion. our approach guarantees that the <phrase>camera</phrase> motion can be estimated reliably by analyzing the feature correspondences between three consecutive views. another problem for <phrase>long</phrase> uncalibrated <phrase>video</phrase> sequences is <phrase>projective</phrase> drift. error accumulation leads to a non-<phrase>projective</phrase> <phrase>distortion</phrase> of the <phrase>model</phrase>. this causes the <phrase>projective</phrase> basis at the beginning and the end of the <phrase>sequence</phrase> to become inconsistent and leads to the failure of self-calibration. we propose a self-calibration approach that is insensitive to this global <phrase>projective</phrase> drift. afterself-calibration triplets of key-frames are aligned using absolute orientation and hierarchically merged into a complete metric <phrase>reconstruction</phrase>. next, we compute a detailed 3d surface <phrase>model</phrase> using <phrase>stereo</phrase> matching. the 3d <phrase>model</phrase> is textured using some of the frames.
correction of color <phrase>information</phrase> of a 3d <phrase>model</phrase> using a <phrase>range</phrase> intensity image.
a <phrase>range</phrase> intensity image, which is also called a <phrase>reflectance</phrase> image, refers to the intensity image that is acquired simultaneously with the <phrase>range</phrase> image captured using an active <phrase>range</phrase> <phrase>sensor</phrase>. this <phrase>paper</phrase> proposes a method that uses this image to correct the color <phrase>information</phrase> of a textured 3d <phrase>model</phrase>. the color <phrase>information</phrase> is usually obtained by <phrase>texture mapping</phrase> of <phrase>color images</phrase> acquired by a <phrase>digital camera</phrase>. the lighting condition for the <phrase>color images</phrase> are usually not controlled, thus the color <phrase>information</phrase> is not precise. on the other hand, the lighting condition for the <phrase>range</phrase> intensity image is controlled since it is obtained from a controlled and known lighting as required for the purpose of <phrase>range</phrase> measurement. the <phrase>paper</phrase> describes the method for combining the two sources of <phrase>information</phrase>; experiments show the effectiveness of the correction method.
modeling structured environments by a <phrase>single</phrase> moving <phrase>camera</phrase>.
combining off- and on-line calibration of a <phrase>digital camera</phrase>.
shape recovery and analysis of large screw threads.
stable real-time interaction between virtual humans and real scenes .
the <phrase>caesar</phrase> project: a 3-d surface <phrase>anthropometry</phrase> survey.
surflet-pair-relation histograms: a statistical 3d-shape representation for rapid classification.
3d shape recovery and registration based on the projection of non coherent <phrase>structured light</phrase>.
efficient <phrase>reconstruction</phrase> of indoor scenes with color.
<phrase>computing</phrase> <phrase>camera</phrase> positions from a <phrase>multi-camera</phrase> head.
shape <phrase>reconstruction</phrase> of <phrase>human</phrase> foot from <phrase>multi-camera</phrase> images based on <phrase>pca</phrase> of <phrase>human</phrase> shape <phrase>database</phrase>.
recently, researches and developments for measuring and modeling of <phrase>human</phrase> body are taking much attention. our aim is to capture accurate shape of <phrase>human</phrase> foot, using 2d images acquired by multiple cameras, which can capture dynamic behavior of the object. in this <phrase>paper</phrase>, 3d active shape models is used for accurate <phrase>reconstruction</phrase> of surface shape of <phrase>human</phrase> foot. we apply <phrase>principal component analysis</phrase> (<phrase>pca</phrase>) of <phrase>human</phrase> shape <phrase>database</phrase>, so that we can represent humans foot shape by approximately 12 principal component shapes. because of the reduction of dimensions for representing the object shape, we can efficiently recover the object shape from <phrase>multi-camera</phrase> images, even though the object shape is partially occluded in some of input views. to demonstrate <phrase>the proposed method</phrase>, two kinds of experiments are presented: <phrase>high</phrase> accuracy <phrase>reconstruction</phrase> of <phrase>human</phrase> foot in a <phrase>virtual reality</phrase> environment with cg <phrase>multi-camera</phrase> images and in <phrase>real world</phrase> with eight <phrase>ccd</phrase> cameras. in those experiments, the recovered shape error with our method is around 2mm, while the error is around 4mm with volume intersection method.
registering two overlapping <phrase>range</phrase> images.
registration of 3-d partial surface models using <phrase>luminance</phrase> and depth <phrase>information</phrase>.
textured surface models of <phrase>three-dimensional</phrase> objects are gaining importance in computer graphics applications. these models often have to be merged from several overlapping partial models which have to be registered (i.e. the relative transformation between the partial models has to be determined) prior to the merging process. in this <phrase>paper</phrase> a method is presented that makes use of both <phrase>camera</phrase>-based depth <phrase>information</phrase> (e.g. from <phrase>stereo</phrase>) and the <phrase>luminance</phrase> image. the <phrase>luminance</phrase> <phrase>information</phrase> is exploited to determine corresponding point sets on the partial surfaces using an <phrase>optical flow</phrase> approach. <phrase>quaternions</phrase> are then employed to determine the transformation between the partial models which minimizes the sum of the 3-d euclidian distances between the corresponding point sets. in <phrase>order</phrase> to find corresponding points on the partial surfaces <phrase>luminance</phrase> <phrase>information</phrase> is linearized. the procedure is iterated until convergence is reached. in contrast to only using depth <phrase>information</phrase>, employing <phrase>luminance</phrase> speeds up convergence and reduces remaining degrees of freedom (e.g. when registering <phrase>sphere</phrase>-like shapes).
curve and surface models to drive 3d <phrase>reconstruction</phrase> using <phrase>stereo</phrase> and shading.
faithful recovering of <phrase>quadric</phrase> surfaces from 3d <phrase>range</phrase> <phrase>data</phrase>.
segmentation and modeling of approximately rotationally symmetric objects in 3d <phrase>ultrasound</phrase>.
indoor scene <phrase>reconstruction</phrase> from sets of noisy <phrase>range</phrase> images.
3d capture for computer graphics.
real-time <phrase>image based</phrase> rendering from uncalibrated images.
we present a novel real-time <phrase>image-based</phrase> rendering system for generating realistic novel views of complex scenes from a set of uncalibrated images. a combination ofstructure-and-motion and <phrase>stereo</phrase> techniques is used to obtain calibrated cameras and dense depth maps for all recorded images. these depth maps are converted into restrictive quadtrees, which allow for adaptive, view-dependent tessellations while storing per-vertex quality. when rendering a novel view, a <phrase>subset</phrase> of suitable cameras is selected based upon a ranking criterion. in the <phrase>spirit</phrase> of the unstructured lumigraph rendering approach a blending field is evaluated, although the implementation is adapted on several points. we alleviate the need for the creation of a geometric proxy for each novel view while the <phrase>camera</phrase> blending field is sampled in a more optimal, non-uniform way and combined with the per-vertex quality to reduce texture artifacts. in <phrase>order</phrase> to make real-time visualization possible, all critical steps of the visualization pipeline are programmed in a highly optimized way on <phrase>commodity</phrase> <phrase>graphics hardware</phrase> using the <phrase>opengl</phrase> shading <phrase>language</phrase>. the proposed system can handle complex scenes such as large outdoor scenes as well as small objects with a large number of acquired images.
<phrase>computing</phrase> consistent normals and colors from <phrase>photometric</phrase> <phrase>data</phrase>.
<phrase>model</phrase>-based scanning path generation for inspection.
<phrase>image-based</phrase> object <phrase>editing</phrase>.
enhanced, robust <phrase>genetic algorithms</phrase> for multiview <phrase>range</phrase> <phrase>image registration</phrase>.
efficient variants of the <phrase>icp</phrase> <phrase>algorithm</phrase>.
combining texture and shape for automatic crude patch registration.
on estimating the position of fragments on rotational symmetric <phrase>pottery</phrase>.
a discrete reeb <phrase>graph</phrase> approach for the segmentation of <phrase>human</phrase> body scans .
next view planning for a combination of passive and active acquisition techniques.
additional reviewers.
<phrase>curvature</phrase> estimation for segmentation of triangulated surfaces.
3-d modeling of <phrase>human</phrase> hand with motion constraints.
taking consensus of signed distance field for complementing unobservable surface.
progressive multilevel meshes from <phrase>octree</phrase> particles.
effective <phrase>nearest neighbor search</phrase> for aligning and merging <phrase>range</phrase> images.
<phrase>digitizing</phrase> <phrase>archaeological</phrase> excavations from <phrase>multiple views</phrase>.
we present a novel approach on <phrase>digitizing</phrase> <phrase>large scale</phrase> unstructured environments like <phrase>archaeological</phrase> excavations using off-the-shelf <phrase>digital</phrase> still cameras. the cameras are calibrated with respect to few markers captured by a <phrase>theodolite</phrase> system. having all cameras registered in the same coordinate system enables a volumetric approach. our new <phrase>algorithm</phrase> has as input multiple calibrated images and outputs an occupancy <phrase>voxel</phrase> space where occupied <phrase>pixels</phrase> have a local orientation and a confidence value. both, orientation and confidence facilitate an efficient rendering and <phrase>texture mapping</phrase> of the resulting <phrase>point cloud</phrase>. our <phrase>algorithm</phrase> combines the following new features: images are back-projected to hypothesized local patches in the world and correlated on these patches yielding the best orientation. adjacent cameras build tuples which yield a product of pairwise correlations, called strength. multiple <phrase>camera</phrase> tuples compete each other for the best strength for each <phrase>voxel</phrase>. a <phrase>voxel</phrase> is regarded as occupied if strength is maximum along the normal. unlike other <phrase>multi-camera</phrase> <phrase>algorithms</phrase> using silhouettes, photoconsistency, or global correspondence, our <phrase>algorithm</phrase> makes no assumption on <phrase>camera</phrase> locations being outside the <phrase>convex hull</phrase> of the scene. we present compelling <phrase>results</phrase> of outdoors excavation areas.
appearance-based virtual view generation of temporally-varying events from <phrase>multi-camera</phrase> images in the 3d room.
a mechanism for <phrase>range</phrase> image integration without <phrase>image registration</phrase>.
a mechanism is introduced that automatically integrates <phrase>multi-view</phrase> <phrase>range</phrase> images without registering the images. the mechanism is based on a reference double-frame that acts as the coordinate system of the scene. a <phrase>single</phrase>-view <phrase>range</phrase> image of a scene is obtained by sweeping a <phrase>laser</phrase> line over the scene by hand and analyzing the acquired <phrase>light</phrase> stripes. <phrase>range</phrase> images captured from different views of the scene will be in the coordinate system of the double-frame, and thus, will automatically integrate without further processing.
<phrase>three-dimensional</phrase> shape modeling with extended hyperquadrics.
dynamic gaze-controlled levels of detail of polygonal objects in 3-d environment modeling.
recovery of shape and surface <phrase>reflectance</phrase> of <phrase>specular</phrase> object from rotation of <phrase>light</phrase> source.
active modeling of 3-d objects: planning on the next best pose (nbp) for acquiring <phrase>range</phrase> images.
we propose a new method of creating a complete <phrase>model</phrase> of a curved object from <phrase>multiple range</phrase> images acquired by showing it at different poses. the pose of the object is changed by a manipulator in <phrase>order</phrase> to view the object from some specified viewpoints. the pose is planned after each new image is merged into a unified representation. a rating <phrase>function</phrase> for the planning is defined to take into consideration the factors such as possibility of merging new <phrase>data</phrase>, registration accuracy and control point selection.
shape measurement of discontinuous objects using projected fringes and temporal phase unwrapping.
temporal phase unwrapping is a method of analyzing fringe patterns in which the fringe phase, /<phrase>spl</phrase> phi/, at each <phrase>pixel</phrase> is measured and <phrase>unwrapped</phrase> as a <phrase>function</phrase> of time, t. we propose a method for improving the <phrase>signal-to-noise ratio</phrase> of the total <phrase>phase change</phrase> by incorporating <phrase>data</phrase> from the intermediate phase values. the performance of the method is compared theoretical and experimentally, using <phrase>data</phrase> from a fringe projector based on a spatial tight modulator. the best way to use the method is with /<phrase>spl</phrase> phi/ decreasing exponentially with time from its maximum value to zero; this provides significant improvements in reliability, accuracy and computation time compared with the original temporal unwrapping <phrase>algorithm</phrase>.
bootstrapped real-time ego <phrase>motion estimation</phrase> and scene modeling.
estimating the motion of a moving <phrase>camera</phrase> in an unknown environment is essential for a number of applications ranging from as-built <phrase>reconstruction</phrase> to <phrase>augmented reality</phrase>. it is a challenging problem especially when real-time performance is required. our approach is to estimate the <phrase>camera</phrase> motion while reconstructing the shape and appearance of the most salient visual features in the scene. in our 3d <phrase>reconstruction</phrase> process, correspondences are obtained by tracking the visual features from frame to frame with <phrase>optical flow</phrase> tracking. <phrase>optical-flow</phrase>-based tracking methods have limitations in tracking the salient features. often larger translational motions and even moderate rotational motions can result in drifts. we propose to augment flow-based tracking by building a landmark representation around reliably reconstructed features. a planar patch around the reconstructed feature point provides matching <phrase>information</phrase> that prevents drifts in flow-based feature tracking and allows establishment of correspondences across the frames with large baselines. selective and periodic such correspondence mappings drastically improve scene and motion <phrase>reconstruction</phrase> while adhering to the real-time requirements. the method is experimentally tested to be both accurate and computational efficient.
3-d imager for dimensional gauging of <phrase>industrial</phrase> workpieces: <phrase>state</phrase>-of-the-<phrase>art</phrase> of the development of a robust and versatile system.
<phrase>experimental</phrase> analysis of <phrase>harmonic</phrase> shape images.
fast <phrase>range</phrase> <phrase>image segmentation</phrase> by an <phrase>edge detection</phrase> strategy.
a physically-based <phrase>model</phrase> for real-time <phrase>facial expression</phrase> <phrase>animation</phrase>.
globally <phrase>convergent</phrase> <phrase>range</phrase> <phrase>image registration</phrase> by <phrase>graph</phrase> kernel <phrase>algorithm</phrase>.
automatic <phrase>range</phrase> <phrase>image registration</phrase> without any <phrase>knowledge</phrase> of the viewpoint requires identification of common regions across different <phrase>range</phrase> images and then establishing point correspondences in these regions. we formulate this as a <phrase>graph</phrase>-based <phrase>optimization problem</phrase>. more specifically, we define a <phrase>graph</phrase> in which each vertex represents a putative match of two points, each edge represents <phrase>binary</phrase> consistency decision between two matches, and each edge orientation represents match quality from worse to better putative match. then strict sub-kernel defined in the <phrase>graph</phrase> is maximized. the maximum strict sub-kernel <phrase>algorithm</phrase> enables us to uniquely determine the largest consistent matching of points. to evaluate the quality of a <phrase>single</phrase> match, we employ the <phrase>histogram</phrase> of triple <phrase>products</phrase> that are generated by all <phrase>surface normals</phrase> in a point <phrase>neighborhood</phrase>. our <phrase>experimental</phrase> <phrase>results</phrase> show the effectiveness of our method for rough <phrase>range</phrase> <phrase>image registration</phrase>.
robust recognition and pose determination of 3-d objects using <phrase>range</phrase> images in eigenspace.
the planar: a <phrase>mobile</phrase> vr tool with pragmatic <phrase>pose estimation</phrase> for generation and manipulation of 3d <phrase>data</phrase> in <phrase>industrial</phrase> environments.
we present the <phrase>prototype</phrase> of the planar, a novel <phrase>input/output</phrase> device designed for applications in task areas focusing on the generation and manipulation of 3d <phrase>data</phrase>, e.g. <phrase>cad</phrase> or styling. technically, the planar offers a spatially aware pen-sensitive display, mounted on an adjustable, scooter-like autonomous platform. the movable screen, with 6 degrees of freedom, can <phrase>act</phrase> like a window into 3d <phrase>virtual environments</phrase> and allows for efficient 2d and 3d interaction at the same time. we <phrase>report</phrase> on the <phrase>pose estimation</phrase> system of the planar's display where we focus on two enhanced optical mice combined to <phrase>track</phrase> horizontal 2d position/orientation. <phrase>results</phrase> of this pragmatic approach are presented and discussed. in <phrase>order</phrase> to demonstrate the potential of the planar we show a small review/sketching/annotation application. the overall goal of this work is to contribute to the development of real vr applications.
from <phrase>range</phrase> <phrase>data</phrase> to <phrase>animated</phrase> <phrase>anatomy</phrase>-based faces: a <phrase>model</phrase> adaptation method.
this <phrase>paper</phrase> presents a new method for reconstructing <phrase>animated</phrase>, <phrase>anatomy</phrase>-based facial models of individuals from <phrase>range</phrase> <phrase>data</phrase> with minimal manual intervention. a <phrase>prototype</phrase> <phrase>model</phrase> with a multi-layer <phrase>skin</phrase>-<phrase>muscle</phrase>-<phrase>skull</phrase> structure serves as the starting point for our method. after the global adaptation, the <phrase>skin</phrase> mesh of the <phrase>prototype</phrase> <phrase>model</phrase> is represented as a dynamic deformable <phrase>model</phrase> which is deformed to fit scanned <phrase>data</phrase> according to internal force stemming from the elastic properties of the surface and external forces <phrase>produced</phrase> from the scanned <phrase>data</phrase> points and features. the underlying <phrase>muscle</phrase> layer that consists of three types of facial muscles is automatically adapted. according to the adapted <phrase>skin</phrase> and <phrase>muscle</phrase> structures, a set of automatically generated <phrase>skull</phrase> <phrase>feature points</phrase> is transformed to drive a volume <phrase>morphing</phrase> of the template <phrase>skull</phrase> <phrase>model</phrase> for <phrase>skull</phrase> fitting. the reconstructed <phrase>model</phrase> realistically reproduces the shape and features of a specific person and can be <phrase>animated</phrase> instantly.
view planning with a registration constraint.
reconstructing <phrase>urban</phrase> 3d <phrase>model</phrase> using vehicle-borne <phrase>laser</phrase> <phrase>range</phrase> scanners.
discrete pose space estimation to improve <phrase>icp</phrase>-based tracking.
<phrase>iterative closest</phrase> point (<phrase>icp</phrase>) -based tracking works well when the interframe motion is within the <phrase>icp</phrase> minimum well space. for large interframe motions resulting from a limited <phrase>sensor</phrase> acquisition rate relative to the speed of the object motion, it suffers from slow convergence and a tendency to be stalled by local minima. a novel method is proposed to improve the performance of <phrase>icp</phrase>-based tracking. the <phrase>method is based</phrase> upon the bounded hough transform (bht) which estimates the object pose in a coarse discrete pose space. given an initial pose estimate, and assuming that the interframe motion is bounded in all 6 pose dimensions, the bht estimates the current frames pose. on its own, the bht is able to <phrase>track</phrase> an objects pose in sparse <phrase>range</phrase> <phrase>data</phrase> both efficiently and reliably, albeit with a limited precision. experiments on both simulated and real <phrase>data</phrase> show the bht to be more efficient than a number of variants of the <phrase>icp</phrase> for a similar <phrase>degree</phrase> of reliability. a <phrase>hybrid</phrase> method has also been implemented wherein at each frame the bht is followed by a few <phrase>icp</phrase> iterations. this <phrase>hybrid</phrase> method is more efficient than the <phrase>icp</phrase>, and is more reliable than either the bht or <phrase>icp</phrase> separately.
3d head <phrase>pose estimation</phrase> with <phrase>optical flow</phrase> and depth constraints.
efficient <phrase>photometric</phrase> <phrase>stereo</phrase> technique for <phrase>three-dimensional</phrase> surfaces with unknown brdf.
the present <phrase>paper</phrase> focuses on efficient inverse rendering using a <phrase>photometric</phrase> <phrase>stereo</phrase> technique for realistic surfaces. the technique primarily assumes the lambertian reflection <phrase>model</phrase> only. for non-lambertian surfaces, application of the technique to real surfaces in <phrase>order</phrase> to estimate 3-d shape and spatially varying <phrase>reflectance</phrase> from sparse images remains difficult. in the present <phrase>paper</phrase>, we propose a new <phrase>photometric</phrase> <phrase>stereo</phrase> technique by which to efficiently recover a full surface <phrase>model</phrase>, starting from a small set of photographs. the proposed technique allows diffuse <phrase>albedo</phrase> to vary arbitrarily over surfaces while non-diffuse characteristics remain constant for a material. specifically, the <phrase>basic</phrase> approach is to first recover the <phrase>specular</phrase> <phrase>reflectance</phrase> parameters of the surfaces by a novel optimization procedure. these parameters are then used to estimate the diffuse <phrase>reflectance</phrase> and <phrase>surface normal</phrase> for each point. as a result, a lighting-<phrase>independent</phrase> <phrase>model</phrase> of the <phrase>geometry</phrase> and <phrase>reflectance</phrase> properties of the surface is established using <phrase>the proposed method</phrase>, which can be used to re-render the images under novel lighting via traditional rendering methods.
extract and display moving object in all direction by using <phrase>stereo</phrase> <phrase>omnidirectional</phrase> system(<phrase>sos</phrase>).
estimation of 3-d pose and shape from a <phrase>monocular</phrase> image <phrase>sequence</phrase> and realtime <phrase>human</phrase> tracking.
<phrase>digital preservation</phrase> of ancient <phrase>cuneiform</phrase> tablets using 3d-scanning.
program committee.
optimized compression of <phrase>triangle</phrase> mesh <phrase>geometry</phrase> using prediction <phrase>trees</phrase>.
efficient interactive rendering of detailed models with hierarchical levels of detail.
recent acquisition systems, such as the one developed at the <phrase>university</phrase> of <phrase>california</phrase> at <phrase>berkeley</phrase>, are capable of collecting large, detailed, highly textured models that standard levels of detail (<phrase>lod</phrase>) rendering techniques [adaptive display <phrase>algorithm</phrase> for interactive frame rates during visualization of complex <phrase>virtual environments</phrase>] cannot handle efficiently. we propose an out-of-core rendering <phrase>engine</phrase> which applies the cost and benefit approach of the adaptive display <phrase>algorithm</phrase> by funkhouser and squin [adaptive display <phrase>algorithm</phrase> for interactive frame rates during visualization of complex <phrase>virtual environments</phrase>] to hierarchical levels of detail (hlods) [hlods for faster display of large static and dynamic environments]. unlike the adaptive display <phrase>algorithm</phrase>, we do not skip objects to maintain <phrase>interactivity</phrase> when many objects are visible. funkhouser and squin apply <phrase>hysteresis</phrase> by adding a penalty in the benefit heuristics to discourage disturbing <phrase>visual effects</phrase> due to fast switching of detail in the <phrase>model</phrase>. however, this penalty may not be sufficient if the user is moving around rapidly in the scene. instead, we have developed a more robust temporal <phrase>hysteresis</phrase> by retaining how much detail is rendered over a time <phrase>period</phrase>. we have implemented our rendering <phrase>engine</phrase> to run on a common personal computer with a standard <phrase>graphics card</phrase>. the <phrase>engine</phrase> is capable of visualizing, in both walk-through and <phrase>fly</phrase>-through mode, a detailed <phrase>model</phrase> of 25 <phrase>city</phrase> blocks comprised of 7 million <phrase>triangles</phrase> and 720 million color <phrase>pixels</phrase>. our <phrase>engine</phrase> maintains a constant <phrase>frame rate</phrase> and limits excessive flickering simultaneously.
<phrase>network protocol</phrase> for i teraction and scalable distributed visualization.
disordered patterns projection for 3d motion recovering.
in this <phrase>paper</phrase> we present a new color <phrase>structured light</phrase> technique based on a disordered codeword strategy. the aim of this method is to recover 3d <phrase>information</phrase> in moving scenes in such a way that the correspondence problem is easily and robustly solved. with this goal in mind a six-connectivity <phrase>topology</phrase> has been introduced in our pattern and color features have been inserted on it. repetition and disorder are allowed in the codeword which implies that the <phrase>hamming distance</phrase> between contiguous codewords increases. as a consequence of that, code loss circumstances can be efficiently handled. additionally, computational cost in the code recovering phase is highly reduced since codewords are defined as sets. a <phrase>structured light</phrase> projection system has been built in our lab and a wide <phrase>test</phrase> under real moving conditions has been carried out. this experimentation has been performed on medium resolution and for slow movements specifications giving promising <phrase>results</phrase>.
object classification by functional parts.
<phrase>reconstruction</phrase> of spherical representation models from multiple partial models.
automated <phrase>texture mapping</phrase> of 3d <phrase>city</phrase> models with oblique aerial imagery.
this <phrase>paper</phrase> describes an approach to <phrase>texture mapping</phrase> a 3d <phrase>city</phrase> <phrase>model</phrase> obtained from aerialand ground-based <phrase>laser</phrase> scans with oblique aerial imagery. first, the images are automatically registered by matching 2d image lines with projections of 3d lines from the <phrase>city</phrase> <phrase>model</phrase>. then, for each <phrase>triangle</phrase> in the <phrase>model</phrase>, the optimal image is selected by taking into account occlusion, <phrase>image resolution</phrase>, <phrase>surface normal</phrase> orientation, and coherence with neighboring <phrase>triangles</phrase>. finally, the utilized texture patches from all images are combined into one texture atlas for compact representation and efficient rendering. we evaluate our approach on a <phrase>data set</phrase> of <phrase>downtown</phrase> <phrase>berkeley</phrase>.
<phrase>human</phrase> motion: modeling and recognition of actions and interactions.
processing of <phrase>image sequences</phrase> has progressed from simple <phrase>structure from motion</phrase> <phrase>paradigm</phrase> to the recognition of actions / interactions as events. understanding <phrase>human</phrase> activities in <phrase>video</phrase> has many potential applications including automated <phrase>surveillance</phrase>, <phrase>video</phrase> archival/retrieval, <phrase>medical diagnosis</phrase>, <phrase>sports</phrase> analysis, and <phrase>human</phrase>-computer interaction. understanding <phrase>human</phrase> activities involves various steps of low-level vision processing such as segmentation, tracking, pose recovery, and trajectory estimation as well as <phrase>high</phrase>-level processing tasks such as body modeling and representation of <phrase>action</phrase>. while low-level processing has been actively studied, <phrase>high</phrase>-level processing is just beginning to receive attention. this is partly because <phrase>high</phrase>-level processing depends on the <phrase>results</phrase> of low-level processing. however, <phrase>high</phrase>-level processing also requires some <phrase>independent</phrase> and additional approaches and methodologies. in this <phrase>paper</phrase>, we focus on the following aspects of <phrase>high</phrase>-level processing: (1) <phrase>human</phrase> body modeling, (2) level of detail needed to understand <phrase>human</phrase> actions, (3) approaches to <phrase>human</phrase> <phrase>action</phrase> recognition, and (4) <phrase>high</phrase>-level recognition schemes with domain <phrase>knowledge</phrase>. the review is illustrated by examples of each of the areas discussed, including recent developments in our work on understanding <phrase>human</phrase> activities.
<phrase>data processing</phrase> <phrase>algorithms</phrase> for generating textured 3d building faade meshes from <phrase>laser</phrase> scans and <phrase>camera</phrase> images.
linear shift-invariant operators for processing surface meshes.
shift-invariant operators for surface meshes are defined using geometric realizations of the mesh. then, shift-invariance essentially means <phrase>isotropy</phrase> w.r.t. a distance metric. the particular case of the so-defined <phrase>lsi</phrase> operators with small support is analyzed in detail, showing a connection to mean value coordinates. the <phrase>topological</phrase> <phrase>laplacian</phrase> operator turns out to be the <phrase>lsi</phrase> operator of the <phrase>topological</phrase> realization of the mesh. more generally, assuming different geometric realizations or metrics allows interpreting various mesh processing techniques as <phrase>lsi</phrase> operators.
combining fringe projection method of 3d object monitoring with <phrase>virtual reality</phrase> environment: concept and initial <phrase>results</phrase>.
seeing into the past: creating a 3d modeling pipeline for <phrase>archaeological</phrase> visualization.
<phrase>archaeology</phrase> is a destructive process in which accurate and detailed <phrase>recording</phrase> of a site is imperative. as a site is exposed, documentation is required in <phrase>order</phrase> to recreate and understand the site in context. we have developed a 3d modeling pipeline that can assist <phrase>archaeologists</phrase> in the documentation effort by building rich, geometrically and photometrically accurate 3d models of the site. the modeling effort begins with <phrase>data acquisition</phrase> (images, <phrase>range</phrase> scans, <phrase>gis</phrase> <phrase>data</phrase>, and <phrase>video</phrase>) and ends with the use of a sophisticated visualization tool that can be used by researchers to explore and understand the site. the pipeline includes new methods for shadow-based registration of 2d images and temporal change detection. our multimodal <phrase>augmented reality</phrase> system allows users wearing head-tracked, see-through, head-worn displays to visualize the site <phrase>model</phrase> and associated <phrase>archaeological</phrase> artifacts, and to interact with them using speech and gesture.
real time visualization of 3d <phrase>variable</phrase> in time object based on <phrase>cloud</phrase> of points <phrase>data</phrase> gathered by <phrase>coloured</phrase> structure <phrase>light</phrase> projection system.
the problem of virtual view creation has received increasing attention in recent years. <phrase>major</phrase> current approaches are based on modified <phrase>stereo</phrase> vision systems. recently the structure <phrase>light</phrase> measurement system based on <phrase>digital</phrase> <phrase>light</phrase> projection supported by special <phrase>data</phrase> coding and processing allow to rapid 3d shape acquisition. application of this <phrase>technology</phrase> to record 3d <phrase>data</phrase> has increased significantly the accuracy of reconstructed shape and simplified <phrase>data</phrase> manipulation process. in the <phrase>paper</phrase> the <phrase>general</phrase> concept of <phrase>virtual reality</phrase> system supported by <phrase>data</phrase> gathered by means of structure <phrase>light</phrase> projection is presented. the methodology of conversion of <phrase>cloud</phrase> of measurement points (x,y,z,r,g,b) into <phrase>virtual reality</phrase> environment is described. it is supported by implementation of a virtual <phrase>camera</phrase> concept, as the mean for interactive object visualization. the methodology of real time 3d object visualization based on its coding by means of specially formed contours and their <phrase>b-spline</phrase> approximation is presented. the applicability of the methodology has been shown on numerically generated <phrase>data</phrase> which simulate performance of the measurement system. the total processing path was successfully tested.
<phrase>frequency domain</phrase> registration of computer tomography <phrase>data</phrase>.
this <phrase>paper</phrase> presents a new method for registering computer tomography (ct) volumetric <phrase>data</phrase> of <phrase>human</phrase> <phrase>bone</phrase> structures relative to observations made at different times. the system we advance was tested with different kinds of ct <phrase>data</phrase> sets. in this <phrase>paper</phrase> we <phrase>report</phrase> on some representative <phrase>experimental</phrase> <phrase>results</phrase> obtained with the ct <phrase>data</phrase> of the hip <phrase>bones</phrase> of a patient prior to and after prosthetic <phrase>surgery</phrase> aimed at the <phrase>reconstruction</phrase> of the hip articulation. the method works with rigid <phrase>data</phrase> having arbitrary relative position and orientation and proves to be robust with respect to ct acquisition noise and with respect to the segmentation technique adopted to select the <phrase>region</phrase> of interest for registration. the method is capable of registering correctly <phrase>data</phrase> sets taken from the same articulation and whose components have undergone small relative displacements. the method is also amenable to registration of heteregeneous kinds of volumetric <phrase>data</phrase>, e.g., <phrase>ct scans</phrase> and <phrase>magnetic resonance</phrase> imagery (<phrase>mri</phrase>) scans, which show different characteristics in correspondence to the same <phrase>organic</phrase> structure.
recognition of object contours from <phrase>stereo</phrase> images: an edge combination approach.
in this <phrase>paper</phrase>, we present an <phrase>algorithm</phrase> to combine edge <phrase>information</phrase> from <phrase>stereo</phrase>-derived depth maps with edges from the original intensity/color image to improve the contour detection in images of natural scenes. after <phrase>computing</phrase> the disparity map, we generate a so-called "edge combination image", which relies on those edges of the original image that are also present in the <phrase>stereo</phrase> map. we describe an <phrase>algorithm</phrase> to identify corresponding intensity and depth edges, which are usually slightly displaced due to non-perfect <phrase>stereo</phrase> <phrase>reconstruction</phrase>. our experiments demonstrate how the proposed edge combination approach can be used in conjunction with an active contours <phrase>algorithm</phrase> to achieve better segmentation <phrase>results</phrase>.
enhanced real-time <phrase>stereo</phrase> using bilateral filtering.
in recent years, there have been significant strides in increasing quality of <phrase>range</phrase> from <phrase>stereo</phrase> using global techniques such as <phrase>energy</phrase> minimization. these methods cannot yet achieve real-time performance. however, the need to improve <phrase>range</phrase> quality for real-time applications persists. all real-time <phrase>stereo</phrase> implementations rely on a simple correlation step which employs some local similarity metric between the left and right image. typically, the correlation takes place on an image pair modified in some way to compensate for <phrase>photometric</phrase> variations between the left and right cameras. improvements and modifications to such <phrase>algorithms</phrase> tend to fall into one of two broad categories: those which address the correlation step itself (e.g., shiftable <phrase>windows</phrase>, adaptive <phrase>windows</phrase>) and those which address the pre-processing of input imagery (e.g. <phrase>band</phrase>-<phrase>pass</phrase> filtering, rank, <phrase>census</phrase>). our efforts lie in the latter <phrase>area</phrase>. we present in this <phrase>paper</phrase> a modification of the standard <phrase>band</phrase>-<phrase>pass</phrase> filtering technique used by many <phrase>ssd</phrase>- and sad-based correlation <phrase>algorithms</phrase>. by using the bilateral filter of tomasi and manduchi [bilateral filtering for gray and <phrase>color images</phrase>], we minimize blurring at the filtering stage. we show that in conjunction with sad correlation, our new method improves <phrase>stereo</phrase> quality at <phrase>range</phrase> discontinuities while maintaining real-time performance.
fast interpolated cameras by combining a <phrase>gpu</phrase> based plane sweep with a max-flow regularisation <phrase>algorithm</phrase>.
the <phrase>paper</phrase> presents a method for the <phrase>high</phrase> speed calculation of crude depth maps. performance and applicability are illustrated for view <phrase>interpolation</phrase> based on two input <phrase>video</phrase> streams, but the <phrase>algorithm</phrase> is perfectly amenable to <phrase>multi-camera</phrase> environments. first a fast plane sweep <phrase>algorithm</phrase> generates the crude <phrase>depth map</phrase>. speed <phrase>results</phrase> from hardware accelerated transformations and <phrase>parallel processing</phrase> available on the <phrase>gpu</phrase>. all computations on the <phrase>graphical</phrase> board are performed <phrase>pixel</phrase>-wise and a <phrase>single</phrase> <phrase>pass</phrase> of the sweep only processes one input resolution. a second step uses a min-cut/max-flow <phrase>algorithm</phrase> to ameliorate the previous result. the <phrase>depth map</phrase>, a noisy interpolated image and correlation measures are available on the <phrase>gpu</phrase>. they are reused and combined with spatial connectivity <phrase>information</phrase> and temporal continuity considerations in a <phrase>graph</phrase> formulation. position dependent sampling densities allow the system to use multiple image resolutions. the min-cut separation of this <phrase>graph</phrase> yields the global minimum of the associated <phrase>energy</phrase> <phrase>function</phrase>. limiting the search <phrase>range</phrase> according to the initialisation provided by the plane sweep further speeds up the process. the required hardware are only two cameras and a regular <phrase>pc</phrase>.
a <phrase>bayesian</phrase> framework for 3d models retrieval based on characteristic views.
the <phrase>management</phrase> of big <phrase>databases</phrase> of <phrase>three-dimensional</phrase> models (used in <phrase>cad</phrase> applications, visualization, <phrase>games</phrase>, etc.) is a very important domain. the ability to characterize and easily retrieve models is a key issue for the designers and the final users. in this frame, two main approaches exist: search by example of a <phrase>three-dimensional</phrase> <phrase>model</phrase>, and search by a 2d view. in this <phrase>paper</phrase>, we present a novel framework for the characterization of a 3d <phrase>model</phrase> by a set of views (called characteristic views), and an indexing process of these models with a <phrase>bayesian</phrase> probabilistic approach using the characteristic views. the framework is <phrase>independent</phrase> from the descriptor used for the indexing. we illustrate our <phrase>results</phrase> using different descriptors on a collection of threedimensional models supplied by <phrase>renault</phrase> group.
3d <phrase>model</phrase> watermarking for indexing using the generalized <phrase>radon transform</phrase>.
the present <phrase>paper</phrase> proposes a novel method for 3d-<phrase>model</phrase> watermarking for indexing. the proposed approach is based on the use of a generalized <phrase>radon</phrase> transformation. more specifically, the cylindrical integration transform (cit) is initially applied to the 3d models in <phrase>order</phrase> to produce descriptor vectors. at the same time a watermarking technique, based on cit is used in <phrase>order</phrase> to embed a specific <phrase>model</phrase> identifier in the nodes of the 3d <phrase>model</phrase>. this identifier links the <phrase>model</phrase> to its descriptor <phrase>vector</phrase>, which is extracted only once and stored in a <phrase>database</phrase>. every time this <phrase>model</phrase> is employed as a query <phrase>model</phrase>, <phrase>watermark</phrase> detection is used so as to retrieve the corresponding identifier and further the descriptor <phrase>vector</phrase>, which can be further used in a matching <phrase>algorithm</phrase>. the proposed techniques are evaluated experimentally in terms of both watermarking efficiency and <phrase>content-based</phrase> retrieval performance.
a <phrase>structure-from-motion</phrase> method: use of motion in <phrase>three-dimensional</phrase> <phrase>reconstruction</phrase> of <phrase>moving objects</phrase> from <phrase>multiple-view</phrase> <phrase>image sequences</phrase>.
solving the correspondence problem is the most essential task for multiview <phrase>reconstruction</phrase> techniques, yet finding unique correspondences between <phrase>multiple views</phrase> is impossible at some points, due to such problems as occlusions and ambiguities. we have developed a <phrase>closed-form</phrase> <phrase>solution</phrase> through constructive <phrase>geometry</phrase> for a special case of the <phrase>structure-from-motion</phrase> (sfm) problem with four rigidly moving points. this <phrase>solution</phrase> allows the 3-d position of a point on a moving object to be computed without having to find the correspondence between its projections on the image planes of <phrase>multiple views</phrase>, given its projected 2-d motion <phrase>vector</phrase> on an image plane and 3-d <phrase>information</phrase> of three other points. with this method we do not have to depend entirely on <phrase>stereo</phrase>/multiview feature correspondences in reconstructing 3-d objects, hence easing those problems caused by occlusions and ambiguities.
3d <phrase>data acquisition</phrase> and elaboration for classification and recognition of objects and people.
distributed quantitative evaluation of 3d patient specific arterial models.
new imaging frontiers: 3d and <phrase>mixed reality</phrase>.
projection <phrase>model</phrase>, 3d <phrase>reconstruction</phrase> and rigid <phrase>motion estimation</phrase> from non-central catadioptric images.
this <phrase>paper</phrase> addresses the problem of rigid <phrase>motion estimation</phrase> and 3d <phrase>reconstruction</phrase> in vision systems where it is possible to recover the incident <phrase>light</phrase> ray direction from the image points. such systems include <phrase>pinhole</phrase> cameras and catadioptric cameras. given two images of the same scene acquired from two different positions, the transformation is estimated by means of an iterative process. the estimation process aims at having corresponding incident <phrase>rays</phrase> intersecting at the same 3d point. geometrical relationships are derived to support the estimation method. furthermore, this <phrase>paper</phrase> also addresses the problem of the mapping from 3d points to image points, for non-central catadioptric cameras with <phrase>mirror</phrase> surfaces given by quadrics. the projection <phrase>model</phrase> presented can be expressed in a non-<phrase>linear equation</phrase> of only one <phrase>variable</phrase>, being more stable and easier to solve than the <phrase>classical</phrase> <phrase>snell's law</phrase>. experiments with real images are presented, by using <phrase>simulated annealing</phrase> as estimation method.
scalable and efficient coding of 3d <phrase>model</phrase> extracted from a <phrase>video</phrase>.
this <phrase>paper</phrase> presents an efficient and scalable coding scheme for transmitting a <phrase>stream</phrase> of 3d models extracted from a video.as in <phrase>classical</phrase> <phrase>model</phrase>-based <phrase>video</phrase> coding, the <phrase>geometry</phrase>, connectivity, and texture of the 3d models have to be transmitted, as well as the <phrase>camera</phrase> position for each frame in the original <phrase>video</phrase>. <phrase>the proposed method</phrase> is based on exploiting the interrelations existing between each type of <phrase>information</phrase>, instead of coding them independently, allowing a better prediction of the next 3d <phrase>model</phrase> in the <phrase>stream</phrase>. <phrase>scalability</phrase> is achieved through the use of <phrase>wavelet</phrase>-based representations for both texture and <phrase>geometry</phrase> of the models. a consistent connectivity is built for all 3d models extracted from the <phrase>video</phrase> <phrase>sequence</phrase>, which allows a more compact representation and straightforward geometric <phrase>morphing</phrase> between successive models. furthermore this leads to a consistent <phrase>wavelet</phrase> decomposition for 3d models in the <phrase>stream</phrase>. quantitative and qualitative <phrase>results</phrase> for the proposed scheme are compared with the <phrase>state</phrase> of the <phrase>art</phrase> <phrase>video</phrase> coder h264, 3d <phrase>model</phrase>-based galpin coder and <phrase>independent</phrase> <phrase>mpeg4</phrase>-based coding of the <phrase>information</phrase>. targeted applications include distant visualization of the original <phrase>video</phrase> at very low <phrase>bitrate</phrase> and interactive <phrase>navigation</phrase> in the extracted 3d scene on heterogeneous terminals.
modeling shapes and textures from images: new frontiers.
fusing multiple <phrase>color images</phrase> for texturing models.
a commonly encountered problem when creating 3d models of large real scenes is unnatural color texture <phrase>fusion</phrase>. due to variations in lighting and <phrase>camera</phrase> settings (both manual and automatic), captured color texture maps of the same structure can have very different colors. when fusing <phrase>multiple views</phrase> to create larger models, this color variation leads to a poor appearance with odd color tilings on homogeneous surfaces. this <phrase>paper</phrase> extends previous <phrase>research</phrase> on pairwise global <phrase>color correction</phrase> to multiple overlapping images. the central idea is to estimate a set of blending transformations that minimize the overall color discrepancy in the overlapping regions, thus spreading residual color errors, rather than letting them accumulate.
half-edge multi-<phrase>tessellation</phrase>: a compact representation for <phrase>multi-resolution</phrase> <phrase>tetrahedral</phrase> meshes.
a <phrase>prototype</phrase> of <phrase>video</phrase> see-through <phrase>mixed reality</phrase> interactive system.
<phrase>mixed reality</phrase> (mr), sometimes called enhanced <phrase>reality</phrase>, is a <phrase>variety</phrase> of <phrase>virtual environment</phrase> (ve) which explores various approaches to combine <phrase>natural environment</phrase> with immersive display <phrase>technology</phrase>. ve technologies completely immerse a user inside a synthetic environment. in contrast, mr system adds <phrase>electronic</phrase> <phrase>data</phrase> from a <phrase>cyberspace</phrase> on the physical space as a base, allows the users to see the <phrase>real world</phrase> with virtual objects superimposed upon. moreover, mr can assist the users interact with the virtual object in the more realistic environment. the objective of our <phrase>research</phrase> is to investigate the potential of mr technique on improving interaction between <phrase>human</phrase> and computer through developing a <phrase>video</phrase>-see through <phrase>mixed reality</phrase> system. further applications will be built upon this generic platform.
novel diffractive optical elements and <phrase>algorithms</phrase> for real-time 3d and <phrase>hyperspectral imaging</phrase>.
<phrase>geometry</phrase> processing - a personal perspective.
optimized spectral estimation methods for improved co orimetry with <phrase>laser</phrase> scanning systems.
using the expectation-maximization <phrase>algorithm</phrase> for depth estimation and segmentation of <phrase>multi-view</phrase> images.
3d <phrase>stereoscopic</phrase> image pairs by <phrase>depth-map</phrase> generation.
this <phrase>paper</phrase> presents a new unsupervised technique aimed to generate <phrase>stereoscopic</phrase> views estimating depth <phrase>information</phrase> from a <phrase>single</phrase> input image.using a <phrase>single</phrase> input image, vanishing lines/points are extracted using a few heuristics to generate an approximated depth map.the <phrase>depth map</phrase> is then used to generate <phrase>stereo</phrase> pairs.the overall method is well suited for real time application and works also on <phrase>cfa</phrase> (colour filtering array) <phrase>data</phrase> acquired by <phrase>consumer</phrase> imaging devices. <phrase>experimental</phrase> <phrase>results</phrase> on a large dataset are reported.
visualizing legacy stratigraphic <phrase>data</phrase> from <phrase>archaeological</phrase> handbooks.
<phrase>architecture</phrase> of a 3d - <phrase>simulation</phrase> environment for active vision systems and <phrase>mobile</phrase> <phrase>robots</phrase>.
multi-<phrase>stereo</phrase> 3d object <phrase>reconstruction</phrase>.
the <phrase>origami</phrase> project: advanced tools and techniques for <phrase>high</phrase>-end <phrase>mixing</phrase> and interaction between real and virtual content.
<phrase>stereo</phrase> image coder based on mrf analysis for disparity estimation and morphological encoding.
this <phrase>paper</phrase> presents a <phrase>stereoscopic</phrase> image coder based on the mrf <phrase>model</phrase> and map estimation of the disparity field. the mrf <phrase>model</phrase> minimizes the noise of the disparity compensation because it takes into account the residual <phrase>energy</phrase>, <phrase>smoothness</phrase> constraints and the occlusion field. the disparity compensation is formulated as a map-mrf problem in the spatial domain and the mrf field consists of the disparity <phrase>vector</phrase> and occlusion field, which is partitioned into three regions by an initial double-threshold setting. the map search is conducted in a block-based sense on one or two of the three regions, providing faster execution. the reference and the residual images are decomposed by a <phrase>discrete wavelet transform</phrase> and the transform coefficients are encoded by employing the morphological representation of <phrase>wavelet</phrase> coefficients <phrase>algorithm</phrase>. as a result of the morphological encoding, the reference and residual images together with the disparity <phrase>vector field</phrase> are transmitted in <phrase>partitions</phrase> lowering the total <phrase>entropy</phrase>. the <phrase>experimental</phrase> evaluation on <phrase>synthetic and real</phrase> images shows beneficial performance of the proposed <phrase>algorithm</phrase>.
remote machinery maintenance system with the use of <phrase>virtual reality</phrase>.
point samples for efficient 3d processing and content creation.
improvement of metric accuracy of <phrase>digital 3d</phrase> models through <phrase>digital</phrase> <phrase>photogrammetry</phrase>. a <phrase>case study</phrase>: donatello's maddalena.
how can we exploit typical <phrase>architectural</phrase> structures to improve <phrase>model</phrase> recovery?
an analysis of errors in feature-preserving mesh simplification based on edge contraction.
the <phrase>quadric</phrase> error metric (qem) [1] criterion has been widely applied in mesh simplification procedures. related criteria--such as the quasi-<phrase>covariance</phrase> error metric (qcem) [2], which may produce <phrase>superior</phrase> <phrase>results</phrase> to the qem--have also been reported in recent years. in this <phrase>paper</phrase>, the underlying reasons that criteria such as qem and qcem are so successful in mesh simplification processes are considered through analysis of error in the simplification process. focus is on the use of the qem and qcem criteria in edge contraction-based mesh simplification.
ibr-based compression for remote visualization.
analysis of <phrase>secondary</phrase> structure elements of <phrase>proteins</phrase> using <phrase>indexing techniques</phrase>.
real-time, accurate depth of field using <phrase>anisotropic</phrase> <phrase>diffusion</phrase> and programmable <phrase>graphics cards</phrase>.
computer graphics cameras lack the finite depth of field (dof) present in <phrase>real world</phrase> ones. this <phrase>results</phrase> in all objects being rendered sharp regardless of their depth, reducing the realism of the scene. on top of that, <phrase>real-world</phrase> dof provides a depth cue, that helps the <phrase>human</phrase> visual system decode the elements of a scene. several methods have been proposed to render images with finite dof, but these have always implied an important <phrase>trade</phrase>-off between speed and accuracy. in this <phrase>paper</phrase>, we introduce a novel <phrase>anisotropic</phrase> <phrase>diffusion</phrase> <phrase>partial differential equation</phrase> (<phrase>pde</phrase>) that is applied to the 2d image of the scene rendered with a pin-hole <phrase>camera</phrase>. in this <phrase>pde</phrase>, the amount of blurring on the 2d image depends on the depth <phrase>information</phrase> of the 3d scene, present in the z-buffer. this equation is well posed, has existence and uniqueness <phrase>results</phrase>, and it is a good approximation of the optical phenomenon, without the visual artifacts and depth inconsistencies present in other approaches. because both inputs to our <phrase>algorithm</phrase> are present at the <phrase>graphics card</phrase> at every moment, we can run the processing entirely in the <phrase>gpu</phrase>. this fact, coupled with the particular numerical scheme chosen for our <phrase>pde</phrase>, allows for <phrase>real-time rendering</phrase> using a programmable <phrase>graphics card</phrase>.
dynamically optimised 3d (<phrase>virtual reality</phrase>) <phrase>data transmission</phrase> for <phrase>mobile</phrase> devices.
nowadays the processing power of <phrase>mobile phones</phrase>, <phrase>smartphones</phrase> and pda's is increasing as well as the transmission bandwidth.nevertheless there is still the need to reduce the content and the need of processing the data.discussed will be proposals and solutions for dynamic reduction of the transmitted content.for that, device specific properties will be taken into account, as much as for the aim to reduce the need of processing power at the <phrase>client side</phrase> to be able to display the 3d (<phrase>virtual reality</phrase>) data.therefore, well known technologies e.g. <phrase>data compression</phrase> are combined with new developed ideas to reach the goal of adaptive content transmission.to achieve a device dependent reduction of processing power the <phrase>data</phrase> have to be pre processed at the server side or the server even has to take over functionality of weak <phrase>mobile</phrase> devices.
learning illumination models while tracking.
in this <phrase>paper</phrase> we present a method for estimation of 3d motion of a rigid object from a <phrase>video</phrase> <phrase>sequence</phrase>, while simultaneously learning the parameters of an illumination <phrase>model</phrase> that describe the lighting conditions under which the <phrase>video</phrase> was captured. this is achieved by alternately estimating motion and illumination parameters in a recently proposed <phrase>mathematical</phrase> framework for integrating the effects of motion, illumination and structure. the motion is represented in terms of <phrase>translation</phrase> and rotation of the object <phrase>centroid</phrase>, and the illumination is represented using a <phrase>spherical harmonics</phrase> linear basis. the method does not assume any <phrase>model</phrase> for the variation of the illumination conditions-lighting can change slowly or drastically, locally or globally. also, it can be composed of combinations of point and extended sources. for multiple cameras viewing an object, we derive a new <phrase>photometric</phrase> constraint that relates the illumination parameters in two or more <phrase>independent</phrase> <phrase>video</phrase> sequences. this constraint allows verification of the illumination parameters obtained from <phrase>multiple views</phrase> and synthesis of new views under the same lighting conditions. we demonstrate the effectiveness of our <phrase>algorithm</phrase> in tracking under severe changes of lighting conditions.
<phrase>venus</phrase> subsurface <phrase>ionosphere</phrase> <phrase>radar</phrase> <phrase>sounder</phrase>: vensis.
due to optically opaque <phrase>atmosphere</phrase>, <phrase>radar</phrase> is the best way to observe the surface of <phrase>venus</phrase> from <phrase>orbit</phrase>. <phrase>magellan</phrase> has obtained global sar imaging, as well as altimetry and <phrase>emissivity</phrase>. as a subsurface <phrase>sounder</phrase>, vensis would obtain fundamentally different kinds of geologic <phrase>information</phrase> than <phrase>magellan</phrase>. mapping of interfaces of geologic units (e.g. <phrase>tessera</phrase>, plains, <phrase>lava flows</phrase>, impact debris) could be extended into the third <phrase>dimension</phrase>. reflectivity variations recorded at the surface by <phrase>magellan</phrase> are likely to extend into subsurface, providing <phrase>dielectric</phrase> contrast at interfaces.
<phrase>human</phrase> <phrase>action</phrase> recognition by <phrase>sequence</phrase> of movelet codewords.
fast and robust bore detection in <phrase>range</phrase> image <phrase>data</phrase> for <phrase>industrial automation</phrase>.
this <phrase>paper</phrase> presents a fast and robust method to precisely segment and locate bore holes of 4 to 50mm <phrase>diameter</phrase>. the task is solved by a <phrase>robot</phrase> moving a compact <phrase>triangulation</phrase> scanning <phrase>sensor</phrase> to all sides of the object and scanning the bore holes. exploiting the <phrase>knowledge</phrase> about the expected bore <phrase>diameter</phrase> and bore pose makes it possible to develop highly robust <phrase>algorithms</phrase> for an <phrase>industrial</phrase> application. sparse <phrase>data</phrase> of the bore hole is sufficient to segment the bore <phrase>independent</phrase> of bore hole <phrase>chamfer</phrase> type using a robust normal <phrase>vector</phrase> fit and a classification based on the gaussian image. a sequential <phrase>algorithm</phrase> to fit the bore cylinder makes it possible to calculate the bore pose in less than 1 second. experiments demonstrate that 120 degrees of the bore hole surface are sufficient for robust localization within 0.3mm and 0.5 degrees even in the presence of <phrase>ghost</phrase> points and notches in the bore holes.
<phrase>helmholtz</phrase> <phrase>stereopsis</phrase> on rough and strongly textured surfaces.
<phrase>helmholtz</phrase> <phrase>stereopsis</phrase> (hs) has recently been explored as a promising technique for capturing shape of objects with unknown <phrase>reflectance</phrase>. so far, it has been widely applied to objects of smooth <phrase>geometry</phrase> and <phrase>piecewise</phrase> uniform bidirectional <phrase>reflectance</phrase> <phrase>distribution function</phrase> (brdf). moreover, for non-convex surfaces the inter-reflection effects have been completely neglected. we extend the method to surfaces which exhibit strong texture, nontrivial <phrase>geometry</phrase> and are possibly non-convex. the problem associated with these surface features is that <phrase>helmholtz</phrase> reciprocity is apparently violated when point-based measurements are used independently to establish the matching constraint as in the standard hs implementation. we argue that the problem is avoided by <phrase>computing</phrase> <phrase>radiance</phrase> measurements on image regions corresponding exactly to projections of the same surface point <phrase>neighbourhood</phrase> with appropriate scale. the <phrase>experimental</phrase> <phrase>results</phrase> demonstrate the success of the novel method proposed on real objects.
<phrase>ellipsoid</phrase> decomposition of 3d-<phrase>model</phrase>.
<phrase>construction</phrase> of <phrase>large-scale</phrase> <phrase>virtual environment</phrase> by fusing <phrase>range</phrase> <phrase>data</phrase>, texture images, and airborne altimetry <phrase>data</phrase>.
accurate 3d acquisition of freely <phrase>moving objects</phrase>.
this <phrase>paper</phrase> presents a new acquisition method for 3d <phrase>laser</phrase> scanners that combines imaging, fast geometrical object tracking, and automatic <phrase>pose estimation</phrase> to register <phrase>range</phrase> profiles of freely <phrase>moving objects</phrase>. the method was developed to solve the constraint of rigidity between <phrase>free</phrase>-<phrase>moving objects</phrase> and a 3d scanner while preserving the accuracy of the <phrase>range</phrase> measurements. rigidity constraint imposes that a 3d scanner or any external positioning devices must be perfectly stable relative to the object during scanning. this is often impossible for moving structures such as when using <phrase>scaffolding</phrase>, <phrase>industrial</phrase> conveyers, or robotic <phrase>arms</phrase>. the method starts by creating a rough, partial, and distorted estimate of the <phrase>model</phrase> of the object from an initial <phrase>subset</phrase> of sparse <phrase>range</phrase> <phrase>data</phrase>. then, it recursively improves and refines the <phrase>model</phrase> by adding new <phrase>range</phrase> <phrase>information</phrase>. in parallel, real-time tracking of the object is performed to <phrase>center</phrase> the scan on the object. a <phrase>high</phrase>-resolution and accurate 3d <phrase>model</phrase> of a <phrase>free</phrase>-floating object, and real-time tracking of its position is obtained.
using <phrase>omnidirectional</phrase> <phrase>structure from motion</phrase> for registration of <phrase>range</phrase> images of minimal overlap.
in this <phrase>paper</phrase>, we propose a novel method of merging a series of <phrase>range</phrase> images with a minimal overlap between any two consecutive <phrase>range</phrase> images. we rigidly <phrase>mount</phrase> a parabolic catadioptric <phrase>camera</phrase> to the <phrase>range</phrase> scanner. using two omniviews we are able to accurately estimate the relative displacement between two <phrase>range</phrase> views. the resultant motion is used for the registration of all <phrase>range</phrase> <phrase>data</phrase> to the same coordinate system. an additional perspective <phrase>camera</phrase> calibrated with respect to the scanner is used for <phrase>texture mapping</phrase>.
a statistical method for robust 3d <phrase>surface reconstruction</phrase> from sparse <phrase>data</phrase>.
<phrase>general</phrase> <phrase>information</phrase> about a class of objects, such as <phrase>human</phrase> faces or teeth, can help to solve the otherwise ill-posed problem of reconstructing a complete surface from sparse 3d <phrase>feature points</phrase> or 2d projections of points. we present a technique that uses a <phrase>vector space</phrase> representation of shape (3d morphable <phrase>model</phrase>) to infer missing vertex coordinates. regularization derived from a statistical approach makes the system stable and robust with respect to noise by <phrase>computing</phrase> the optimal tradeoff between fitting quality and plausibility. we present a direct, non-iterative <phrase>algorithm</phrase> to calculate this optimum efficiently, and a method for simultaneously compensating unknown rigid transformations. the system is applied and evaluated in two different fields: (1) <phrase>reconstruction</phrase> of 3d faces at unknown orientations from 2d <phrase>feature points</phrase> at interactive rates, and (2) <phrase>restoration</phrase> of missing surface regions of teeth for <phrase>cad</phrase>-cam <phrase>production</phrase> of dental inlays and other <phrase>medical</phrase> applications.
enhanced <phrase>vector quantization</phrase> for <phrase>data</phrase> reduction and filtering.
modern automatic digitizers can sample huge amounts of 3d <phrase>data</phrase> points on the object surface in a <phrase>short</phrase> time. point based graphics is becoming a popular framework to reduce the <phrase>cardinality</phrase> of these <phrase>data</phrase> sets and to filter measurement noise, without having to store in <phrase>memory</phrase> and process mesh connectivity. main contribution of this <phrase>paper</phrase> is the introduction of soft clustering techniques in the field of point clouds processing. in this approach <phrase>data</phrase> points are not assigned to a <phrase>single</phrase> cluster, but they contribute in the determination of the position of several cluster centres. as a result a better representation of the <phrase>data</phrase> is achieved. in soft clustering techniques, a <phrase>data set</phrase> is represented with a reduced number of points called reference vectors (rv), which minimize an adequate error measure. as the position of the <phrase>rvs</phrase> is determined by "learning", which can be viewed as an iterative optimization procedure, they are inherently slow. we show here how partitioning the <phrase>data</phrase> domain into disjointed regions called hyperboxes (hb), the computation can be localized and the computational time reduced to linear in the number of <phrase>data</phrase> points (o(n)), saving more than 75% on real applications with respect to <phrase>classical</phrase> soft-vq solutions, making therefore vq suitable to the task. the procedure is suitable for a parallel hw implementation, which would <phrase>lead</phrase> to a complexity sub-linear in n. an automatic procedure for setting the <phrase>voxel</phrase> side and the other parameters can be derived from the <phrase>data-set</phrase> analysis. <phrase>results</phrase> obtained in the <phrase>reconstruction</phrase> of faces of both humans and puppets as well as on models from clouds of points made available on the web are reported and discussed in comparison with other available methods.
shapelab: a unified framework for 2d & 3d shape retrieval.
2d or 3d shapes are the most important visual <phrase>information</phrase> that we use to recognize an object. we propose a unified framework "shapelab" to search similar 2d or 3d shapes from an existing <phrase>database</phrase>. users can search 3d shapes with a 2d input, and vice versa. shapelab is composed of four key components: (1) pose determination for 3d models; (2) 2d orthogonal view generation based on multiple levels of detail; (3) similarity measurement between 2d shapes; and (4) freehand <phrase>sketch</phrase>-based <phrase>user interface</phrase>. key <phrase>algorithms</phrase> supporting the above components are briefly described. experiments show shapelab can provide a better performance such as <phrase>high</phrase> accuracy, flexibility and <phrase>scalability</phrase> compared to the available methods.
online <phrase>surface reconstruction</phrase> from unorganized 3d-points for the <phrase>dlr</phrase> hand-guided scanner system.
hand-guided scanners allow for digitization by manually sweeping a <phrase>laser beam</phrase> over an object's surface. the result highly depends on the way the user handles the system and his ability to keep <phrase>track</phrase> of the parts of the surface that are already scanned. processing and visualization during <phrase>data acquisition</phrase> are helpful in this context. in this <phrase>paper</phrase>, we propose an online <phrase>surface reconstruction</phrase> <phrase>algorithm</phrase> for the visualization of the <phrase>dlr</phrase> scanner system <phrase>data</phrase>. the <phrase>algorithm</phrase> successively generates a <phrase>triangle</phrase> mesh by incrementally inserting 3d points. point neighborhoods are used to limit the point <phrase>density</phrase>, to estimate the <phrase>surface normal</phrase> at the inserted point, and to locally re-triangulate the mesh. a dynamic <phrase>data structure</phrase> for fast <phrase>neighborhood</phrase> search without restrictions to the amount of vertices or the object size and with low complexity is introduced. finally, <phrase>results</phrase> with the hand-guided scanner system are presented.
robust <phrase>structure from motion</phrase> under weak perspective.
it is widely known that, for the affine <phrase>camera</phrase> <phrase>model</phrase>, both shape and motion <phrase>data</phrase> can be factorized directly from the measurement matrix constructed from 2d image points coordinates. however, <phrase>classical</phrase> <phrase>algorithms</phrase> for <phrase>structure from motion</phrase> (sfm) are not robust: measurement <phrase>outliers</phrase>, that is, incorrectly detected or matched <phrase>feature points</phrase> can destroy the result. a few methods to robustify sfm have already been proposed. different <phrase>outlier</phrase> detection schemes have been used. we examine an efficient <phrase>algorithm</phrase> by trajkovic et al. [robust recursive structure and motion recovery under affine projection] who use affine <phrase>camera</phrase> <phrase>model</phrase> and the least median of squares (lmeds) method to separate inliers from <phrase>outliers</phrase>. lmeds is only applicable when the ratio of inliers exceeds 50%. we show that the least trimmed squares (lts) method is more efficient in robust sfm than lmeds. in particular, we demonstrate that lts can handle inlier ratios below 50%. we also show that using the real (<phrase>euclidean</phrase>) motion <phrase>data</phrase> <phrase>results</phrase> in a more precise sfm <phrase>algorithm</phrase> that using the affine <phrase>camera</phrase> <phrase>model</phrase>. based on these observations, we propose a novel robust sfm <phrase>algorithm</phrase> and discuss its advantages and limits. <phrase>the proposed method</phrase> and the trajkovicprocedure are quantitatively compared on synthetic <phrase>data</phrase> in different simulated situations. the methods are also tested on synthesized and real <phrase>video</phrase> sequences.
<phrase>gpu</phrase>-assisted z-field simplification.
height fields and depth maps which we collectively refer to as z-fields, usually carry a lot of redundant <phrase>information</phrase> and are often used in real-time applications. this is the reason why efficient methods for their simplification are necessary. on the other hand, the computation power and programmability of <phrase>commodity</phrase> <phrase>graphics hardware</phrase> has significantly grown. we present an adaptation of an existing real-time z-field simplification method for execution in <phrase>graphics hardware</phrase>. the main parts of the <phrase>algorithm</phrase> are implemented as fragment programs which run on the <phrase>gpu</phrase>. the resulting polygonal models are identical to the ones obtained by the original method. the main benefit is that the computation load is imposed on the <phrase>gpu</phrase>, freeing-up the <phrase>cpu</phrase> for other tasks. additionally, the new method exhibits a <phrase>performance improvement</phrase> when compared to a pure <phrase>cpu</phrase> implementation.
estimating the principal curvatures and the darboux frame from real 3d <phrase>range</phrase> <phrase>data</phrase>.
tracking densely moving markers.
using principal curvatures and darboux frame to recover 3d geometric primitives from <phrase>range</phrase> images.
3d shape estimation based on <phrase>density</phrase> driven <phrase>model</phrase> fitting.
<phrase>multiple view</phrase> <phrase>reconstruction</phrase> of people.
this <phrase>paper</phrase> presents a unified framework for <phrase>model</phrase>-based and <phrase>model</phrase>-<phrase>free</phrase> <phrase>reconstruction</phrase> of people from multiple <phrase>camera</phrase> views in a studio environment. shape and appearance of the reconstructed <phrase>model</phrase> are optimised simultaneously based on <phrase>multiple view</phrase> <phrase>silhouette</phrase>, <phrase>stereo</phrase> and feature correspondence. a priori <phrase>knowledge</phrase> of surface structure is introduced as regularisation constraints. <phrase>model</phrase>-based <phrase>reconstruction</phrase> assumes a known generic <phrase>humanoid</phrase> <phrase>model</phrase> a priori, which is fitted to the <phrase>multi-view</phrase> observations to produce a structured representation for <phrase>animation</phrase>. <phrase>model</phrase>-<phrase>free</phrase> <phrase>reconstruction</phrase> makes no priori assumptions on scene <phrase>geometry</phrase> allowing the <phrase>reconstruction</phrase> of complex dynamic scenes. <phrase>results</phrase> are presented for <phrase>reconstruction</phrase> of sequences of people from <phrase>multiple views</phrase>. the <phrase>model</phrase>-<phrase>based approach</phrase> produces a consistent structured representation, which is robust in the presence of visual ambiguities. this overcomes limitations of existing <phrase>visual-hull</phrase> and <phrase>stereo</phrase> techniques. <phrase>model</phrase>-<phrase>free</phrase> <phrase>reconstruction</phrase> allows <phrase>high</phrase>-quality novel <phrase>view-synthesis</phrase> with accurate reproduction of the detailed dynamics for <phrase>hair</phrase> and loose <phrase>clothing</phrase>. <phrase>multiple view</phrase> optimisation achieves a visual quality comparable to the captured <phrase>video</phrase> without visual artefacts due to misalignment of images.
blind watermarking of 3d shapes using localized constraints.
this <phrase>paper</phrase> develops a <phrase>digital</phrase> watermarking methodology for 3-d <phrase>graphical</phrase> objects defined by polygonal meshes. in watermarking or fingerprinting the aim is to embed a code in a given <phrase>media</phrase> without producing identifiable changes to it. one should be able to retrieve the embedded <phrase>information</phrase> even after the shape had suffered various modifications. two blind watermarking techniques applying perturbations onto the local <phrase>geometry</phrase> for selected vertices are described in this <phrase>paper</phrase>. the proposed methods produce localized changes of vertex locations that do not alter the mesh <phrase>topology</phrase>. a study of the effects caused by vertex location modification is provided for a <phrase>general</phrase> class of surfaces. the robustness of the proposed <phrase>algorithms</phrase> is tested at noise <phrase>perturbation</phrase> and object cropping.
tele-3d - developing a handheld scanner using <phrase>structured light</phrase> projection.
acquisition, modelling and rendering of very large <phrase>urban</phrase> environments.
in this <phrase>paper</phrase> we describe a vehicle borne <phrase>data acquisition</phrase> system for <phrase>urban</phrase> environments and associated 3d <phrase>data management</phrase> and interactive rendering <phrase>software</phrase>. the <phrase>data acquisition</phrase> system is capable of acquire 3d <phrase>data</phrase> from <phrase>urban areas</phrase> with <phrase>centimetre</phrase> resolution including automatic capturing of colour. the system includes a <phrase>management</phrase> and interactive rendering <phrase>software</phrase> which is designed to cope with the huge quantities of <phrase>data</phrase> generated by the acquisition system. it uses out-of-core pre-processing to transform <phrase>data</phrase> into octrees. real-time interactive rendering is achieved by using novel techniques such as front-to-back <phrase>octree</phrase> traversal, occlusion query and <phrase>speculative</phrase> pre-fetching. the <phrase>paper</phrase> presents the <phrase>results</phrase> of the described techniques applied to large <phrase>public</phrase> areas including the <phrase>city</phrase> centre of <phrase>verona</phrase>, <phrase>italy</phrase>.
from 3d shape capture to <phrase>animated</phrase> models.
neuroanatomical imaging: constrained 3d <phrase>reconstruction</phrase> using variational implicit techniques.
real-time speech-driven 3d face <phrase>animation</phrase>.
some unusual ways of visually sensing 3d shapes.
user-controlled simplification of polygonal models.
polygonal models are ubiquitous in computer graphics but their real time manipulation and rendering especially in interactive application environments have become a threat because of their huge sizes and complexity. a whole <phrase>family</phrase> of automatic <phrase>algorithms</phrase> emerged during the last decade to help out this problem, but they reduce the complexity of the models uniformly without caring about <phrase>semantic</phrase> importance of localized parts of a mesh. only a few <phrase>algorithms</phrase> deal with adaptive simplification of polygonal models. we propose a new <phrase>model</phrase> for user-driven simplification exploiting the simplification hierarchy and hypertriangulation <phrase>model</phrase> [a resolution modeling system] that lends a user the most of the functionalities of existing adaptive simplification <phrase>algorithms</phrase> in one place and is quite simple to implement. the proposed new underlying <phrase>data structures</phrase> are compact and support real time <phrase>navigation</phrase> across continuous lods of a <phrase>model</phrase>; any desirable <phrase>lod</phrase> can be extracted efficiently and can further be fine-tuned. the proposed <phrase>model</phrase> for adaptive simplification supports two key operations for selective refinement and selective simplification; their effect has been shown on various polygonal models. comparison with related work shows that the proposed <phrase>model</phrase> provides combined environment at reduced overhead of <phrase>memory</phrase> space usage and faster running times.
non-rigid <phrase>range</phrase>-scan alignment using thin-plate splines.
we present a non-rigid alignment <phrase>algorithm</phrase> for aligning <phrase>high</phrase>-resolution <phrase>range</phrase> <phrase>data</phrase> in the presence of low-<phrase>frequency</phrase> deformations, such as those caused by scanner calibration error. traditional <phrase>iterative closest</phrase> points (<phrase>icp</phrase>) <phrase>algorithms</phrase>, which rely on <phrase>rigid-body</phrase> alignment, fail in these cases because the error appears as a non-rigid warp in the <phrase>data</phrase>. our <phrase>algorithm</phrase> combines the robustness and efficiency of <phrase>icp</phrase> with the expressiveness of thin-plate splines to align <phrase>high</phrase>-resolution scanned <phrase>data</phrase> accurately, such as scans from the <phrase>digital</phrase> <phrase>michelangelo</phrase> project [the <phrase>digital</phrase> <phrase>michelangelo</phrase> project: 3-d scanning of large statues]. this application is distinguished from previous uses of the thin-plate spline by the fact that the resolution and size of <phrase>warping</phrase> are several orders of <phrase>magnitude</phrase> smaller than the extent of the mesh, thus requiring especially precise feature correspondence.
shape <phrase>distortion</phrase> analysis of the <phrase>shape-from-shading</phrase> <phrase>algorithm</phrase> using jacobi <phrase>iterative method</phrase>.
metrological analysis of a procedure for the automatic 3d modeling of dental <phrase>plaster</phrase> casts.
as well known, in the <phrase>reconstruction</phrase> of the 3d models through optical systems, the errors are due to the <phrase>single</phrase>-view acquisition error and to the 3d modeling procedure. the latter can be ascribed to the various phases of the 3d modeling pipeline: pairwise registration, global registration, surface integration. this work examines the acquisition error as well as the errors due to an automatic procedure recently proposed for the 3d modeling of dental <phrase>plaster</phrase> casts. this contribution derives a simple error propagation <phrase>model</phrase>, rather useful for practical <phrase>simulation</phrase> purposes. from a <phrase>general</phrase> viewpoint, this contribution proposes a useful <phrase>simulation</phrase> of error propagation in 3d modeling, it shows the quality of an automatic 3d modeling procedure recently proposed and it shows the accuracy of 3d modeling dental <phrase>plaster</phrase> casts by current commercial <phrase>range</phrase> cameras and the considered automatic method.
<phrase>mathematical</phrase> aspects of shape <phrase>reconstruction</phrase> from an image <phrase>sequence</phrase>.
spherical <phrase>diffusion</phrase> for 3d surface smoothing.
a <phrase>diffusion</phrase>-<phrase>based approach</phrase> to surface smoothing is presented. surfaces are represented as scalar functions defined on the <phrase>sphere</phrase>. the approach is equivalent to gaussian smoothing on the <phrase>sphere</phrase> and is computationally efficient since it does not require iterative smoothing. furthermore, it does not suffer from the well-known shrinkage problem. <phrase>evolution</phrase> of important shape features (parabolic curves) under <phrase>diffusion</phrase> is demonstrated. a nonlinear modification of the <phrase>diffusion</phrase> process is introduced in <phrase>order</phrase> to improve smoothing behavior of elongated and poorly centered objects.
coding with <phrase>ascii</phrase>: compact, yet text-based 3d content.
realistic models of children heads from 3d-<phrase>mri</phrase> segmentation and <phrase>tetrahedral</phrase> mesh <phrase>construction</phrase>.
in <phrase>order</phrase> to analyze the sensitivity of children to rf fields and <phrase>mobile phones</phrase> in particular, the sar (specific absorption ratio) defined as the power absorbed by a unit of <phrase>mass</phrase> of tissues (w/kg) should be computed based on a numerical <phrase>model</phrase> of the head.we propose to build realistic models from 3d-<phrase>mri</phrase> of children heads.the method is composed of two steps. the first one consists in segmenting the main tissues in these images (<phrase>skin</phrase>, <phrase>fat</phrase>, muscles, <phrase>cortical</phrase> and <phrase>marrow</phrase> <phrase>bones</phrase>, <phrase>cerebrospinal fluid</phrase>, grey and <phrase>white matter</phrase>, <phrase>blood</phrase>, etc).the segmentation is based on <phrase>mathematical morphology</phrase> methods which are well adapted to this aim and provide a robust and automatic method requiring minimum user intervention.using simplified segmented images, the second step concerns the <phrase>tetrahedral</phrase> mesh generation.our method uses almost regular meshes and <phrase>topological</phrase> tools to preserve the <phrase>topological</phrase> <phrase>arrangement</phrase> of the head tissues.a method to guarantee a good geometrical quality is also provided.
theoretical accuracy analysis of n-ocular vision systems for scene <phrase>reconstruction</phrase>, <phrase>motion estimation</phrase>, and positioning.
theoretical models are derived to analyze the accuracy of n-ocular vision systems for scene <phrase>reconstruction</phrase>, <phrase>motion estimation</phrase> and self positioning. <phrase>covariance</phrase> matrices are given to estimate the uncertainty bounds for the reconstructed points in 3-d space, motion parameters, and 3-d position of the vision system. <phrase>simulation</phrase> <phrase>results</phrase> of various experiments, based on <phrase>synthetic and real</phrase> <phrase>data</phrase> acquired with <phrase>a 12</phrase>-<phrase>camera</phrase> <phrase>stereo</phrase> panoramic imaging system, are given to demonstrate the application of these models, as well as to evaluate the performance of the panoramic system for <phrase>high</phrase>-precision 3-d mapping and positioning.
local approximate 3d matching of <phrase>proteins</phrase> in <phrase>viral</phrase> cryo-em <phrase>density</phrase> maps.
<phrase>experimental</phrase> structure analysis of biological <phrase>molecules</phrase> (e.g, <phrase>proteins</phrase>) or <phrase>macromolecular</phrase> complexes (e.g, <phrase>viruses</phrase>) can be used to generate <phrase>three-dimensional</phrase> <phrase>density</phrase> maps of these entities. such a <phrase>density</phrase> map can be viewed as a <phrase>three-dimensional</phrase> gray-scale image where space is subdivided in voxels of a given size. the focus of this <phrase>paper</phrase> is the analysis of <phrase>virus</phrase> <phrase>density</phrase> maps. the <phrase>hull</phrase> of a <phrase>virus</phrase> consists of many copies of one or several different <phrase>proteins</phrase>. an important tool for the study of <phrase>viruses</phrase> is <phrase>cryo-electron microscopy</phrase> (cryo-em), a technique with insufficient resolution to directly determine the <phrase>arrangement</phrase> of the <phrase>proteins</phrase> in the <phrase>virus</phrase>. we therefore created a tool that locates <phrase>proteins</phrase> in the <phrase>three-dimensional</phrase> <phrase>density</phrase> map of a <phrase>virus</phrase>. the goal is to fully determine the locations and orientations of the <phrase>protein</phrase>(s) in the <phrase>virus</phrase> given the <phrase>virus</phrase>' <phrase>three-dimensional</phrase> <phrase>density</phrase> map and a <phrase>database</phrase> of <phrase>density</phrase> maps of one or more <phrase>protein</phrase> candidates.
<phrase>stochastic</phrase> mesh-based multiview <phrase>reconstruction</phrase>.
an <phrase>experimental</phrase> comparison of <phrase>feature-based</phrase> 3d retrieval methods.
3d objects are an important type of <phrase>multimedia</phrase> <phrase>data</phrase> with many promising application possibilities. defining the aspects that constitute the similarity among 3d objects, and designing <phrase>algorithms</phrase> that implement such similarity definitions is a difficult problem. over the last few years, a strong interest in methods for <phrase>feature-based</phrase> 3d similarity search has arisen, and a growing number of competing <phrase>algorithms</phrase> for <phrase>content-based</phrase> retrieval of 3d objects have been proposed. we present an extensive <phrase>experimental</phrase> evaluation of the retrieval effectiveness and efficiency of a large part of the current <phrase>state</phrase>-of-the-<phrase>art</phrase> <phrase>feature-based</phrase> methods for 3d similarity search, giving a contrasting assessment of the different approaches.
neural mesh ensembles.
this <phrase>paper</phrase> proposes the use of <phrase>neural network</phrase> ensembles to boost the performance of a <phrase>neural network</phrase> based <phrase>surface reconstruction</phrase> <phrase>algorithm</phrase>. ensemble is a very popular and powerful statistical technique based on the idea of averaging several outputs of a probabilistic <phrase>algorithm</phrase>. in the context of <phrase>surface reconstruction</phrase>, two main problems arise. the first is finding an efficient way to <phrase>average</phrase> meshes with different connectivity, and the second is tuning the parameters for <phrase>surface reconstruction</phrase> to maximize the performance of the ensemble. we solve the first problem by voxelizing all the meshes on the same regular grid and taking <phrase>majority vote</phrase> on each <phrase>voxel</phrase>. we tune the parameters experimentally, borrowing ideas from weak learning methods.
weaver, an automatic texture builder.
from the lab to the <phrase>silver</phrase> screen: computer vision and the <phrase>art</phrase> of <phrase>special effects</phrase>.
roboscan: an automatic system for accurate and unattended 3d scanning.
we describe an automatic system for fast unattended acquisition of accurate and complete 3d models, called roboscan. the <phrase>design</phrase> goal is to reduce the three main bottlenecks in <phrase>human</phrase>-assisted 3d scanning: the selection of the <phrase>range</phrase> maps to be taken (view planning), the positioning of the scanner in the environment, and the <phrase>range</phrase> maps' alignment. the system is designed around a commercial <phrase>laser</phrase>-based 3d scanner moved by a <phrase>robotic arm</phrase>. the acquisition session is organised in two stages. first, an initial sampling of the surface is performed by the automatic selection of a set of views. then, some added views are automatically selected, acquired and merged to the initial set in <phrase>order</phrase> to fill the surface regions left unsampled. both the initial set of <phrase>range</phrase> maps and the subsequently added ones are post-processed automatically, by using the known scanner positions to initialise the alignment phase. <phrase>results</phrase> of the assessment of the system on real acquisitions are presented and discussed.
computational experiments with <phrase>area</phrase>-based <phrase>stereo</phrase> for <phrase>image-based</phrase> rendering.
optimal and near - optimal solutions for 3d structure comparisons.
photo-consistency based registration of an uncalibrated image pair to a 3d surface <phrase>model</phrase> using <phrase>genetic algorithm</phrase>.
we consider the following <phrase>data fusion</phrase> problem. a 3d object with textured lambertian surface is measured and independently photographed. a triangulated <phrase>model</phrase> of the object and two uncalibrated images are obtained. the goal is to precisely register the images to the <phrase>model</phrase>. solving this problem is necessary for building a geometrically accurate, <phrase>photorealistic</phrase> <phrase>model</phrase> from <phrase>laser</phrase>-scanned 3d <phrase>data</phrase> and <phrase>high</phrase> quality images. recently, we have proposed a novel method that generalises the photo-consistency approach by clarkson et al. [using photo-consistency to register 2d optical images of the <phrase>human</phrase> face to a 3d surface <phrase>model</phrase>] to the case of uncalibrated cameras, when both intrinsic and extrinsic parameters are unknown. this gives a user the freedom of taking the pictures by a conventional <phrase>digital camera</phrase>, from arbitrary positions and with varying zoom. the <phrase>method is based</phrase> on manual pre-registration followed by a <phrase>genetic</phrase> optimisation <phrase>algorithm</phrase>. a brief description of the <phrase>pilot</phrase> version of the method [precise registration of an uncalibrated image pair to a 3d surface <phrase>model</phrase>] has been given together with the <phrase>results</phrase> of a few initial <phrase>tests</phrase>. in this <phrase>paper</phrase>, we <phrase>report</phrase> on some new significant developments in this project. the critical issue of robustness against illumination changes is addressed and various colour representations and cost functions are tested and compared. natural constraints are introduced and experimentally validated to simplify the <phrase>camera</phrase> <phrase>model</phrase> and accelerate the <phrase>algorithm</phrase>. finally, we present <phrase>synthetic and real</phrase> <phrase>data</phrase> with <phrase>ground truth</phrase>, apply the improved method to the <phrase>data</phrase> and measure the quality of the <phrase>results</phrase>.
multiresolution approach to <phrase>three-dimensional</phrase> <phrase>stereo</phrase> vision.
an approach to using <phrase>image-based</phrase> techniques across unreliable <phrase>peer-to-peer</phrase> networks.
acquiring height maps of faces from a <phrase>single</phrase> image.
in this <phrase>paper</phrase> we explore how to improve the quality of the height map recovered from faces using <phrase>shape-from-shading</phrase>. one of the problems with reliable face <phrase>surface reconstruction</phrase> using <phrase>shape-from-shading</phrase> is that local errors in the needle map can cause implosion of facial features, and in particular the nose. to overcome this problem in this <phrase>paper</phrase> we develop a method for ensuring surface convexity. this is done by modifying the <phrase>gradient</phrase> orientations in accordance with critical points on the surface. we utilize a local shape indicator as a criteria to decide which <phrase>surface normals</phrase> are to be modified. experiments show that altering the directions of a <phrase>surface normal</phrase> field of a face leads to a considerable improvement in its integrated height map.
towards <phrase>urban</phrase> 3d <phrase>reconstruction</phrase> from <phrase>video</phrase>.
the <phrase>paper</phrase> introduces a <phrase>data</phrase> collection system and a processing pipeline for automatic geo-registered 3d <phrase>reconstruction</phrase> of <phrase>urban</phrase> scenes from <phrase>video</phrase>. the system collects multiple <phrase>video</phrase> streams, as well as <phrase>gps</phrase> and ins measurements in <phrase>order</phrase> to place the reconstructed models in geo-registered coordinates. besides <phrase>high</phrase> quality in terms of both <phrase>geometry</phrase> and appearance, we aim at real-time performance. even though our processing pipeline is currently far from being real-time, we select techniques and we <phrase>design</phrase> processing modules that can achieve fast performance on multiple cpus and <phrase>gpus</phrase> aiming at real-time performance in the near future. we present the main considerations in designing the system and the steps of the processing pipeline. we show <phrase>results</phrase> on real <phrase>video</phrase> sequences captured by our system.
improving environment modelling by edge occlusion surface completion.
visual <phrase>data</phrase> navigators "collaboratories".
view dependence of 3d recovery from folded pictures and warped 3d faces.
in a popular visual illusion, the <phrase>portrait</phrase> on <phrase>paper currency</phrase> is folded into an m shape along vertical lines through the nose and the eyes. when this folded picture is tilted back and forth horizontally the face undergoes striking changes in expression. this <phrase>distortion</phrase> reveals two insights concerning 3d representation in the <phrase>human</phrase> visual system and we have explored these with experiments on simple schematic faces and observations on distortions of <phrase>laser</phrase> <phrase>range</phrase> images of faces. the observations show first that when recovering depicted depth, pictorial cues are interpreted independently of <phrase>binocular</phrase> depth <phrase>information</phrase> and second, that the recovery of <phrase>facial expression</phrase> is based on a scaled prototypical face structure.
variational multiframe <phrase>stereo</phrase> in the presence of <phrase>specular</phrase> reflections.
a <phrase>depth map</phrase> representation for real-time transmission and view-based rendering of a dynamic 3d scene.
probabilistic 3d <phrase>data fusion</phrase> for adaptive resolution surface generation.
3d shape registration using regularized medial scaffolds.
this <phrase>paper</phrase> proposes a novel method for global registration based on matching 3d medial structures of unorganized point clouds or triangulated meshes. most practical known methods are based on the <phrase>iterative closest</phrase> point (<phrase>icp</phrase>) <phrase>algorithm</phrase>, which requires an initial alignment close to the globally optimal <phrase>solution</phrase> to ensure convergence to a valid <phrase>solution</phrase>. furthermore, it can also fail when there are points in one dataset with no corresponding matches in the other dataset. <phrase>the proposed method</phrase> automatically finds an initial alignment close to the global optimal by using the medial structure of the datasets. for this purpose, we first compute the medial scaffold of a 3d dataset: a 3d <phrase>graph</phrase> made of special shock curves linking special shock nodes. this medial scaffold is then regularized exploiting the known transitions of the 3d medial <phrase>axis</phrase> under deformation or <phrase>perturbation</phrase> of the input <phrase>data</phrase>. the resulting simplified medial scaffolds are then registered using a modified graduated assignment <phrase>graph</phrase> matching <phrase>algorithm</phrase>. <phrase>the proposed method</phrase> shows robustness to noise, shape deformations, and varying surface sampling densities.
a <phrase>multi-resolution</phrase> scheme <phrase>icp</phrase> <phrase>algorithm</phrase> for fast shape registration.
super <phrase>high</phrase> resolution 3d imaging and efficient visualization.
applying mesh conformation on shape analysis with missing <phrase>data</phrase>.
a mesh conformation approach that makes use of deformable generic meshes has been applied to establishing correspondences between 3d shapes with missing data.given a group of shapes with correspondences, we can build up a statistical shape <phrase>model</phrase> by applying <phrase>principal component analysis</phrase> (<phrase>pca</phrase>). the conformation at first globally maps the generic mesh to the 3d shape based on manually located corresponding landmarks, and then locally deforms the generic mesh to clone the 3d shape.the local deformation is constrained by minimizing the <phrase>energy</phrase> of an elastic model.an <phrase>algorithm</phrase> was also embedded in the conformation process to fill missing surface <phrase>data</phrase> of the shapes.using synthetic <phrase>data</phrase>, we demonstrate that the conformation preserves the configuration of the generic mesh and hence it helps to establish good correspondences for shape analysis.case studies of the <phrase>principal component analysis</phrase> of shapes were presented to illustrate the successes and advantages of our approach.
adaptive online transmission of 3d texmesh using <phrase>scale-space</phrase> analysis.
<phrase>wavelet</phrase> coding of structured <phrase>geometry</phrase> <phrase>data</phrase> considering rate-<phrase>distortion</phrase> properties.
reliability and judging fatigue reduction in 3d <phrase>perceptual</phrase> quality estimation.
<phrase>content-based image retrieval</phrase> from large <phrase>medical</phrase> <phrase>databases</phrase>.
markerless <phrase>human</phrase> motion transfer.
in this <phrase>paper</phrase> we develop a computer vision-based system to transfer <phrase>human</phrase> motion from one subject to another. our system uses a network of eight calibrated and synchronized cameras. we first build detailed <phrase>kinematic</phrase> models of the subjects based on our <phrase>algorithms</phrase> for extracting <phrase>shape from silhouette</phrase> across time [a 3d <phrase>reconstruction</phrase> <phrase>algorithm</phrase> combining shape-frame-shilhouette]. these models are then used to capture the motion (joint angles) of the subjects in new <phrase>video</phrase> sequences. finally we describe an <phrase>image-based</phrase> rendering <phrase>algorithm</phrase> to render the captured motion applied to the articulated <phrase>model</phrase> of another person. our rendering <phrase>algorithm</phrase> uses an ensemble of spatially and temporally distributed images to generate photo-realistic <phrase>video</phrase> of the transferred motion. we demonstrate the performance of the system by rendering throwing and kungfu motions on subjects who did not perform them.
<phrase>topology</phrase> and <phrase>geometry</phrase> of unorganized point clouds.
we present a new method for defining neighborhoods, and assigning principal <phrase>curvature</phrase> frames, and mean and <phrase>gauss</phrase> curvatures to the points of an unorganized oriented <phrase>point-cloud</phrase>. the neighborhoods are estimated by measuring implicitly the surface distance between points. the 3d shape recovery is based on <phrase>conformal geometry</phrase>, works directly on the <phrase>cloud</phrase>, does not rely on the generation of polygonal, or smooth models. <phrase>test</phrase> <phrase>results</phrase> on publicly available synthetic <phrase>data</phrase>, as <phrase>ground truth</phrase>, demonstrate that the method compares favorably to the established approaches for quantitative 3d shape recovery. <phrase>the proposed method</phrase> is developed to serve applications involving point based rendering and reliable extraction of differential properties from noisy unorganized point-clouds.
a <phrase>content-based</phrase> retrieval system with a customizeable 3d output visualizer.
<phrase>octree</phrase>-based <phrase>fusion</phrase> of <phrase>shape from silhouette</phrase> and shape from <phrase>structured light</phrase>.
uncalibrated 3 metric <phrase>reconstruction</phrase> and flattened texture acquisition from a <phrase>single</phrase> view of a surface of <phrase>revolution</phrase>.
inpainting from <phrase>multiple views</phrase>.
face recognition from 3d <phrase>data</phrase> using <phrase>iterative closest</phrase> point <phrase>algorithm</phrase> and gaussian mixture models.
a new approach to face verification from 3d <phrase>data</phrase> is presented. the method uses 3d registration techniques designed to work with resolution levels typical of the irregular <phrase>point cloud</phrase> representations provided by <phrase>structured light</phrase> scanning. preprocessing using a-priori <phrase>information</phrase> of the <phrase>human</phrase> face and the <phrase>iterative closest</phrase> point <phrase>algorithm</phrase> are employed to establish correspondence between <phrase>test</phrase> and <phrase>target</phrase> and to compensate for the non-rigid <phrase>nature</phrase> of the surfaces. statistical modelling in the form of gaussian mixture models is used to parameterise the distribution of errors in facial surfaces after registration and is employed to differentiate between intra- and extra-personal comparison of <phrase>range</phrase> images. an equal error rate of 2.67% was achieved on the 30 subject manual <phrase>subset</phrase> of the the 3d_rma <phrase>database</phrase>.
entire <phrase>model</phrase> acquisition system using handheld 3d digitizer.
in this <phrase>paper</phrase>, a real-time, handheld 3d <phrase>model</phrase> acquisition system consisting of a <phrase>laser</phrase> projector, a <phrase>video camera</phrase> and a turntable is described. the user projects a stripe of <phrase>light</phrase> at the 3d object by hand while rotating the object on a turntable. the projected <phrase>light</phrase> and <phrase>led</phrase> markers attached to the <phrase>laser</phrase> projector and turntable are captured by the <phrase>video camera</phrase>. by estimating the 3d orientation of the <phrase>laser</phrase> projector and the turntable angle from the 2d locations of the markers, the 3d location of the surface lit by the <phrase>laser</phrase> can be calculated. in addition, post-processing <phrase>algorithms</phrase> for refining the estimated 3d <phrase>data</phrase> have been proposed. the <phrase>algorithm</phrase> not only improves the accuracy of the 3d measurement, but also achieves to decrease the number of <phrase>leds</phrase> for 3d <phrase>data</phrase> estimation; therefore, it significantly improves the users convenience in scanning the object. with this system, users can measure an entire 3d object in real-time.
3d effect generation from <phrase>monocular</phrase> view.
synthetic image of multiresolution <phrase>sketch</phrase> leads to new features.
a new approach to <phrase>construction</phrase> of robust features is proposed and applied to an instance of the correspondence problem. the main idea is to construct a synthetic image by a multiresolution <phrase>sketch</phrase> (ms) of an image and involve it into extraction of the invariants. the ms is constructed by processing the image with a scalable detector of the semi-local 1d-elements. then, a synthetic image is constructed with all elements of the ms. local maxima of the first and second derivatives of the synthetic image along discrete curves of the ms <phrase>lead</phrase> to some singular elements represented by the points of a 4d <phrase>manifold</phrase>. it turns out that a representative <phrase>subset</phrase> of the singular elements is stable. to prove that, the pair-wise correspondence between subsets of singular elements of two shots of a <phrase>film</phrase> was established experimentally by a consistency technique, which, unlike past approaches, does not involve epipolar constraints.
hierarchical 3d <phrase>surface reconstruction</phrase> based on <phrase>radial</phrase> basis functions.
volumetric methods based on implicit surfaces are commonly used in <phrase>surface reconstruction</phrase> from uniformly distributed sparse 3d <phrase>data</phrase>. the case of non-uniform distributed <phrase>data</phrase> has recently deserved more attention, because it occurs frequently in practice. this <phrase>paper</phrase> describes a volumetric approach to <phrase>surface reconstruction</phrase> from non-uniform <phrase>data</phrase> which is suitable for the <phrase>reconstruction</phrase> of surfaces from images, in particular from <phrase>multiple views</phrase>. differently from volumetric methods which use both 3d surface points and <phrase>surface normals</phrase>, the approach does not use the <phrase>surface normals</phrase> because they are often unreliable when estimated from image <phrase>data</phrase>. the <phrase>method is based</phrase> on a hierarchical partitioning of the volume <phrase>data set</phrase>. the working volume is <phrase>split</phrase> and classified at different scales of spatial resolution into surface, internal and external voxels and this hierarchy is described by an <phrase>octree</phrase> structure in a multiscale framework. the <phrase>octree</phrase> structure is used to build a multiresolution description of the surface by means of compact support <phrase>radial</phrase> basis functions (rbf). a hierarchy of surface approximations at different levels of details is built by representing the voxels at the same <phrase>octree</phrase> level as rbf of similar spatial support. at each scale, <phrase>information</phrase> related to the <phrase>reconstruction</phrase> error drives the <phrase>reconstruction</phrase> process at the following finer scale. preliminary <phrase>results</phrase> on synthetic <phrase>data</phrase> and future perspectives are presented.
shape matching using the 3d <phrase>radon transform</phrase>.
in this <phrase>paper</phrase> a novel method for 3d <phrase>model</phrase> <phrase>content-based</phrase> search and retrieval based on the 3d <phrase>radon transform</phrase> and a querying-by-3d-<phrase>model</phrase> approach, is presented. descriptors are extracted using the 3d <phrase>radon transform</phrase> and applying a set of functionals on the transform coefficients. similarity measures are then created for the extracted descriptors and introduced into a 3d <phrase>model</phrase>-matching <phrase>algorithm</phrase>. this <phrase>results</phrase> to a very fast and accurate matching method. experiments were performed using two different <phrase>databases</phrase> and comparing <phrase>the proposed method</phrase> with others. <phrase>experimental</phrase> <phrase>results</phrase> show that <phrase>the proposed method</phrase> can be used for 3d <phrase>model</phrase> search and retrieval in a highly efficient manner.
archiving <phrase>technology</phrase> for <phrase>plant</phrase> inspection images captured by <phrase>mobile</phrase> active cameras - 4d visible <phrase>memory</phrase>.
filling holes in complex surfaces using volumetric <phrase>diffusion</phrase>.
robust concealment for erroneous block bursts in <phrase>stereoscopic</phrase> images.
with the increasing number of image <phrase>communication</phrase> applications especially in the low complexity domain, error concealment has become a very important field of <phrase>research</phrase>. since many compression standards for images and videos are block-based a lot of methods were applied to conceal block losses in <phrase>monocular</phrase> images. the fast progress of capture, representation and display technologies for 3d image <phrase>data</phrase> advances the efforts on 3d concealment strategies. because of their psycho-visual characteristics, <phrase>stereoscopic</phrase> images have to fulfill a very <phrase>high</phrase> quality demand. we propose an <phrase>algorithm</phrase> that makes use of the redundancies between two views of a <phrase>stereo</phrase> image pair. in many cases erroneous block bursts occur and can be highly disturbing, thus we will mainly concentrate on these errors. in addition, we focused on the quality assessment of several error concealment strategies. beside the objective evaluation measures, we carried out a subjective quality <phrase>test</phrase> following the dscqs methodology as proposed by <phrase>mpeg</phrase>. the <phrase>results</phrase> of this <phrase>test</phrase> demonstrate the efficiency of our approach.
fast landmark-based registration via deterministic and efficient processing, some preliminary <phrase>results</phrase>.
texture at the <phrase>terminator</phrase>.
using 3d-bresenham for resampling structured grids.
structured grids and some of their applications in <phrase>natural sciences</phrase> are discussed. the problem of their visualization and quantitative evaluation is considered and possible ways for itssolution sketched. resampling a structured grid onto a regular one is such a possible <phrase>solution</phrase> offering the additional benefit of enabling quantitative evaluations, too. this resampling is achieved by a preliminary tetrahedronization of the structured grid(s)and a subsequent digitalization of the constituent tetrahedrons using an adaptation of the 3d-bresenham <phrase>algorithm</phrase>. advantages and disadvantages of this approach as compared to other possible schemes are discussed. an implementation based on our <phrase>open source</phrase> f3d-<phrase>file format</phrase> for storage and transmission of volumetric <phrase>data</phrase> is presented, and <phrase>results</phrase> from applying it to <phrase>real-world</phrase> <phrase>data</phrase> shown.
interactive walkthroughs using "morphable 3d-<phrase>mosaics</phrase>".
this <phrase>paper</phrase> presents a <phrase>hybrid</phrase> (<phrase>geometry</phrase>- & <phrase>image-based</phrase>) technique suitable for providing interactive walkthroughs of large, complex outdoor scenes. motion is restricted along a smooth predefined path and the input to the system is a sparse set of <phrase>stereoscopic</phrase> views at certain points (key-positions) along that path (one view per position). an approximate local 3d <phrase>model</phrase> is constructed from each view, capable of capturing <phrase>photometric</phrase> and geometric properties of the scene only locally. then during the rendering process, a continuous <phrase>morphing</phrase> (both <phrase>photometric</phrase> & geometric) takes place between successive local 3d models, using what we call a "morphable 3d-<phrase>model</phrase>". the <phrase>morphing</phrase> proceeds in a physically-valid way. for this reason, a wide-baseline image matching technique is proposed, handling cases where the wide baseline between the two images is mainly due to a looming of the <phrase>camera</phrase>. our system can be extended in the event of multiple <phrase>stereoscopic</phrase> views (and therefore multiple local models) per key-position of the path (related by a <phrase>camera</phrase> rotation). in that case one local 3d-<phrase>mosaic</phrase> (per key-position) is constructed comprising all local 3d models therein and a "morphable 3d-<phrase>mosaic</phrase>" is used during the rendering process. a <phrase>partial-differential equation</phrase> is adopted to handle the problem of geometric consistency of each 3d-<phrase>mosaic</phrase>.
color, <phrase>fusion</phrase>, and <phrase>stereopsis</phrase>.
a <phrase>graph</phrase> cut based adaptive <phrase>structured light</phrase> approach for real-time <phrase>range</phrase> acquisition.
this <phrase>paper</phrase> describes a new <phrase>algorithm</phrase> that yields dense <phrase>range</phrase> maps in real-time.reconstruction are based on a <phrase>single</phrase> frame <phrase>structured light</phrase> illumination.on-the-<phrase>fly</phrase> adaptation of the projection pattern renders the system more robust against scene variability. a continuous <phrase>trade</phrase> off between speed and quality is made. the correspondence problem is solved by using geometric pattern coding in combination with sparse color coding. only local spatial and temporal continuity are assumed. this allows to construct a neighbor relationship within every frame and to <phrase>track</phrase> correspondences over time.all cues are integrated in one consistent labeling.this is achieved by reformulating the problem as a <phrase>graph</phrase> cut.every cue is weighted based on its <phrase>average</phrase> consistency with the result within a small time window.integration and weighting of additional cues is straightforward. the correctness of the <phrase>range</phrase> maps is not guaranteed, but an estimation of the uncertainty is provided for each part of the reconstruction.our <phrase>prototype</phrase> is implemented using unmodified <phrase>consumer</phrase> hardware only.frame rates vary between 10 and 25fps dependent on scene complexity.
3d volume extraction and mesh generation using <phrase>energy</phrase> minimization techniques.
multi-spectral <phrase>stereo</phrase> image matching using <phrase>mutual information</phrase>.
<phrase>mutual information</phrase> (mi) has shown promise as an effective <phrase>stereo</phrase> matching measure for images affected by <phrase>radiometric</phrase> <phrase>distortion</phrase>. this is due to the robustness of mi against changes in illumination. however, mi-based approaches are particularly prone to the generation of false matches due to the small <phrase>statistical power</phrase> of the matching <phrase>windows</phrase>. consequently, most previous mi approaches utilise large matching <phrase>windows</phrase> which smooth the estimated disparity field. this <phrase>paper</phrase> proposes extensions to mi-based <phrase>stereo</phrase> matching in <phrase>order</phrase> to increase the robustness of the <phrase>algorithm</phrase>. firstly, prior <phrase>probabilities</phrase> are incorporated into the mi measure in <phrase>order</phrase> to considerably increase the <phrase>statistical power</phrase> of the matching <phrase>windows</phrase>. these prior <phrase>probabilities</phrase>, which are calculated from the global joint <phrase>histogram</phrase> between the <phrase>stereo</phrase> pair, are tuned to a two level hierarchical approach. a 2d match surface, in which the match score is computed for every possible combination of template and matching window, is also utilised. this enforces left-right consistency and uniqueness constraints. these additions to mi-based <phrase>stereo</phrase> matching significantly enhance the algorithm's ability to detect correct matches while decreasing computation time and improving the accuracy. <phrase>results</phrase> show that the mi measure does not perform quite as well for standard <phrase>stereo</phrase> pairs when compared to traditional <phrase>area</phrase>-based metrics. however, the mi approach is far <phrase>superior</phrase> when matching across multi-spectra <phrase>stereo</phrase> pairs.
3d object modelling in <phrase>mobile robot</phrase> environment using <phrase>b-spline</phrase> surfaces.
automated <phrase>multi-view</phrase> 3d image acquisition in <phrase>human genome</phrase> <phrase>research</phrase>.
pictorial techniques and intrinsic images.
<phrase>high</phrase>-resolution cytometry network project: towards remote a d distributed acquisition, processing and visualisation of 3d image <phrase>data</phrase> in <phrase>human genome</phrase> <phrase>research</phrase>.
towards automatic modeling of monuments and towers.
<phrase>feature based</phrase> registration of <phrase>range</phrase> images for mapping of natural outdoor <phrase>feature based</phrase> registration of <phrase>range</phrase> images for mapping of natural outdoor.
challenges related to viewpoint registration in rough <phrase>forest</phrase> terrain can be quite different compared to those faced in structured environments. manoeuvring the <phrase>sensor</phrase> between measurement positions introduces large error into the a priori estimates of the registration coordinates. as a consequence, locally optimal registration methods may not work properly. moreover, due to the clutter, the scene contents can change substantially even due to a relative small displacement of the <phrase>sensor</phrase>. often, the <phrase>sensor</phrase> has to be moved to the other side of the <phrase>target</phrase> object, such as a group of <phrase>trees</phrase>, to get a good coverage of its <phrase>geometry</phrase>. in both cases, overlap between the two 3d <phrase>data</phrase> sets will be minimal ruling out conventional registration methods. in this <phrase>paper</phrase>, a <phrase>feature-based</phrase> method for registering 3d <phrase>range</phrase> scans for mapping natural outdoor environments is proposed. the method utilizes cylindrical, rotation symmetric features extracted from the 3d measurement <phrase>data</phrase> for viewpoint registration. the method is tested on real <phrase>range</phrase> images.
multiresolution distance volumes for progressive surface compression.
progressive compression of volumetric subdivision meshes.
we present a progressive compression technique for volumetric subdivision meshes based on the slow growing refinement <phrase>algorithm</phrase>. the system is comprised of a <phrase>wavelet transform</phrase> followed by a progressive encoding of the resulting <phrase>wavelet</phrase> coefficients. we compare the efficiency of two <phrase>wavelet</phrase> transforms. the first transform is based on the smoothing rules used in the slow growing subdivision technique. the second transform is a generalization of lifted linear <phrase>b-spline</phrase> wavelets to the same multi-tier refinement structure. direct coupling with a hierarchical coder produces progressive <phrase>bit</phrase> streams. rate <phrase>distortion</phrase> metrics are evaluated for both <phrase>wavelet</phrase> transforms. we tested the practical performance of the scheme on synthetic <phrase>data</phrase> as well as <phrase>data</phrase> from <phrase>laser</phrase> indirect-drive <phrase>fusion</phrase> simulations with multiple fields per vertex. both <phrase>wavelet</phrase> transforms result in <phrase>high</phrase> quality <phrase>trade</phrase> off curves and produce qualitatively good coarse representations.
robust identification and matching of fiducial points for the <phrase>reconstruction</phrase> of 3d <phrase>human</phrase> faces from raw <phrase>video</phrase> sequences.
effects of <phrase>joystick</phrase> mapping and field-of-view on <phrase>human</phrase> performance in virtual walkthroughs.
applications of 3d <phrase>medical imaging</phrase> in <phrase>orthopaedic</phrase> <phrase>surgery</phrase>: introducing the hip-op system.
a real-time realization of geometrical valid <phrase>view synthesis</phrase> for tele-conferencing with viewpoint adaptation.
<phrase>reconstruction</phrase> of <phrase>euclidean</phrase> planes from voxels.
in this <phrase>paper</phrase>, we aim to formulate the recognition of a planes from a discrete point set as a nonlinear <phrase>optimization problem</phrase>, and we prove a uniqueness theorem for the <phrase>solution</phrase> of this problem. we deal with the supercover <phrase>model</phrase> in a space for the expression of discrete planes. the <phrase>algorithm</phrase> achieves invertible <phrase>data compression</phrase> of <phrase>digital</phrase> objects, since the <phrase>algorithm</phrase> transforms a collection voxels to a collection of plane parameters, which classify the voxels.
thickness <phrase>histogram</phrase> and statistical <phrase>harmonic</phrase> representation for 3d <phrase>model</phrase> retrieval.
similarity measuring is a key problem for 3d <phrase>model</phrase> retrieval. in this <phrase>paper</phrase>, we propose a novel shape descriptor "thickness <phrase>histogram</phrase>" (th) by uniformly estimating thickness of a <phrase>model</phrase> using <phrase>statistical methods</phrase>. it is <phrase>translation</phrase> and rotation-invariant, discriminative to different shapes, and very efficient to compute with the shape distribution (sd) proposed by osada etc. for <phrase>high</phrase> performance of the retrieval, we propose a robust method for translating the directional form of the statistical distribution to the <phrase>harmonic</phrase> representation. by summing up energies at different <phrase>frequencies</phrase>, a matrix shape signature is formed to provide an exhaustive characterization of 3d <phrase>geometry</phrase>. experiments show that the performance of the statistical <phrase>harmonic</phrase> representation is among the top ones of existing shape descriptors.
motion-induced error correction in <phrase>ultrasound</phrase> imaging.
3d image sensing for <phrase>bit</phrase> plane method of progressive transmission.
<phrase>image compression</phrase> has received a lot of interest over the years. almost all the compression <phrase>algorithms</phrase> and standards, discussed in <phrase>literature</phrase>, gather <phrase>statistics</phrase> and compress on the complete image and compresses to suit various requirements such as <phrase>lossy</phrase> / lossless, baseline/progressive, spatial, <phrase>region</phrase> on interest. natural images such as gray scale images and <phrase>color images</phrase> are best compressed in the existing <phrase>literature</phrase> based on the local and global properties such as attributes of constituent <phrase>pixels</phrase> of the given image. in this <phrase>paper</phrase>, it is proposed to quantize the amplitudes of <phrase>pixel</phrase> values to form a number of <phrase>bit</phrase> planes and these <phrase>bit</phrase> planes are transmitted in either <phrase>lossy</phrase>, lossless, progressive manner. <phrase>bit</phrase> plane formation is attempted from the image acquiring stage to compress and then to transmission stage. <phrase>results</phrase> obtained are promising and give rise to new method or <phrase>ideology</phrase> in image sensing, acquiring, storage and transmission.
small <phrase>cpu</phrase> times and fast <phrase>interactivity</phrase> in <phrase>sonar</phrase> seabottom surveys.
<phrase>sonar</phrase> profiling of the seabottom provide 3d <phrase>data</phrase> sets that can <phrase>cover</phrase> huge survey areas with many gaps. this <phrase>paper</phrase> describes a multiresolution framework or visualization pipeline that is being optimized for dealing with such <phrase>data</phrase>, taking into account both the <phrase>cpu</phrase> time and the user <phrase>interactivity</phrase>. this <phrase>paper</phrase> describes the techniques employed: (a) the <phrase>construction</phrase> of a <phrase>quadtree</phrase> that allows to eliminate gaps by <phrase>interpolating</phrase> available 3d <phrase>data</phrase>, (b) a first but coarse visualization at a <phrase>high</phrase> <phrase>tree</phrase> level in <phrase>order</phrase> to rapidly change or adjust the <phrase>region</phrase> of interest, and (c) a very efficient <phrase>triangulation</phrase> (mesh reduction) that allows for a fast <phrase>interactivity</phrase> even at the highest detail level. by using one <phrase>single</phrase> <phrase>octree</phrase>, all processing can be combined because (1) gaps can be filled by <phrase>interpolation</phrase> since they are smaller at higher <phrase>tree</phrase> levels, and (2) connected components can be projected down the <phrase>tree</phrase> and refined using the <phrase>data</phrase> available there. as a result, huge <phrase>data</phrase> sets can be visualized in near realtime on normally-sized discrete grids using shading instead of <phrase>wire</phrase>-frames, and this enables a fast searching for objects in the seabottom. real <phrase>cpu</phrase> times are presented for a real <phrase>sonar</phrase> <phrase>data set</phrase> which is visualized at a low resolution, showing the overall shape of the seabottom, and at a <phrase>high</phrase> resolution, showing a (semi-)buried pipeline. in <phrase>order</phrase> to detect an object at such a <phrase>high</phrase> resolution additional techniques are applied to the <phrase>data</phrase>: (a) an interslice <phrase>interpolation</phrase> in <phrase>order</phrase> to cope with the increased <phrase>data</phrase> sparseness and (b) a maximum-homogeneity filtering in <phrase>order</phrase> to cope with the decreased <phrase>signal-to-noise-ratio</phrase>. after the extraction of the pipeline a thinning technique is applied in <phrase>order</phrase> to be able to quantify its length.
visualizing i/o predictability.
3d <phrase>performance capture</phrase> for facial <phrase>animation</phrase>.
this <phrase>paper</phrase> describes how a <phrase>photogrammetry</phrase> based 3d capture system can be used as an <phrase>input device</phrase> for <phrase>animation</phrase>. the 3d dynamic capture system is used to capture the motion of a <phrase>human</phrase> face which is extracted from a <phrase>sequence</phrase> of 3d models captured at <phrase>tv</phrase> <phrase>frame rate</phrase>. initially the positions of a set of landmarks on the face are extracted. these landmarks are then used to provide motion <phrase>data</phrase> in two different ways. first, a <phrase>high</phrase> level description of the movements are extracted, and these can be used as input to a procedural <phrase>animation</phrase> package (i.e. creatoon). second the landmarks can be used as registration points for a conformation process where the <phrase>model</phrase> to be <phrase>animated</phrase> is modified to match the captured <phrase>model</phrase>. this approach gives a new <phrase>sequence</phrase> of models which have the structure of the drawn <phrase>model</phrase> but the movement of the captured <phrase>sequence</phrase>.
on robustness and localization accuracy of <phrase>optical flow</phrase> computation from color imagery.
accurate and efficient <phrase>optical flow</phrase> estimation is a <phrase>major</phrase> step in many computational vision problems, including tracking and 2-d/3-d mapping applications. processing of <phrase>grayscale</phrase> images has been the dominant approach, with only a few studies investigating selected aspects in the use of color imagery. in a <phrase>physics</phrase>-based analysis to study the impact of the spectral-dependent medium <phrase>attenuation</phrase> on the color channels, we have shown merit in the use of color cues in the computation of <phrase>optical flow</phrase> for underwater imagery-the primary <phrase>motivation</phrase> of the investigation [robust <phrase>optical flow</phrase> estimation using underwater <phrase>color images</phrase>]. comparisons among various color representations and traditional intensity component on the <phrase>optical flow</phrase> computation are given, suggesting that the <phrase>hsv</phrase> representation could be the most suitable. for both underwater and <phrase>terrestrial</phrase> imagery, even where <phrase>data</phrase> in the 3 color channels are highly correlated, one expects multiple constraints from color channels to give increased robustness due to the <phrase>independent</phrase> <phrase>channel</phrase> noises. <phrase>results</phrase> of experiments are given to demonstrate improved localization and accuracy.
<phrase>data</phrase>-driven approaches to <phrase>digital</phrase> <phrase>human</phrase> modeling.
<phrase>data</phrase>-driven approach is an appealing way to depict people in a <phrase>virtual world</phrase>. the captured shape and movement <phrase>data</phrase> from real people are structured and combined to reproduce or create new samples in an intuitive and controllable way. we focus on the <phrase>body shape</phrase> modeling and elucidate the issues related to <phrase>data</phrase>-driven methods. the difficulty of adopting <phrase>data</phrase>-driven approach for <phrase>human body</phrase> shape modeling is due in part to the intrinsic articulated structure of the body. since such internal structure is not measured with most of existing capture devices available today, it has to be calculated through estimation. we develop a framework for collecting and managing <phrase>range</phrase> scan <phrase>data</phrase> that automatically estimates this structure from user-tagged landmarks. by framing the captured and structurally annotated <phrase>data</phrase> so that statistic implicit is exploited for synthesizing new body shapes, our technique support time-saving generation of animatable body models with <phrase>high</phrase> realism.
<phrase>spacetime</phrase>-coherent <phrase>geometry</phrase> <phrase>reconstruction</phrase> from multiple <phrase>video</phrase> streams.
by reconstructing time-varying <phrase>geometry</phrase> one frame at a time, one ignores the continuity of natural motion, wasting useful <phrase>information</phrase> about the underlying <phrase>video</phrase>-image formation process and taking into account temporally discontinuous <phrase>reconstruction</phrase> <phrase>results</phrase>. in 4d <phrase>spacetime</phrase>, the surface of a dynamic object describes a continuous 3d hyper-surface. this hyper-surface can be implicitly defined as the minimum of an <phrase>energy</phrase> functional designed to optimize photo-consistency. based on an <phrase>euler</phrase>-lagrange reformulation of the problem, we find this hyper-surface from a handful of synchronized <phrase>video</phrase> recordings. the resulting object <phrase>geometry</phrase> varies smoothly over time, and intermittently invisible object regions are correctly interpolated from previously and/or future frames.
3wps : a 3d <phrase>web-based</phrase> process visualization framework.
a <phrase>gesture recognition</phrase> system using 3d <phrase>data</phrase>.
concentric <phrase>strips</phrase>: <phrase>algorithms</phrase> and <phrase>architecture</phrase> for the compression/<phrase>decompression</phrase> of <phrase>triangle</phrase> meshes.
description of simple method in 3d <phrase>reconstruction</phrase> in <phrase>medical imaging</phrase>.
encoding volumetric grids for streaming <phrase>isosurface</phrase> extraction.
gridded volumetric <phrase>data</phrase> sets representing <phrase>simulation</phrase> or tomography output are commonly visualized by displaying a triangulated <phrase>isosurface</phrase> for a particular isovalue. when the grid is stored in a standard format, the entire volume must be loaded from disk, even though only a fraction of the grid cells may intersect the <phrase>isosurface</phrase>. we propose a compressed on-disk representation for regular volume grids that allows streaming, i/o-efficient, out-of-core <phrase>isosurface</phrase> extraction. unlike previous methods, we provide a guaranteed bound on the ratio between the number of cells loaded and number of cells intersecting the <phrase>isosurface</phrase> that applies for any isovalue. as grid cells are decompressed, we immediately extract vertices and <phrase>triangles</phrase> of the <phrase>isosurface</phrase>. our output is a coherent streaming mesh, which facilitates subsequent processing, including on-the-<phrase>fly</phrase> simplification and compression.
object shape modelling from <phrase>multiple range</phrase> images by matching signed distance fields.
filling the signed distance field by fitting local quadrics.
we propose a method of filling unmeasured regions of shape models integrated from multiple measurements of surface shapes. we use the signed distance field (<phrase>sdf</phrase>) as shape representation that contains <phrase>information</phrase> of the <phrase>surface normal</phrase> along with the signed distance at the <phrase>closest point</phrase> on the surface from the sampling point. we solve this problem by iteratively fitting <phrase>quadratic function</phrase> to generate smoothly connected <phrase>sdf</phrase>. we analyzed the relationship between the quadratic coefficients and the surface <phrase>curvature</phrase>, and by using the coefficients, we evenly propagated the <phrase>sdf</phrase> so that it satisfies the constraints of the field. <phrase>the proposed method</phrase> was tested on synthetic <phrase>data</phrase> and real <phrase>data</phrase> that was generated by integrating <phrase>multiple range</phrase> images.
generation, visualization, and <phrase>editing</phrase> of 3d <phrase>video</phrase>.
effectivity of spherical object <phrase>reconstruction</phrase> using <phrase>star</phrase> -shaped <phrase>simplex</phrase> meshes.
<phrase>high</phrase>-resolution cytometry network project: client/server system for 3d <phrase>optical microscope</phrase> <phrase>data</phrase> storage and analysis.
if several laboratories work in the same field and need to cooperate, it is necessary to exchange and mutually compare their computer <phrase>results</phrase>. the quick way to solve this problem is to unite their file formats (if it is practicable) or to install new <phrase>software</phrase> systems, which are <phrase>data</phrase> or <phrase>file format</phrase> compatible. a much better <phrase>solution</phrase> is to use only one common system. this article describes the <phrase>architecture</phrase> of a new client-server system, which offers possibilities of effective cooperation for laboratories doing the <phrase>research</phrase> in the field of the spatial <phrase>organization</phrase> of <phrase>human genome</phrase>. however, the system <phrase>design</phrase> is determined mainly by <phrase>optical microscopy</phrase> principles and therefore the system can easily be modified for use in other applications or environments. the most pressing problem during the system <phrase>design</phrase> was how to share and process large 3d image <phrase>data</phrase> in client-server <phrase>architecture</phrase>.
a <phrase>model</phrase> (in)validation approach to <phrase>gait</phrase> recognition.
edge-constrained <phrase>marching</phrase> <phrase>triangles</phrase>.
enhanced <phrase>surface reconstruction</phrase> from wide baseline images.
this <phrase>paper</phrase> deals with the problem of dense matching from <phrase>stereo</phrase> images under wide baseline conditions. by considering the characteristic properties of widely separated views, we propose an extension to a recently published <phrase>algorithm</phrase> for detailed <phrase>reconstruction</phrase> of continuous surfaces. the <phrase>algorithm</phrase> compensates for the occurrent affine <phrase>distortion</phrase> between the views to allow low level intensity based comparison necessary for dense matching. the matching itself is performed by an enhanced <phrase>region</phrase> <phrase>rowing</phrase> based affine propagation method that takes surface <phrase>distortion</phrase> into account to handle complex <phrase>piecewise</phrase>-smooth surfaces. it is experimentally shown that this new method can achieve smooth and accurate <phrase>reconstruction</phrase> from wide baseline images of both indoor and outdoor scenes. to quantify the <phrase>reconstruction</phrase> <phrase>results</phrase> we have created realistic synthetic datasets with <phrase>ground truth</phrase>. these datasets form the core of a future <phrase>testbed</phrase> for comparison of different wide baseline <phrase>surface reconstruction</phrase> techniques. in the current study, <phrase>results</phrase> on the synthetic images are compared to the <phrase>ground truth</phrase> to measure the accuracy of our method.
an easy viewer for out-of-core visualization of huge point-sampled models.
in this <phrase>paper</phrase>, we propose a viewer for huge point-sampled models by combining out-of-core technologies with view-dependent level-of-detail (<phrase>lod</phrase>) control. this viewer is designed on the basis of a multiresolution <phrase>data structure</phrase> we have developed for gaze-guided visualization and transmission of 3d point sets. in <phrase>order</phrase> to reduce <phrase>memory</phrase> loads for huge point sets on <phrase>general</phrase> <phrase>pc</phrase> platforms, we introduce a <phrase>partition</phrase>-based out-of-core strategy to balance usage of main and external memories. at first, the <phrase>data</phrase> surface is partitioned into small blocks and points in each block are reorganized into error-controlled lods by <phrase>hierarchical clustering</phrase> and <phrase>lod</phrase> <phrase>organization</phrase>. in the interactive rendering process, a <phrase>data</phrase> <phrase>block scheduling</phrase> <phrase>algorithm</phrase> is used to realize the view-dependent <phrase>paging</phrase>. <phrase>experimental</phrase> <phrase>results</phrase> show that the viewer can perform interactive visualization of huge point models on <phrase>commodity</phrase> graphics platforms with ease.
direct and robust voxelization and polygonization of <phrase>free</phrase>-form csg solids.
network-based raditional <phrase>japanese</phrase> crafting presentation system using agent and <phrase>virtual reality</phrase> technologies.
network-based raditional <phrase>japanese</phrase> crafting presentation system using agent and <phrase>virtual reality</phrase> echnologies.
3d objects visualization for remote interactive <phrase>medical</phrase> applications.
adaptive space carving.
in this work, we present an adaptive space carving method for scene <phrase>reconstruction</phrase> from a set of images obtained from low cost calibrated webcams. our method uses a combination of <phrase>silhouette</phrase> and <phrase>photometric</phrase> <phrase>information</phrase> to efficiently carve the shape of the observed scene out of a volumetric space represented by an <phrase>octree</phrase> <phrase>data structure</phrase>. in this method different resolutions are considered both in object space and in image space. this <phrase>led</phrase> us to adopt a strategy in which the <phrase>information</phrase> used by the photo-consistency <phrase>test</phrase> is registered in scene space by <phrase>projective</phrase> <phrase>texture mapping</phrase>. another important question addressed in this work is the <phrase>high</phrase> level of noise present in low cost webcams. to deal with this problem we devised a statistical photo-consistency <phrase>test</phrase> that uses statistical estimators for the noise introduced by the sensors of the cameras.
dense <phrase>multiple view</phrase> <phrase>stereo</phrase> with <phrase>general</phrase> <phrase>camera</phrase> placement using <phrase>tensor</phrase> voting.
we present a computational framework for the inference of dense descriptions from <phrase>multiple view</phrase> <phrase>stereo</phrase> with <phrase>general</phrase> <phrase>camera</phrase> placement. thus far <phrase>research</phrase> on dense <phrase>multiple view</phrase> <phrase>stereo</phrase> has evolved along three axes: computation of scene approximations in the form of visual hulls; merging of depth maps derived from simple configurations, such as <phrase>binocular</phrase> or trinocular; and <phrase>multiple view</phrase> <phrase>stereo</phrase> with restricted <phrase>camera</phrase> placement. these approaches are either sub-optimal, since they do not maximize the use of available <phrase>information</phrase>, or cannot be applied to <phrase>general</phrase> <phrase>camera</phrase> configurations. our approach does not involve <phrase>binocular</phrase> processing other than the detection of tentative <phrase>pixel</phrase> correspondences. we require calibration <phrase>information</phrase> for all cameras and that there exist <phrase>camera</phrase> pairs which enable automatic <phrase>pixel</phrase> matching. the inference of scene surfaces is based on the premise that correct <phrase>pixel</phrase> correspondences, reconstructed in 3-d, form salient, coherent surfaces, while wrong correspondences form less coherent structures. the <phrase>tensor</phrase> voting framework is suitable for this task since it can process the very large datasets we generate with reasonable <phrase>computational complexity</phrase>. we show <phrase>results</phrase> on real images that present numerous challenges.
fast 3d <phrase>model</phrase> acquisition from <phrase>stereo</phrase> images.
<phrase>bayesian</phrase> <phrase>surface reconstruction</phrase>.
in this <phrase>paper</phrase> we illustrate an innovative method which estimates the surfaces -modelled as polygonal meshes-bounding objects present in a scene, viewed by arbitrarily placed cameras. we present a montecarlo based iterative approach which, at every step, increases its <phrase>knowledge</phrase> about the scene sampling the unknown volume around the current estimation. then, the samples which mostly appear to be consistent with the measurements are used to extend the mesh representing the surface. the <phrase>reconstruction</phrase> is regularized applying a filter -based on a dynamic system- to the mesh. this operation will preserve the <phrase>high</phrase> <phrase>curvature</phrase> areas of the surface, while smoothing away the noise in the estimation.
a non causal <phrase>bayesian</phrase> framework for object tracking and occlusion handling for the synthesis of <phrase>stereoscopic</phrase> <phrase>video</phrase>.
this <phrase>paper</phrase> presents a framework for the synthesis of <phrase>stereoscopic</phrase> <phrase>video</phrase> using as input only a monoscopic image <phrase>sequence</phrase>. initially, bi-directional 2d <phrase>motion estimation</phrase> is performed, which is followed by an efficient method for the reliable tracking of object contours. rigid 3d motion and structure is recovered utilizing extended kalman filtering. finally, occlusions are dealt with a novel <phrase>bayesian</phrase> framework, which exploits future <phrase>information</phrase> to correctly reconstruct occluded areas. <phrase>experimental</phrase> evaluation shows that the layered object scene representation, combined with the proposed methods for object tracking throughout the <phrase>sequence</phrase> and occlusion handling, yields very accurate <phrase>results</phrase>.
exploring boundary concavities in active contours and surfaces.
active contours and surfaces are deformable models used for 2d and 3d <phrase>image segmentation</phrase>. these models generally lack of convergence into boundary concavities, when segmenting highly concave objects. in this <phrase>paper</phrase>, we propose a method for exploring concavities, applicable on discrete active contours and surfaces deformed thanks to the <phrase>greedy algorithm</phrase>, which minimizes the <phrase>energy</phrase> functional. as a basis of this method, we introduce the <phrase>gradient</phrase> line <phrase>energy</phrase> (gle), which goal is to make discrete parts of the surface coincide with boundaries, and the exploration force, computed from this <phrase>energy</phrase>, driving the surface into the object concavities. our method lies on a <phrase>general</phrase> framework, enabling application on discrete 2d contours and 3d meshes with easy adaptation between both models.
segmenting correlation <phrase>stereo</phrase> <phrase>range</phrase> images using surface elements.
this <phrase>paper</phrase> describes methods for segmenting planar surfaces from noisy 3d <phrase>data</phrase> obtained from correlation <phrase>stereo</phrase> vision. we make use of local planar surface elements called patchlets. patchlets have 3d position, orientation and size parameters. as well, they have positional confidence measures based on the <phrase>stereo</phrase> <phrase>sensor</phrase> <phrase>model</phrase>. patchlet orientations (i.e., <phrase>surface normals</phrase>) provide important additional dimensionality that reduces the ambiguity of segmentation-by-clustering. patchlet size allows the use of continuity or coverage constraints when segmenting bounded surfaces from depth images. we use a <phrase>region</phrase>-growing approach to identify the number of surfaces that exist in a <phrase>stereo</phrase> image and obtain an initial estimate of the surface parameters. we refine segmentation using a <phrase>maximum likelihood</phrase> clustering approach that is optimised with expectation-maximisation. confidence measures on the patchlet parameters allow proper weighting of patchlet contributions to the <phrase>solution</phrase>. we provide <phrase>experimental</phrase> <phrase>results</phrase> of the segmentation on complex outdoor scenes.
viewpoint consistent texture synthesis.
the purpose of this work is to synthesize textures of rough, <phrase>real world</phrase> surfaces under freely chosen viewing and illumination directions. moreover, such textures are <phrase>produced</phrase> for continuously changing directions in such a way that the different textures are mutually consistent, i.e. emulate the same piece of surface. this is necessary for 3d <phrase>animation</phrase>. it is assumed that the mesostructure (small-scale) <phrase>geometry</phrase> of a surface is not known, and that the only input consists of a set of images, taken under different viewing and illumination directions. these are automatically aligned to build an appropriate bidirectional texture <phrase>function</phrase> (btf). directly extending 2d synthesis methods for <phrase>pixels</phrase> to complete btf columns has drawbacks which are exposed, and a <phrase>superior</phrase> sequential but highly parallelizable <phrase>algorithm</phrase> is proposed. examples demonstrate the quality of the <phrase>results</phrase>.
a hierarchy of cameras for 3d <phrase>photography</phrase>.
the view-<phrase>independent</phrase> visualization of 3d scenes is most often based on rendering accurate 3d models or utilizes <phrase>image-based</phrase> rendering techniques. to compute the 3d structure of a scene from a moving vision <phrase>sensor</phrase> or to use <phrase>image-based</phrase> rendering approaches, we need to be able to estimate the motion of the <phrase>sensor</phrase> from the recorded image <phrase>information</phrase> with <phrase>high</phrase> accuracy, a problem that has been well-studied. in this work, we investigate the relationship between <phrase>camera</phrase> <phrase>design</phrase> and our ability to perform accurate 3d <phrase>photography</phrase>, by examining the influence of <phrase>camera</phrase> <phrase>design</phrase> on the estimation of the motion and structure of a scene from <phrase>video</phrase> <phrase>data</phrase>. by relating the differential structure of the time varying plenoptic <phrase>function</phrase> to different known and new <phrase>camera</phrase> designs, we can establish a hierarchy of cameras based upon the stability and complexity of the computations necessary to estimate structure and motion. at the low end of this hierarchy is the standard planar <phrase>pinhole camera</phrase> for which the <phrase>structure from motion</phrase> problem is non-linear and ill-posed. at the <phrase>high</phrase> end is a <phrase>camera</phrase>, which we call the full field of view polydioptric <phrase>camera</phrase>, for which the <phrase>motion estimation</phrase> problem can be solved independently of the depth of the scene which leads to fast and robust <phrase>algorithms</phrase> for 3d <phrase>photography</phrase>. in between are <phrase>multiple view</phrase> cameras with a large field of view which we have built, as well as omni-directional sensors.
automatic passive recovery of 3d from images and <phrase>video</phrase>.
this <phrase>paper</phrase> gives a summary of automatic passive <phrase>reconstruction</phrase> of 3d scenes from images and <phrase>video</phrase>. features are tracked between the images. relative <phrase>camera</phrase> poses are then estimated based on the feature tracks. both the feature tracking and the robust method used to estimate the <phrase>camera</phrase> poses has recently been shown to provide robust real-time performance. given the <phrase>camera</phrase> poses, a more elaborate method is used to derive dense textured surfaces. the dense <phrase>reconstruction</phrase> method was originally part of the authors <phrase>thesis</phrase> [automatic dense <phrase>reconstruction</phrase> from uncalibrated <phrase>video</phrase> sequences]. it first computesdepth maps using <phrase>graph</phrase> cuts, optimizing a <phrase>bayesian</phrase> formulation. the <phrase>graph</phrase> cut operation is used to let <phrase>depth map</phrase> hypotheses from various sources compete in <phrase>order</phrase> to optimize the cost <phrase>function</phrase>. the <phrase>hypothesis</phrase> generators used are <phrase>feature based</phrase> surface <phrase>triangulation</phrase>, plane fitting and multi-<phrase>hypothesis</phrase> multi-scale patch-based <phrase>stereo</phrase>. the requirements for a cost <phrase>function</phrase> to fit into the <phrase>graph</phrase> cut framework were already given in [automatic dense <phrase>reconstruction</phrase> from uncalibrated <phrase>video</phrase> sequences], along with a procedure for constructing the <phrase>graph</phrase> weights corresponding to any cost <phrase>function</phrase> that satisfies the requirements. depth maps from many different viewpoints are then robustly fused to give a consistent global result using a process called median <phrase>fusion</phrase>. the robustness of the median <phrase>fusion</phrase> is important. it departs from the too common practice of fusing <phrase>results</phrase> by either using the <phrase>union</phrase> of <phrase>free</phrase>-space constraints as the resulting <phrase>free</phrase>-space, or the <phrase>union</phrase> of all predicted surfaces as the resulting surfaces, which is tremendously error-prone, since it essentially corresponds to taking either the minimum or maximum of the individual <phrase>results</phrase>.
heterogeneous deformation <phrase>model</phrase> for 3d shape and motion recovery from multi-viewpoint images.
this <phrase>paper</phrase> presents a framework for dynamic 3d shape and motion <phrase>reconstruction</phrase> from multi-viewpoint images using a deformable <phrase>mesh model</phrase>. by deforming a mesh at a frame to that at the next frame, we can obtain both 3d shape and motion of the object simultaneously. the deformation process of our <phrase>mesh model</phrase> is heterogeneous. each vertex changes its deformation process according to its 1) <phrase>photometric</phrase> <phrase>property</phrase> (i.e., if it has prominent texture or not), and 2) <phrase>physical property</phrase> (i.e., if it is an element of rigid part of the object or not). this heterogeneous deformation <phrase>model</phrase> enables us to reconstruct the object which consists of different kinds of materials or parts with different motion models, e.g., rigidly acting body parts and deforming soft clothes or its skins, by a <phrase>single</phrase> and unified computational framework.
the influence of shape on image correspondence.
we examine the implications of shape on the process of finding dense correspondence and half-occlusions for a <phrase>stereo</phrase> pair of images. the desired <phrase>property</phrase> of the <phrase>depth map</phrase> is that it should be a <phrase>piecewise</phrase> <phrase>continuous function</phrase> which is consistent with the images and which has the minimum number of discontinuities. to zeroeth <phrase>order</phrase>, <phrase>piecewise</phrase> continuity becomes <phrase>piecewise</phrase> constancy. using this approximation, we first discuss an approach for dealing with such a fronto-parallel shapeless world, and the problems involved therein. we then introduce horizontal and vertical <phrase>slant</phrase> to create a first <phrase>order</phrase> approximation to <phrase>piecewise</phrase> continuity. we highlight the fact that a horizontally slanted surface (ie. having depth variation in the direction of the separation of the two cameras) will appear horizontally stretched in one image as compared to the other image. thus, while corresponding two images, n <phrase>pixels</phrase> on a scanline in one image may correspond to a different number of <phrase>pixels</phrase> m in the other image, which has consequences with regard to sampling and occlusion detection. we also discuss the asymmetry between vertical and horizontal <phrase>slant</phrase>, and the central role of non-horizontal edges in the context of vertical <phrase>slant</phrase>. using experiments, we discuss cases where existing <phrase>algorithms</phrase> fail, and how the incorporation of new constraints provides correct <phrase>results</phrase>.
registration of <phrase>range</phrase> images that preserves local surface structures and color.
we propose an <phrase>icp</phrase>-based registration method for <phrase>range</phrase> images that preserves fundamental features, i.e., local structures and color, of object surfaces. the method employs local surfaces as an attribute for establishing correspondences between <phrase>range</phrase> images where local surfaces are evaluated geometrically and photometrically. in estimating correspondences between <phrase>range</phrase> images, our method evaluates consistency of shape patterns and <phrase>chromaticity</phrase> of local surfaces together. in estimating transformation parameters relating the coordinates between different <phrase>range</phrase> images, on the other hand, our method evaluates <phrase>skewness</phrase> and <phrase>chromaticity</phrase> of correspondences. these two kinds of evaluation enhances accuracy of the estimation and <phrase>results</phrase> in preserving local structures and color of object surfaces.
<phrase>construction</phrase> of <phrase>animal</phrase> models and motion synthesis in 3d <phrase>virtual environments</phrase> using <phrase>image sequences</phrase>.
in this <phrase>paper</phrase>, we describe a system that can build 3d <phrase>animal</phrase> models and synthesize animations in 3d <phrase>virtual environments</phrase>. the <phrase>model</phrase> is constructed by 2d images captured by specific views. the <phrase>animation</phrase> is synthesised by using physical motion models of the <phrase>animal</phrase> and tracking <phrase>data</phrase> from <phrase>image sequences</phrase>. finally, the user selects some points of the 3d world and a smooth and safe motion path, which <phrase>passes</phrase> by these points, is created. the main assumption of the 3d modelling is that the <phrase>animal</phrase> could be divided into parts whose normal sections are ellipses. joints and angles between <phrase>skeleton</phrase> points are used in <phrase>order</phrase> to decrease models complexity. using the above methodology, a <phrase>snake</phrase>, a <phrase>lizard</phrase> and a <phrase>goat</phrase> are reconstructed.
the virtual boutique: a synergic approach to <phrase>virtualization</phrase>, <phrase>content-based</phrase> <phrase>management</phrase> of 3d <phrase>information</phrase>, 3d <phrase>data mining</phrase> an <phrase>virtual reality</phrase> for <phrase>e-commerce</phrase>.
specularity elimination in <phrase>range</phrase> sensing for accurate 3d modeling of <phrase>specular</phrase> objects.
we present a novel <phrase>range</phrase> sensing method that is capable of constructing accurate 3dmodels of <phrase>specular</phrase> objects. our method utilizes a new <phrase>range</phrase> imaging concept called multi-<phrase>peak</phrase> <phrase>range</phrase> imaging, which accounts for the effects of mutual reflections. false measurements generated by mutual reflections are then eliminated by applying a series of constraint <phrase>tests</phrase> based on local <phrase>smoothness</phrase>, global coordinate consistency and visibility consistency. we show the usefulness of our method by applying the method to three real objects with <phrase>specular</phrase> surfaces. the <phrase>ground truth</phrase> <phrase>data</phrase> for those three objects were also acquired in <phrase>order</phrase> to evaluate the elimination of false measurements and to justify the selection of the parameters in the constraint <phrase>tests</phrase>. <phrase>experimental</phrase> <phrase>results</phrase> indicate that our method significantly improves upon the traditional methods for constructing reliable 3d models of <phrase>specular</phrase> objects with complex shapes.
3d mesh <phrase>wavelet</phrase> coding using efficient <phrase>model</phrase>-based <phrase>bit</phrase> allocation.
3d non-linear invisible boundary detection filters.
the <phrase>human</phrase> vision system can discriminate regions which differ up to the second <phrase>order</phrase> <phrase>statistics</phrase> only. a lot of <phrase>malignant</phrase> tumours have boundaries which are not visible to the <phrase>human eye</phrase>. we present an <phrase>algorithm</phrase> designed to reveal "hidden" boundaries in grey level images, by <phrase>computing</phrase> gradients in higher <phrase>order</phrase> <phrase>statistics</phrase> of the <phrase>data</phrase>. we demonstrate it by applying it to the identification of possible "hidden" boundaries of gliomas as manifest themselves in <phrase>mri</phrase> 3d scans.
surface segmentation using <phrase>geodesic</phrase> centroidal tesselation.
in this <phrase>paper</phrase>, we solve the problem of mesh <phrase>partition</phrase> using intrinsic computations on the 3d surface. the key concept is the notion of centroidal tesselation that is widely used in an eucidan settings. using the fast <phrase>marching</phrase> <phrase>algorithm</phrase>, we are able to recast this powerful tool in the <phrase>language</phrase> of mesh processing. this method naturally fits into a framework for 3d <phrase>geometry</phrase> modelling and processing that uses only fast <phrase>geodesic</phrase> computations. with the use of <phrase>classical</phrase> <phrase>geodesic</phrase>-based building blocks, we are able to take into account any available <phrase>information</phrase> or requirement such as a 2d texture or the <phrase>curvature</phrase> of the surface.
detection and compensation of image <phrase>sequence</phrase> <phrase>jitter</phrase> due to an unstable <phrase>ccd camera</phrase> for <phrase>video</phrase> tracking of a moving <phrase>target</phrase>.
nowadays image motion analysis is considered as a significant subject in <phrase>image processing</phrase> and according to its characteristics is divided into different categories. this <phrase>paper</phrase> focuses on <phrase>camera</phrase> <phrase>jitter</phrase> as fast translations in image <phrase>sequence</phrase> due to an unstable platform. our studies have <phrase>led</phrase> to do correlation matching [motion displacement estimation using an affine modelfor image matching] between well featured templates[<phrase>edge detection</phrase> using <phrase>kl</phrase> transform, extracting good features for <phrase>motion estimation</phrase>] of successive frames to prevent <phrase>aperture</phrase> problems and thus have good estimation of the translations. but <phrase>camera</phrase> <phrase>vibration</phrase> may also suffer from rotation and scaling as nonlinear motions [detection of non-uniform motion in <phrase>image sequences</phrase> using a reduced <phrase>order</phrase> <phrase>likelihood ratio test</phrase>] that may defect thematching process. therefore mapping of 3d image points into <phrase>camera</phrase> image plane has been investigated to trace the effects of these nonlinear parameters in 2d motion equations [<phrase>image based</phrase> measurement systems]. now by approximating these equations for minute rotation and bounded scaling between frame i and i+1, the set of motion equations with four unknowns (rotation, scaling, horizontal /vertical translations) could be solved just by <phrase>lse</phrase> [kalman filtering, theory and practice using <phrase>matlab</phrase>] method for a unique and optimized response. after all when you apply the estimated displacements on each frame for compensation, an stabilized <phrase>sequence</phrase> will conclude.
surface <phrase>illuminance</phrase> flow.
we introduce the concept of "surface <phrase>illuminance</phrase> flow" in two steps.first we reiterate the notion of the "<phrase>light</phrase> <phrase>vector</phrase>", then we proceed to decompose the <phrase>light</phrase> <phrase>vector</phrase> into a scalar "normal <phrase>illuminance</phrase>" and a <phrase>vector</phrase> "surface <phrase>illuminance</phrase> flow".the scalar normal <phrase>illuminance</phrase> generates the familiar <phrase>illuminance</phrase> pattern that is often known as the "shading".the <phrase>vector</phrase> surface <phrase>illuminance</phrase> flow is irrelevant for smooth surfaces (which probably explains why it is conventionally ignored) but it generates the texture due to surface mesorelief.we show how observation of the <phrase>illuminance</phrase> induced texture leads to robust inferences of the surface <phrase>illuminance</phrase> flow direction <phrase>modulo</phrase> 180. this makes surface <phrase>illuminance</phrase> flow a <phrase>property</phrase> that is <phrase>observable</phrase> in natural scenes.muchl ike <phrase>optical flow</phrase>, the surface <phrase>illuminance</phrase> flow is largely due to a global <phrase>property</phrase> that exists <phrase>independent</phrase> of local structure, namely the direction of the <phrase>general</phrase> illuminating <phrase>beam</phrase> (<phrase>sunlight</phrase> or <phrase>light</phrase> from the overcast sky in a typical outdoors scene).however, due to the fact that there exists "true screening" in <phrase>radiometry</phrase> (thus removing any hope for a "<phrase>field theory</phrase>" of <phrase>radiometry</phrase>) the local scene structure has an influence on the local <phrase>light</phrase> field.such "<phrase>vignetting</phrase>" is a main cause of the complexity of <phrase>radiometry</phrase>, the other cause being the ever present multiple scatterings.this complicates matters but also introduces additional sources of information.we illustrate instances of the observation of <phrase>illuminance</phrase> flow in scenes.we also present a <phrase>data</phrase> base of roughly spherical objects, <phrase>illuminated</phrase> in a number of standard ways that we have used to study <phrase>illuminance</phrase> flow over actual rough objects.
interactive modeling from dense color and sparse depth.
we present <phrase>research</phrase> in scene modeling. the task is to build <phrase>digital</phrase> models of natural scenes that support interactive, <phrase>photorealistic</phrase> rendering. scene modeling is the bottleneck in many computer graphics applications, notably virtual training, geometric modeling for physical <phrase>simulation</phrase>, <phrase>cultural heritage</phrase> preservation, <phrase>internet marketing</phrase>, and gaming. capturing complex scenes with current modeling <phrase>technology</phrase> is slow, difficult, and expensive. we describe an interactive modeling system that has the potential to solve these problems.
comparison of 3d measurement techniques in <phrase>cultural heritage</phrase> application: user point of view.
3d <phrase>model</phrase> retrieval based on 2d slice similarity measurements.
<phrase>volume rendering</phrase> on the <phrase>internet</phrase>.
<phrase>surface normals</phrase> and height from non-lambertian image <phrase>data</phrase>.
it is well known that many surfaces exhibit <phrase>reflectance</phrase> that is not well modelled by lambert's <phrase>law</phrase>. this is the case not only for surfaces that are rough or shiny, but also those that are matte and composed of materials that are <phrase>particle</phrase> suspensions. as a result, standard lambertian shape-from-shadingmethods can not be applied directly to the analysis of rough and shiny surfaces. in <phrase>order</phrase> to overcome this difficulty, in this <phrase>paper</phrase>, weconsider how to reconstruct the lambertian component for rough and shiny surfaceswhen the object is <phrase>illuminated</phrase> in the viewing direction. to do this we make use of the diffuse <phrase>reflectance</phrase> models described by oren and nayar, and by wolff. our experiments with <phrase>synthetic and real</phrase>-world <phrase>data</phrase> reveal the effectiveness of the correction method, leading to improved <phrase>surface normal</phrase> and height recovery.
testing <phrase>reflectance</phrase> models against <phrase>radiance</phrase> <phrase>data</phrase>.
this <phrase>paper</phrase> describes an empirical investigation of departures from lambert's <phrase>law</phrase> for rough and shiny surfaces. we commence by using a recently reported method to recover estimates ofthe surface <phrase>radiance</phrase> <phrase>function</phrase> for objects <phrase>illuminated</phrase> in the viewer direction. this method is non-parametric, and offers the advantage that it is simple to use since it does not require detailed <phrase>camera</phrase> calibration. we compare the <phrase>radiance</phrase> <phrase>data</phrase> with a number of phenomenological and <phrase>physics</phrase>-basedreflectance models. the models studied include those of oren-nayar, wolff and different variants of the beckmann <phrase>model</phrase>. the main conclusion of the study is that among these models the best fit to the empirical <phrase>data</phrase> is found to be the wolff <phrase>model</phrase> for smooth objects and the modified beckmann <phrase>model</phrase> for rough objects.
a unified approach for motion analysis and <phrase>view synthesis</phrase>.
<phrase>image based</phrase> rendering (ibr) consists of several steps: (i) calibration (or ego-motion computation) of all input images. (<phrase>ii</phrase>) determination of regions in the input images used to synthesize the new view. (iii) <phrase>interpolating</phrase> the new view from the selected areas of the input images. we propose a unified representation for all these aspects of ibr using the space-time (x-y-t) volume. the presented approach is very robust, and allows to use ibr in <phrase>general</phrase> conditions even with a <phrase>hand-held camera</phrase>. to take care of (i), the space-time volume is constructed by placing frames at locations along the time <phrase>axis</phrase> so that image features create straight lines in the epi (epipolar plane images). different slices of the space-time volume are used to produce new views, taking care of (<phrase>ii</phrase>). step (iii) is done by <phrase>interpolating</phrase> between image samples using the feature lines in the epi images. ibr examples are shown for various cases: sequences taken from a driving <phrase>car</phrase>, from a <phrase>hand held camera</phrase>, or when using a <phrase>tripod</phrase>.
a surface partitioning <phrase>spectrum</phrase> (<phrase>sps</phrase>) for retrieval and indexing of 3d <phrase>cad</phrase> models.
manual indexing of large <phrase>databases</phrase> of geometric <phrase>information</phrase> is costly and often impracticable. because of this <phrase>research</phrase> into retrieval and indexing schemes has focused on the development of various 3d to 2d mappings that characterise a shape as a <phrase>histogram</phrase> with a small number of parameters. many methods of generating such 2d signatures (i.e. histograms) have been proposed, generally based on geometric measures of say <phrase>curvature</phrase> or distance. however these geometric signatures lack <phrase>information</phrase> about <phrase>topology</phrase> and tend to become indistinct as the complexity of the shape increases. this <phrase>paper</phrase> describes a new method for characterising both the <phrase>geometry</phrase> and <phrase>topology</phrase> of shapes in a <phrase>single</phrase> 2d <phrase>graph</phrase>, the surface partitioning <phrase>spectrum</phrase> (<phrase>sps</phrase>). we evaluate the effectiveness of using the <phrase>sps</phrase> with a <phrase>neural network</phrase> to assess the similarity of shapes within a <phrase>test</phrase> set.
visualization of arbitrary-shaped 3d scenes on depth-limited 3d displays.
we propose a depth scaling method that enables visualization of arbitrary-shaped 3d scenes on 3d displays.most current 3d displays have a depth limitation, while the scene to be displayed has not.the trivial solutions as clipping or linear scaling of the scene's 3d bounding box suffer from non-optimal utilization of the display's capabilities. our approach uses spatially adaptive depth scaling that maximizes the <phrase>perceptual</phrase> 3d effect.from the original scene <phrase>geometry</phrase>, the <phrase>topology</phrase> and local depth ordering among objects are preserved, while depth linearity is disregarded. the scaling method applies to nearly all 3d displays, such as glasses-based, head-tracked, <phrase>multi-view</phrase>, holographic and volumetric 3d displays.subjective <phrase>tests</phrase> with the dynamic <phrase>dimension</phrase> display system show that our method significantly increases the <phrase>perceptual</phrase> 3d effect.
attest: advanced <phrase>three-dimensional</phrase> <phrase>television</phrase> system technologies.
accurate and robust marker localization <phrase>algorithm</phrase> for <phrase>camera</phrase> calibration.
empirical calibration method for adding colour to <phrase>range</phrase> images.
estimating the surface <phrase>radiance</phrase> <phrase>function</phrase> from <phrase>single</phrase> images.
this <phrase>paper</phrase> describes a simple method for estimating the surface <phrase>radiance</phrase> <phrase>function</phrase> from <phrase>single</phrase> images of smooth surfaces made of materials whose <phrase>reflectance</phrase> <phrase>function</phrase> is <phrase>isotropic</phrase> and monotonic. the method makes implicit use of the <phrase>gauss</phrase> map between the surface and a <phrase>unit sphere</phrase>. we assume that the material brightness is monotonic with respect to the angle between the illuminant direction and the <phrase>surface normal</phrase>. under conditions in which the <phrase>light</phrase> source and the viewer directions are identical, we show how a tabular representation of the surface <phrase>radiance</phrase> <phrase>function</phrase> can be estimated using the cumulative distribution of image gradients. using this tabular representation of the <phrase>radiance</phrase> <phrase>function</phrase>, surfaces may be rendered under varying <phrase>light</phrase> source direction by rotating the corresponding <phrase>reflectance</phrase> map on the <phrase>gauss</phrase> <phrase>sphere</phrase> about the <phrase>specular</phrase> spike direction. we present a sensitivity study on <phrase>synthetic and real</phrase>-world imagery. we also present two applications which make use of the estimated <phrase>radiance</phrase> <phrase>function</phrase>. the first of these illustrates how the <phrase>radiance</phrase> <phrase>function</phrase> estimates can be used to render objects when the <phrase>light</phrase> and viewer directions are no longer coincident. the second application involves applying corrected lambertian <phrase>radiance</phrase> to rough and shiny surfaces.
surface height recovery using <phrase>heat flow</phrase> and <phrase>manifold</phrase> embedding.
this <phrase>paper</phrase> makes two contributions to the problem of <phrase>shape-from-shading</phrase>. first, we develop a new method for <phrase>surface normal</phrase> recovery. we pose the problem as that of solving the <phrase>steady state</phrase> <phrase>heat equation</phrase> subject to the hard constraint that lambert's <phrase>law</phrase> is satisfied. according to this picture, the <phrase>surface normals</phrase> are found by taking the <phrase>gradient</phrase> of a <phrase>scalar field</phrase>. the <phrase>heat equation</phrase> for the <phrase>scalar field</phrase> can be solved using simple <phrase>finite difference</phrase> methods and leads to an iterative procedure for <phrase>surface normal</phrase> estimation. the second contribution is to show how surface height recovery from the field of <phrase>surface normals</phrase> can be posed as one of low dimensional embedding. we experiment with the resulting method on a <phrase>variety</phrase> of <phrase>real-world</phrase> image <phrase>data</phrase>, where it produces qualitatively good reconstructed surfaces.
audio effects to enhance spatial informition displays.
estimating curvatures and their derivatives on <phrase>triangle</phrase> meshes.
the computation of <phrase>curvature</phrase> and other differential properties of surfaces is essential formany techniques in analysis and rendering. we present a finite-differences approach for estimating curvatures on irregular <phrase>triangle</phrase> meshes that may be thought of as an extension of a common method for estimating per-vertex normals. the technique is efficient in space and time, and <phrase>results</phrase> in significantly fewer <phrase>outlier</phrase> estimates while more broadly offering accuracy comparable to existing methods. it generalizes naturally to <phrase>computing</phrase> derivatives of <phrase>curvature</phrase> and higher-<phrase>order</phrase> surface differentials.
colon centreline calculation for ct colonography using optimised 3d <phrase>topological</phrase> thinning.
<phrase>octree</phrase> approximation and compression methods.
modeling of <phrase>free</phrase>-form surfaces and <phrase>shape from shading</phrase>.
modeling a <phrase>free</phrase>-form 3d-surface from a <phrase>single</phrase> view has been a widely pursued problem. the existing schemes are either fully-automatic shape-from-x techniques or involve adept interaction from the user but little or no geometric (<phrase>photometric</phrase>) basis. we propose a novel scheme of interactive modeling of <phrase>free</phrase>-form lambertian surfaces where the <phrase>solution</phrase> obtained is consistent with the <phrase>shape from shading</phrase> <phrase>model</phrase>. to this end, a <phrase>reinforcement learning</phrase> based scheme has been adopted which allows user intervention at any stage of the <phrase>algorithm</phrase> to guide the <phrase>sfs</phrase> <phrase>solution</phrase> to a global minimum.
automatic extraction of planar projections from panoramic <phrase>range</phrase> images.
this <phrase>paper</phrase> presents a segmentation technique to decompose automatically a panoramic <phrase>range</phrase> image into a set of planar projections. it consists of three stages. firstly, two orthogonal surface orientation histograms are generated. secondly, from these histograms the <phrase>major</phrase> surfaces' orientations are extracted. finally, a <phrase>histogram</phrase> of distances is computed for each one of these orientations; it will be used to define the position of the projection planes as well as the corresponding clipping planes. the original panoramic <phrase>range</phrase> image is divided into as many planar projections as main directions in the orientation histograms are extracted. this technique can be used with both indoor and outdoor scenes. <phrase>experimental</phrase> result with a panoramic <phrase>range</phrase> image is presented.
surface <phrase>model</phrase> generation from <phrase>range</phrase> images of <phrase>industrial</phrase> environments.
this <phrase>paper</phrase> presents an <phrase>hybrid</phrase> segmentation technique that combines both the speed of an edge <phrase>based approach</phrase> with the robustness of a surface <phrase>based approach</phrase>. it consists of three stages. in the first stage a <phrase>scan line</phrase> approximation process extracts the edges contained into the given <phrase>range</phrase> image. these edges are later on used to define the positions of <phrase>seed</phrase> points. through the second stage a two steps <phrase>region</phrase> growing technique is applied. first a 2d growing process enlarges the original <phrase>seed</phrase> points generating bigger regions. next, each <phrase>region</phrase> is fitted to a plane and a cylinder. the one that best fit the given points is selected to represent that <phrase>region</phrase> and used during the 3d growing stage. the 3d growing stage is carried out taking into account the approximation error from candidate points to be added to the fitted surface. in this way, each surface is grown until no points can be added according to a user defined threshold. finally, in the third stage, a post-processing <phrase>algorithm</phrase> merges neighbour regions that belong to the same surface. <phrase>experimental</phrase> <phrase>results</phrase> by using <phrase>industrial</phrase> environments are presented.
unsupervised motion classification by means of efficient <phrase>feature selection</phrase> and tracking.
this <phrase>paper</phrase> presents an efficient technique for <phrase>human</phrase> motion recognition; in particular, it is focused on labeling a movement as a walking or running displacement, which are the most frequent type of locomotion. the proposed technique consists of two stages and is based on the study of <phrase>feature points</phrase>' trajectories. the first stage detects <phrase>peaks</phrase> and valleys of points' trajectories, which are used on the second stage to discern whether the movement corresponds to a walking or a running displacement. prior <phrase>knowledge</phrase> of <phrase>human</phrase> body <phrase>kinematics</phrase> structure together with the corresponding motion <phrase>model</phrase> are the basis for the motion recognition. <phrase>experimental</phrase> <phrase>results</phrase> with different <phrase>video</phrase> sequences are presented.
modeling closed surfaces in a <phrase>multi-resolution</phrase> <phrase>fashion</phrase>.
compression of isosurfaces for structured volumes with context modelling.
second <phrase>order</phrase> local analysis for 3d <phrase>reconstruction</phrase> of <phrase>specular</phrase> surfaces.
implementation of a shadow carving system for shape capture.
3d <phrase>reality</phrase> modelling: photo-realistic 3d models of <phrase>real world</phrase> scenes.
a formulation of boundary mesh segmentation.
we present a formulation of boundary mesh segmentation as an <phrase>optimization problem</phrase>. previous segmentation solutions are classified according to the different segmentation goals, the optimization criteria and the various algorithmic techniques used. we identify two primarily distinct types of mesh segmentation, namely partssegmentation and patch segmentation. we also define generic <phrase>algorithms</phrase> for the <phrase>major</phrase> techniques used for segmentation.
extraction and description of 3d (articulated) <phrase>moving objects</phrase>.
<phrase>pyramid</phrase> coordinates for <phrase>morphing</phrase> and deformation.
many <phrase>model</phrase> <phrase>editing</phrase> operations, such as <phrase>morphing</phrase>, blending, and shape deformation, require the ability to interactively transform the surface of a <phrase>model</phrase> in response to some control mechanism. for most computer graphics applications, it is important to preserve the local shape properties of input models during <phrase>editing</phrase> operations. our work introduces the first, to our <phrase>knowledge</phrase>, mesh <phrase>editing</phrase> technique that explicitly preserves local shape properties. the <phrase>method is based</phrase> on a local shape representation, which we refer to as <phrase>pyramid</phrase> coordinates. the <phrase>pyramid</phrase> coordinates capture the local shape of the mesh around each vertex and help maintain this shape under various <phrase>editing</phrase> operations. they are based on a set of angles and lengths relating a vertex to its immediate neighbors. this representation is invariant under rigid transformations. using <phrase>pyramid</phrase> coordinates, we introduce a new technique for mesh deformation and <phrase>morphing</phrase> based on a small number of user-specified control vertices. our <phrase>algorithm</phrase> generate natural looking deformations and <phrase>morphing</phrase> sequences in seconds with minimal user interaction.
unifying measured point sequences of deforming objects.
recent progress in <phrase>digitizing</phrase> technologies is making it possible to capture the 3d shapes of <phrase>moving objects</phrase>. to efficiently utilize time series records of spatial <phrase>data</phrase>, the <phrase>information</phrase> must be unified to yield coherent deforming models. this <phrase>paper</phrase> presents a <phrase>general</phrase> method that unifies unregistered 3d point sequences to generate deforming mesh models. the method does not assume any specific <phrase>kinematic</phrase> structure, and is applicable to any digitizer. the method first polygonizes the initial points and then deforms meshes to best fit the subsequent <phrase>data</phrase> points while minimizing the deformation <phrase>energy</phrase>. experiments are conducted on real measured <phrase>data</phrase> and cg <phrase>data</phrase>, and successful <phrase>results</phrase> are obtained. as an application of the method, we examine <phrase>data compression</phrase> and achieve a 380 fold reduction rate for a measured <phrase>data</phrase> <phrase>sequence</phrase>.
<phrase>surface reconstruction</phrase> from <phrase>multiple views</phrase> using rational b-splines and knot insertion.
efficient <phrase>algorithm</phrase> for the computation of 3d <phrase>fourier</phrase> descriptors.
<phrase>visual-hull</phrase> <phrase>reconstruction</phrase> from uncalibrated and unsynchronized <phrase>video</phrase> streams.
we present an approach for automatic <phrase>reconstruction</phrase> of a dynamic event using multiple <phrase>video</phrase> cameras <phrase>recording</phrase> from different viewpoints. those cameras do not need to be calibrated or even synchronized. our approach recovers all the necessary <phrase>information</phrase> by analyzing the motion of the silhouettes in the multiple <phrase>video</phrase> streams. the first step consists of <phrase>computing</phrase> the calibration and synchronization for pairs of cameras. we compute the temporal offset and epipolar <phrase>geometry</phrase> using an efficient ransac-based <phrase>algorithm</phrase> to search for the epipoles as well as for robustness. in the next stage the calibration and synchronization for the complete <phrase>camera</phrase> network is recovered and then refined through <phrase>maximum likelihood estimation</phrase>. finally, a <phrase>visual-hull</phrase> <phrase>algorithm</phrase> is used to the recover the dynamic shape of the observed object. for unsynchronized <phrase>video</phrase> streams silhouettes are interpolated to deal with <phrase>subframe</phrase> temporal offsets. we demonstrate the validity of our approach by obtaining the calibration, synchronization and 3d <phrase>reconstruction</phrase> of a moving person from a set of 4 minute videos recorded from 4 widely separated <phrase>video</phrase> cameras.
<phrase>image-based</phrase> photo hulls.
facial <phrase>view synthesis</phrase> from a <phrase>single</phrase> image using <phrase>shape from shading</phrase>.
we present a facial <phrase>view synthesis</phrase> technique based on explicit shape and <phrase>reflectance</phrase> <phrase>information</phrase> extracted from a <phrase>single</phrase> image. the technique combines an <phrase>image based</phrase> <phrase>reflectance</phrase> estimation process with a novel method of <phrase>interpolating</phrase> between needle-maps recovered using <phrase>shape from shading</phrase>. this allows images of a face to be synthesised under novel lighting, pose and <phrase>skin</phrase> <phrase>reflectance</phrase> given only one example image. we exploit facial <phrase>symmetry</phrase> by reflecting the needle-map of a rotated face to yield the needle-map of the face rotated in the opposite direction. this provides two needle-maps between which <phrase>interpolation</phrase> can be performed.
a variational analysis of shape from specularities using sparse <phrase>data</phrase>.
looking around in our every day environment, many of the encountered objects are <phrase>specular</phrase> to some <phrase>degree</phrase>. actively using this fact when reconstructing objects from <phrase>image sequences</phrase> is the scope of the shape from specularities problem. one reason why this problem is important is that standard <phrase>structure from motion</phrase> techniques fail when the object surfaces are <phrase>specular</phrase>. here this problem is addressed by estimating surface shape using <phrase>information</phrase> from the <phrase>specular</phrase> reflections. a <phrase>specular reflection</phrase> gives constraints on the <phrase>surface normal</phrase>. the approach taken in this <phrase>paper</phrase> differs significantly from many earlier shape from specularities methods since the normal <phrase>data</phrase> used is sparse. the main contribution of this <phrase>paper</phrase> is to give a solid foundation for shape from specularities problems. estimation of surface shape using reflections is formulated as a variational problem and the surface is represented implicitly using <phrase>a level</phrase> set formulation. a functional incorporating all surface constraints is proposed and the corresponding <phrase>level set</phrase> motion <phrase>pde</phrase> is explicitly derived. this motion is then <phrase>proven</phrase> to minimize the functional. as a part of this functional a variational approach to normal alignment is proposed and analyzed. also novel methods for implicit surface <phrase>interpolation</phrase> to sparse point sets are presented together with a variational analysis. experiments on both real and synthetic <phrase>data</phrase> support <phrase>the proposed method</phrase>.
<phrase>surface reconstruction</phrase> from the projection of points, curves and contours.
in this <phrase>paper</phrase> the problem of building and reconstructing geometrical surface models from multiple calibrated images is considered. we build an appropriate statistical 3d <phrase>model</phrase> from the images alone and show how this a priori <phrase>model</phrase> can be used to automatically reconstruct new instances of the object category from one or several images. the <phrase>surface reconstruction</phrase> <phrase>method is based</phrase> on <phrase>a level</phrase> set representation and one of the main novel contributions lie within the <phrase>level set</phrase> framework. standard methods use either image-correlation or point correspondences to achieve this goal. we show how this framework can be extended to incorporate image curves and apparent contours (i.e. the projections of silhouettes). in <phrase>order</phrase> to automatically obtain feature correspondences, we use a statistical shape modelfor the object category of interest. the <phrase>model</phrase> is based on the active shape <phrase>model</phrase> using probabilistic <phrase>pca</phrase>. the scheme is applied to build and automatically reconstruct 3d surface models of faces. the resulting system is demonstrated on a <phrase>database</phrase> of real face images.
browsing 3-d spaces with 3-d vision: body-driven <phrase>navigation</phrase> through the <phrase>internet</phrase> <phrase>city</phrase>.
f3d - a <phrase>file format</phrase> and tools for storage and manipulation of volumetric <phrase>data</phrase> sets.
efficient <phrase>model</phrase> creation of large structures based on <phrase>range</phrase> segmentation.
this <phrase>paper</phrase> describes an efficient 3d modeling method from 3d <phrase>range</phrase> <phrase>data</phrase>-sets that is utilizing <phrase>range</phrase> <phrase>data</phrase> segmentation. our <phrase>algorithm</phrase> starts with a set of unregistered 3d <phrase>range</phrase> scans of a <phrase>large scale</phrase> scene. the scans are being pre-processed for noise removal and hole filling. the next step is <phrase>range</phrase> segmentation and the extraction of planar and linear features. these features are utilized for the automatic registration of the <phrase>range</phrase> scans into a common frame of reference [automated <phrase>feature-based</phrase> <phrase>range</phrase> registration of <phrase>urban</phrase> scenes of <phrase>large scale</phrase>]. a volumetric-based <phrase>algorithm</phrase> is used for the <phrase>construction</phrase> of a coherent 3d mesh that encloses all <phrase>range</phrase> scans. finally, the original segmented scans are used in <phrase>order</phrase> to simplify the constructed mesh. the mesh can now be represented as a set of planar regions at areas of low complexity and as a set of dense mesh triangular elements at areas of <phrase>high</phrase> complexity. this is achieved by <phrase>computing</phrase> the overlaps of the original segmented planar areas on the generated 3d mesh. the example of the <phrase>construction</phrase> of the 3d <phrase>model</phrase> of a building in the <phrase>nyc</phrase> <phrase>area</phrase> is presented.
<phrase>pde</phrase>-based <phrase>multi-view</phrase> depth estimation.
<phrase>triangle</phrase> mesh-based surface modeling using adaptive smoothing and implicit surface texture integration.
image matching based on co-motion <phrase>statistics</phrase>.
this <phrase>paper</phrase> presents a method for matching partially overlapping image-pairs where the object of interest is in motion, even if the motion is discontinuous and in an unstructured environment. in a typical outdoor <phrase>multi-camera</phrase> <phrase>surveillance</phrase> system, an observed object as seen by separate cameras may appear very different, due to the <phrase>variable</phrase> influence of factors such as lighting conditions and <phrase>camera</phrase> angles. thus static features such as object color, shape, and contours cannot be used for image matching. in this <phrase>paper</phrase> a different method is proposed for matching partially overlapping images captured by such cameras. the matching is achieved by calculation of co-motion <phrase>statistics</phrase>, followed by detection and rejection of points outside the overlap <phrase>area</phrase> and a nonlinear optimization process. the robust <phrase>algorithm</phrase> we describe finds point correspondences in two images without searching for any structures and without the need for tracking continuous motion. trials using statistical motion-based image cross-registration, a robust rejection <phrase>algorithm</phrase>, and automatic 3d image-transformation and <phrase>camera</phrase> calibration on real-<phrase>life</phrase> outdoor images have demonstrated the feasibility of this approach.
robust 3d segmentation for underwater <phrase>acoustic</phrase> images.
in this <phrase>paper</phrase>, a new technique for 3d <phrase>acoustic</phrase> <phrase>image segmentation</phrase> and modelling is proposed. especially, in the underwater environment, in which optical sensors suffer from visibility problems, the acoustical devices may provide efficient solutions, but, on the other hand, <phrase>acoustic</phrase> image interpretation is surely more difficult for a <phrase>human</phrase> operator. the proposed application involves the use of an <phrase>acoustic</phrase> <phrase>camera</phrase> which directly acquires images structured as a set of 3d points. due to the noisy <phrase>nature</phrase> of this type of <phrase>data</phrase>, the segmentation problem becomes more challenging and the standard <phrase>algorithms</phrase> for <phrase>range</phrase> <phrase>image segmentation</phrase> are likely to fail. <phrase>the proposed method</phrase> is based on a simplified version of the so called recover and select <phrase>paradigm</phrase> in which the <phrase>seed</phrase> areas, from which the segmentation starts, are generated by adopting a robust approach based on the ransac (random sample and consensus) <phrase>algorithm</phrase>. superquadric primitives are directly recovered from raw <phrase>data</phrase> without any pre-segmentation processing. <phrase>experimental</phrase> trials using both <phrase>synthetic and real</phrase> acoustical images confirm the goodness of the method, and a large robustness of the resulting segmented images, associated to a relatively low computational load.
a <phrase>tensor</phrase> voting approach for the hierarchical segmentation of 3-d <phrase>acoustic</phrase> images.
advances in mesh <phrase>signal processing</phrase> and <phrase>geometry</phrase> compression.
3d image <phrase>sequence</phrase> acquisition for <phrase>tv</phrase> & <phrase>film</phrase> <phrase>production</phrase>.
panoramic image transform of <phrase>omnidirectional</phrase> images using <phrase>discrete geometry</phrase> techniques.
this <phrase>paper</phrase> proposes an <phrase>omnidirectional</phrase>-to-panoramic image transform with <phrase>high</phrase> accuracy using <phrase>pde</phrase>-based resampling models. for the application of computer-vision techniques to <phrase>omnidirectional</phrase> images, the transformation of <phrase>omnidirectional</phrase> images to uniform-resolution <phrase>quadric</phrase>-surface images is needed in the two reasons. first, an omni-directional image does not have a uniform resolution. second, the development of the computer-vision-based techniques on the <phrase>quadric</phrase> surface is mathematically accurate compared with the development of the techniques on the <phrase>omnidirectional</phrase> image directly. therefore, our aim is to generate uniform-resolution panoramic images on cylindrical surface from nonuniform-resolution <phrase>omnidirectional</phrase> images. the uniform-resolution panoramic images allow us to reconstruct 3d objects and scenes from <phrase>omnidirectional</phrase> images robustly. our panoramic transformation selects the uniform resolution <phrase>pixels</phrase> on <phrase>omnidirectional</phrase> images employing the geometrical configuration of cameras in the estimation and resampling process. therefore, our method is mathematically accurate comparing to the traditional panoramic transformation using point-to-point correspondences with the geometries of cameras and the cubic <phrase>convolution</phrase>.
generalized ransac framework for relaxed correspondence problems.
finding correspondences between two (widely) separated views is essential for several computer vision tasks, such as structure and <phrase>motion estimation</phrase> and object recognition. in the wide-baseline matching using scale and/or affine invariant features the search for correspondences typically proceeds in two stages. in the first stage a putative set of correspondences is obtained based on distances between feature descriptors. in the second stage the matches are refined by imposing global geometric constraints by means of robust estimation of the epipolar <phrase>geometry</phrase> and the incorrect matches are rejected as <phrase>outliers</phrase>. for a feature in one view, usually only one "best" feature (the nearest neighbor) in the other view is chosen as corresponding feature, despite the fact that several match candidates exist. in this <phrase>paper</phrase>, we will consider multiple candidate matches for each feature, and integrate this choice with the robust estimation stage, thus avoiding the early commitment to the "best" one. this yields a generalized ransac framework for identifying the true correspondences among sets of matches. we examine the effectiveness of different sampling strategies for sets of correspondences and <phrase>test</phrase> the approach extensively using real examples of hard correspondence problems caused by a large motion between views and/or ambiguities due to repetitive scene structures.
exploitation of 3d images for face <phrase>authentication</phrase> under pose and illumination variations.
an appearance-based face <phrase>authentication</phrase> system integrating 2d color or intensity images and 3d <phrase>data</phrase> is presented in this <phrase>paper</phrase>. the proposed system is based on a low-cost 3d and color <phrase>sensor</phrase>, capable of synchronous real-time acquisition of 3d images and associated <phrase>color images</phrase>. novel <phrase>algorithms</phrase> are proposed that exploit depth <phrase>information</phrase> and prior <phrase>knowledge</phrase> of face <phrase>geometry</phrase> and <phrase>symmetry</phrase> to achieve robust <phrase>face detection</phrase>, localization and <phrase>authentication</phrase> under conditions of background clutter, occlusion, face pose alteration and harsh illumination. a method for the enrichment of face <phrase>databases</phrase> with synthetically generated views depicting variations in pose and illumination is proposed to cope with head rotations and illumination variations, avoiding a cumbersome enrolment process. the performance of the proposed <phrase>authentication</phrase> scheme is tested thoroughly on a face <phrase>database</phrase> of 1500 images, recorded with the 3d acquisition system. <phrase>experimental</phrase> <phrase>results</phrase> demonstrate significant gains resulting from the combined use of depth and color or intensity <phrase>information</phrase>, in comparison to the use of 2d images alone.
archiving 3d <phrase>cultural</phrase> objects with surface point-wise <phrase>database</phrase> <phrase>information</phrase>.
a unified representation for interactive 3d modeling.
interactive 3d modeling is the process of building a 3d <phrase>model</phrase> of an object or a scene in real-time while the 3d (<phrase>range</phrase>) <phrase>data</phrase> is acquired. this is possible only if the <phrase>computational complexity</phrase> of all involved <phrase>algorithms</phrase> is linear with respect to the amount of <phrase>data</phrase>. we propose a new framework for 3d modeling where a complete modeling chain meets with this requirement. the framework is based on the use of <phrase>vector fields</phrase> as an implicit surface representation. each modeling step, registration, <phrase>surface reconstruction</phrase>, geometric <phrase>fusion</phrase>, compression and visualization is solved and explained using the <phrase>vector fields</phrase> without anyintermediate representations. the proposed framework allows <phrase>model</phrase> <phrase>reconstruction</phrase> from any type of 3d <phrase>data</phrase>, surface patches, curves, unorganized sets of points or a combination of these.
a volumetric approach for interactive 3d modeling.
<phrase>range</phrase> <phrase>image registration</phrase> and <phrase>surface reconstruction</phrase> have been traditionally considered as two <phrase>independent</phrase> processes where the latter relies on the <phrase>results</phrase> of the former. this <phrase>paper</phrase> presents a new approach to surface recovery from <phrase>range</phrase> images where the two processes are unified and performed in a common volumetric representation. while the reconstructed surface is described in its implicit form as a signed distance field within a volume, registration <phrase>information</phrase> for matching partial surfaces is encoded in the same volume as the <phrase>gradient</phrase> of the distance field. this allows coupling of both <phrase>reconstruction</phrase> and registration and leads to an <phrase>algorithm</phrase> whose complexity is linear with respect to the number of images and the number of measured 3d points. the close integration and performance gain improve <phrase>interactivity</phrase> in the process of modeling from <phrase>range</phrase> image acquisition to <phrase>surface reconstruction</phrase>. the distances computed in the direction of filtered normals improve robustness while preserving the sharp details of the initial <phrase>range</phrase> images. it is shown that the integrated <phrase>algorithm</phrase> is tolerant to initial registration errors as well as to measurement errors. the <phrase>paper</phrase> describes the representation and formalizes the approach. <phrase>experimental</phrase> <phrase>results</phrase> demonstrate performance advantages and tolerance to aforementioned types of errors.
a <phrase>closed-form</phrase> <phrase>solution</phrase> for a two-view self-calibration problem under fixation.
it is well known that the epipolar <phrase>geometry</phrase> between two uncalibrated perspective views is completely encapsulated in the fundamental matrix. since the fundamental matrix has seven degrees of freedom (dof), self-calibration is possible if at most seven of the intrinsic or extrinsic <phrase>camera</phrase> parameters are unknown by extracting them from the fundamental matrix. this <phrase>paper</phrase> presents a linear <phrase>algorithm</phrase> for self-calibrating a perspective <phrase>camera</phrase> which undergoes fixation, that is, a special motion in which the camera's <phrase>optical axis</phrase> is confined in a plane. since this fixation has four degrees of freedom, which is one smaller than that of <phrase>general</phrase> motion, we can extract at most three intrinsic parameters from the fundamental matrix. we here assume that the <phrase>focal length</phrase> (1 dof) and the principal point (2 dof) are unknown but fixed for two views. it will be shown that these three parameters are obtained from the fundamental matrix in an analytical <phrase>fashion</phrase> and a <phrase>closed-form</phrase> <phrase>solution</phrase> is derived. we also characterize all the degenerate motions under which there exists an <phrase>infinite set</phrase> of solutions.
<phrase>long</phrase>-<phrase>range</phrase> <phrase>high</phrase>-performance time-of-flight-based 3d imaging sensors.
3-dimensional object modeling with mesh simplification based resolution adjustment.
a 3-dimensional (3-d) object modeling technique with mesh simplification and refinement based resolution adjustment is proposed in this <phrase>paper</phrase>. polygonal models which are widely utilized in the modeling of 3-d objects are taken as basis, making use of the polygonal structure and vertex coordinates for the display of 3-d models. the amount of <phrase>polygons</phrase> and vertices of a <phrase>model</phrase> is <phrase>proportional</phrase> to the resolution as well as <phrase>data</phrase> quantity. in other words the resolution and <phrase>data</phrase> increases with the number of <phrase>polygons</phrase>. in this <phrase>paper</phrase>, it is proposed to utilize resolution, and hence <phrase>data</phrase> amount, adjustable 3-d modeling so that <phrase>model</phrase> resolution and transmitted <phrase>data</phrase> amount can be regulated according to access constraints.
active <phrase>polygon</phrase> for object tracking.
a practical approach for 3d <phrase>model</phrase> indexing by combining local and global invariants.
<phrase>protein folding</phrase> using inter-residue contacts.
analysis of <phrase>three-dimensional</phrase> motion of an object using a fixed <phrase>monocular</phrase> <phrase>camera</phrase> .
the <phrase>research</phrase> on analysis of <phrase>three-dimensional</phrase> motion by using a <phrase>monocular</phrase> <phrase>camera</phrase> instead of a <phrase>stereo camera</phrase> has important applications for making the <phrase>microscopes</phrase> used in <phrase>microbiology</phrase> or constructing the autonomous <phrase>robots</phrase> used in various fields of <phrase>industry</phrase>. in this <phrase>paper</phrase>, we propose a fixed <phrase>monocular</phrase> <phrase>camera</phrase> whose focus changed cyclically to recognize <phrase>three-dimensional</phrase> absolute motion of a rigid object.
3d interactive, on-site visualization of ancient <phrase>olympia</phrase>.
<phrase>design</phrase> of a service-based framework for generic 3d <phrase>information visualization</phrase>.
posture recognition nd segmentation from 3d <phrase>human</phrase> body scans.
fan-meshes: a geometric primitive for point-based description of 3d models and scenes.
we propose a new <phrase>data structure</phrase>, called fan-meshes (<phrase>fm</phrase>), for reconstructing 3d models and scenes represented by dense scanning point clouds. it is a local <phrase>piecewise</phrase> <phrase>linear approximation</phrase> to the <phrase>data</phrase> <phrase>geometry</phrase>, and can serve as primitives in <phrase>reconstruction</phrase> with a good balance between computational loads and <phrase>reconstruction</phrase> quality. in our <phrase>algorithm</phrase>, local remeshing is performed in preprocessing to obtain regular fms, and a three-level-point <phrase>data structure</phrase> called <phrase>triangle</phrase> selection record (<phrase>tsr</phrase>) is then used to reduce redundancies in the raw <phrase>data</phrase> and overlapping in the original fms. furthermore, to apply the method to raw 3d scanning <phrase>data</phrase>, we use a smoothing operator to the <phrase>point cloud</phrase> in <phrase>order</phrase> to eliminate some <phrase>sensor</phrase> noises. <phrase>experimental</phrase> <phrase>results</phrase> demonstrate that our scheme is effective even for <phrase>large-scale</phrase> scenes with real <phrase>data</phrase>.
uncalibrated narrow baseline <phrase>augmented reality</phrase>.
a surface <phrase>evolution</phrase> approach o probabilistic space carving.
<phrase>reconstruction</phrase> of <phrase>three dimensional</phrase> models from real images.
speech-driven face synthesis from 3d <phrase>video</phrase>.
this <phrase>paper</phrase> presents a framework for speech-driven synthesis of real faces from a corpus of 3d <phrase>video</phrase> of a person speaking.video-rate capture of dynamic 3d face shape and colour appearance provides the basis for a visual <phrase>speech synthesis</phrase> model.a displacement map representation combines face shape and colour into a 3d video.this representation is used to efficiently register and integrate shape and colour <phrase>information</phrase> captured from multiple views.to allow visual <phrase>speech synthesis</phrase> viseme primitives are identified from the corpus using <phrase>automatic speech recognition</phrase>. a novel non-rigid alignment <phrase>algorithm</phrase> is introduced to estimate dense correspondence between 3d face shape and appearance for different visemes.the registered displacement map representation together with a novel <phrase>optical flow</phrase> optimisation using both shape and colour, enables accurate and efficient non-rigid alignment.face synthesis from speech is performed by <phrase>concatenation</phrase> of the corresponding viseme <phrase>sequence</phrase> using the non-rigid correspondence to reproduce both 3d face shape and colour appearance. concatenative synthesis reproduces both viseme timing and co-articulation.face capture and synthesis has been performed for a <phrase>database</phrase> of 51 people.results demonstrate synthesis of 3d visual speech <phrase>animation</phrase> with a quality comparable to the captured <phrase>video</phrase> of a person.
<phrase>multi-camera</phrase> <phrase>reconstruction</phrase> based on <phrase>surface normal</phrase> estimation and best viewpoint selection.
in this <phrase>paper</phrase>, we present a new <phrase>algorithm</phrase> for reconstructing an environment from images recorded by multiple calibrated cameras. multiple <phrase>camera</phrase> systems challenge traditional <phrase>stereo</phrase> <phrase>algorithms</phrase> in many issues including view registration, selection of commonly visible image parts for matching, and the fact that surfaces are imaged differently from different viewpoints and poses. on the other hand, multiple cameras have the advantage of revealing surfaces at occluding contours and covering wide areas. the presented <phrase>algorithm</phrase> makes no assumption on <phrase>camera</phrase> loci and outputs an occupancy <phrase>voxel</phrase> grid, with occupied voxels being accompanied by a <phrase>surface normal</phrase>. it is correlation-based, however, outperforms the conventional correlation-<phrase>based approach</phrase> in <phrase>reconstruction</phrase> quality. it is highly parallelizable, and most importantly, is robust against artifacts due to <phrase>camera</phrase> registration errors that are typically encountered when using multiple cameras.
rapid shape acquisition using color <phrase>structured light</phrase> and multi-<phrase>pass</phrase> <phrase>dynamic programming</phrase>.
the meaning and limitations of <phrase>protein structure</phrase> alignments.
<phrase>mosaic</phrase> <phrase>construction</phrase> from a sparse set of views.
focal <phrase>region</phrase>-guided <phrase>feature-based</phrase> <phrase>volume rendering</phrase>.
3d modelling and visualization of the <phrase>human</phrase> <phrase>lung</phrase>.
a method for modelling and visualizing <phrase>human</phrase> <phrase>lungs</phrase> using <phrase>knowledge</phrase> of <phrase>lung</phrase> <phrase>anatomy</phrase> and <phrase>high</phrase> resolution ct (hrct) images is presented. the <phrase>model</phrase> consists of a symbolic description of <phrase>lung</phrase> <phrase>anatomy</phrase> and a 3d atlas. the 3d atlas is constructed using hrct volume <phrase>data</phrase>. a few anatomical landmarks are determined and are used to divide the <phrase>lungs</phrase> into anatomically and diagnostically important regions. the landmarks and the <phrase>lung</phrase> regions enable accurate mapping of the <phrase>model</phrase> to patient <phrase>data</phrase> and enable the system to deal with image and <phrase>human</phrase> variability. the <phrase>model</phrase> can be displayed as a set of labelled axial slices and as a 3d <phrase>model</phrase> of the <phrase>lungs</phrase>. the 3d visualization enables rotation and viewing of <phrase>lung</phrase> structures, lungfeatures and <phrase>lung</phrase> regions from different angles.
hue flows and scene structure.
<phrase>geometry</phrase> of contour-based correspondence for <phrase>stereo</phrase>.
hierarchical representation of virtual cities for progressive transmission over networks.
interactive network-based <phrase>navigation</phrase> over large <phrase>urban</phrase> environments raises difficult problems due to the size and complexity of these scenes. in this <phrase>paper</phrase>, we present a clientserver system allowing <phrase>navigation</phrase> over 3d cities in real time. due to a novel progressive and hierarchical representation of 3d models of densely built <phrase>urban areas</phrase>, only perceptible details for all the regions visible from a given viewpoint are progressively streamed to visualisation clients. furthermore, efficient coding methods are used to compress the representation <phrase>data</phrase> allowing quick start-up of the interactive visualisation with a highly-detailed <phrase>model</phrase>. this is achieved through a set of dedicated <phrase>algorithms</phrase> allowing a very large <phrase>city</phrase> <phrase>model</phrase> to be structured into a <phrase>multi-resolution</phrase> representation. the method efficiently exploits the fact that most automated modelling techniques of <phrase>urban</phrase> scenes provides 2d\frac{1} {2} models (building footprint, height, <phrase>altitude</phrase>, ...). so as to efficiently and faithfully <phrase>model</phrase> complex buildings, a procedural representation for roofs and <phrase>facades</phrase> is proposed. finally, we present an <phrase>mpeg4</phrase> compatible implementation based on the introduction of new node types with the associated <phrase>bitstream</phrase>.
image guided <phrase>geometry</phrase> inference.
we introduce a new method for filling holes in <phrase>geometry</phrase> obtained from 3d <phrase>range</phrase> scanners. our method makes use of 2d images of the areas where geometric <phrase>data</phrase> is missing. the 2d images guide the filling using the relationship between the images and <phrase>geometry</phrase> learned from the existing 3d scanned <phrase>data</phrase>. our method builds on existing techniques for using scanned <phrase>geometry</phrase> and for estimating shape from shaded images. rather than creating plausibly filled holes, we attempt to approximate the missing <phrase>geometry</phrase>. we present <phrase>results</phrase> for scanned <phrase>data</phrase> from both <phrase>triangulation</phrase> and time-of-flight scanners for various types of materials. to quantitatively validate our proposed method, we also compare the filled areas with <phrase>ground-truth</phrase> <phrase>data</phrase>.
scanline optimization for <phrase>stereo</phrase> on <phrase>graphics hardware</phrase>.
in this work we propose a scanline optimization procedure for computational <phrase>stereo</phrase> using a linear <phrase>smoothness</phrase> cost <phrase>model</phrase> performed by programmable <phrase>graphics hardware</phrase>. the main idea for an efficient implementation of this <phrase>dynamic programming</phrase> approach is a recursive scheme to calculate the min-<phrase>convolution</phrase> in a manner suitable for the parallel <phrase>stream</phrase> computation <phrase>model</phrase> of graphics processing units. since many image similarity functions can be efficiently calculated by modern <phrase>graphics hardware</phrase>, it is reasonable to address the final disparity extraction by graphics processors as well. our timing <phrase>results</phrase> indicate that the proposed approach is beneficial for larger image resolutions and disparity <phrase>ranges</phrase> in particular.
a robust correlation measure for correspondence estimation.
a median correlation for the estimation of corresponding points in stereovision is proposed. it is based on the normalised <phrase>correlation coefficient</phrase> using the median instead of the mean. its performance appears to be <phrase>superior</phrase> than conventional correlation specially in depth discontinuities image areas. this conclusion is derived from an empirical evaluation in which the proposed correlation is compared with the normalised <phrase>correlation coefficient</phrase> and the sum of <phrase>absolute difference</phrase>. the <phrase>results</phrase> show that the median correlation produces higher scores and <phrase>lower</phrase> estimation errors.
scale selection for the analysis of point-sampled curves.
an important task in the analysis and <phrase>reconstruction</phrase> of curvilinear structures from unorganized 3-d point samples is the estimation of <phrase>tangent</phrase> <phrase>information</phrase> at each <phrase>data</phrase> point. its main challenges are in (1) the selection of an appropriate scale of analysis to accommodate noise, <phrase>density</phrase> variation and sparsity in the <phrase>data</phrase>, and in (2) the formulation of a <phrase>model</phrase> and associated objective <phrase>function</phrase> that correctly expresses their effects. we pose this problem as one of estimating the <phrase>neighborhood</phrase> size for which the principal <phrase>eigenvector</phrase> of the <phrase>data</phrase> scatter matrix is best aligned with the true <phrase>tangent</phrase> of the curve, in a probabilistic sense. we analyze the <phrase>perturbation</phrase> on the direction of the <phrase>eigenvector</phrase> due to finite samples and noise using the expected <phrase>statistics</phrase> of the scatter matrix estimators, and employ a simple iterative procedure to choose the optimal <phrase>neighborhood</phrase> size. experiments on <phrase>synthetic and real</phrase> <phrase>data</phrase> validate the behavior predicted by the <phrase>model</phrase>, and show competitive performance and improved stability over leading <phrase>polynomial</phrase>-fitting alternatives that require a preset scale.
3d <phrase>reconstruction</phrase> of natural scenes with view-adaptive multi-texturing.
we present a 3d <phrase>reconstruction</phrase> and modeling system that operates on a number of input photographs that show a natural scene. approaches from computer graphics and <phrase>image processing</phrase> are combined and performance is shown via experiments. furthermore, <phrase>reconstruction</phrase> quality is analyzed w.r.t. the number and distribution of textures, used for <phrase>reconstruction</phrase>. the <phrase>reconstruction</phrase> pipeline starts with image acquisition, which consists of a number of photographs of the scene that are sequentially taken at different positions. since the photographs are not acquired concurrently, they are influenced by different illumination conditions that we mandate to be preserved in the final 3d representation. in the second step, object segmentation is applied and <phrase>camera</phrase> calibration provided. this allows the application of <phrase>shape-from-silhouette</phrase> approaches, namely a hierarchical <phrase>voxel</phrase> approach, where different resolution layers are organized within an <phrase>octree</phrase> structure. for applying <phrase>texture mapping</phrase>, the <phrase>voxel</phrase> <phrase>model</phrase> is transformed into a wireframe, which provides smoothing of the object's surface and also reduces the number of surface primitives. finally, a <phrase>subset</phrase> of original images is mapped onto the 3d <phrase>geometry</phrase> to provide texture <phrase>information</phrase>. here, view-adaptive multi-texturing is used to preserve natural illumination. intermediate views are interpolated automatically using adaptive real-time weight calculations for original textures.
efficient constraint evaluation <phrase>algorithms</phrase> for hierarchical next-best-view planning.
we recently proposed a new and efficient next-best-view <phrase>algorithm</phrase> for 3d <phrase>reconstruction</phrase> of indoor scenes using active <phrase>range</phrase> sensing. we overcome the computation difficulty of evaluating the view metric <phrase>function</phrase> by using an adaptive hierarchical approach to exploit the various spatial coherences inherent in the acquisition constraints and quality requirements. the impressive speedups have allowed our nbv <phrase>algorithm</phrase> to become the first to be able to exhaustively evaluate a large set of 3d views with respect to a large set of surfaces, and to include many practical acquisition constraints and quality requirements. the success of the <phrase>algorithm</phrase> is greatly dependent on the implementation efficiency of the constraint and quality evaluations. in this <phrase>paper</phrase>, we describe the algorithmic details of the hierarchical view evaluation, and present efficient <phrase>algorithms</phrase> that evaluate sensing constraints and surface sampling densities between a view volume and a surface patch instead of simply between a <phrase>single</phrase> view point and a surface point. the presentation here provides examples for the <phrase>design</phrase> of efficient <phrase>algorithms</phrase> for new sensing constraints.
3d face recognition with <phrase>region</phrase> committee voting.
in this <phrase>paper</phrase>, we introduce a new system for face recognition by matching 3d face shape. this <phrase>algorithm</phrase> selects multiple regions of the face for matching in an attempt to reduce the effects caused by variations in expression between gallery and probe images. <phrase>experimental</phrase> <phrase>results</phrase> are reported using the face recognition grand challenge v2.0 <phrase>data set</phrase>. our <phrase>results</phrase> demonstrate improved performance relative to those of four previous papers using this same <phrase>data set</phrase>.
towards on-line <phrase>digital</phrase> <phrase>doubles</phrase>.
we present a modular system for real-time 3d-scanning of <phrase>human</phrase> bodies under motion. the <phrase>high</phrase>-resolution shape and colour appearance is captured by several scanning units positioned around the object of interest. each of these units performs a foreground-background segmentation and computes a valid depth-<phrase>range</phrase> for the spatially neighbouring units. multiple depth-<phrase>ranges</phrase> are combined in a <phrase>visual hull</phrase> representation, which limits the search-<phrase>range</phrase> for the 3d-<phrase>reconstruction</phrase>. depth-estimation is based on a hierarchical mult-view-<phrase>stereo</phrase> plane sweep approach. robustness and accuracy is increased by incorporating imperceptible <phrase>infrared</phrase> illumination as well as adding local <phrase>pixel</phrase> <phrase>gradient</phrase> <phrase>information</phrase>. all parts of the processing pipeline, involving <phrase>camera</phrase> color conversions, segmentation, depth-<phrase>range</phrase> computation, <phrase>visual-hull</phrase> generation, lossless <phrase>image compression</phrase>, network transfer of the <phrase>infrared</phrase> and colour images, and the plane sweep <phrase>algorithm</phrase>, are implemented on the <phrase>gpu</phrase> and highly optimized for speed, allowing scanning times of less than 40ms per frame. <phrase>experimental</phrase> <phrase>results</phrase> demonstrate the applicability of our system to the creation of <phrase>high</phrase>-<phrase>density</phrase> on-line <phrase>digital</phrase> <phrase>doubles</phrase>.
<phrase>high</phrase>-quality real-time <phrase>stereo</phrase> using adaptive cost aggregation and <phrase>dynamic programming</phrase>.
we present a <phrase>stereo</phrase> <phrase>algorithm</phrase> that achieves <phrase>high</phrase> quality <phrase>results</phrase> while maintaining real-time performance. the key idea is simple: we introduce an adaptive aggregation step in a <phrase>dynamic-programming</phrase> (dp) <phrase>stereo</phrase> framework. the per-<phrase>pixel</phrase> matching cost is aggregated in the vertical direction only. compared to traditional dp, our approach reduces the typical "<phrase>streaking</phrase>" artifacts without the penalty of blurry object boundaries. evaluation using the benchmark <phrase>middlebury</phrase> <phrase>stereo</phrase> <phrase>database</phrase> shows that our approach is among the best (ranked first in the new evaluation system) for dp-based approaches. the performance gain mainly comes from a computationally expensive weighting scheme based on color and distance proximity. we utilize the <phrase>vector</phrase> processing capability and parallelism in <phrase>commodity</phrase> <phrase>graphics hardware</phrase> to speed up this process over two orders of <phrase>magnitude</phrase>. over 50 million disparity evaluations per second (mde/s)1 are achieved in our current implementation.
a spatio-temporal modeling method for shape representation.
the <phrase>spherical harmonic</phrase> (spharm) description is a powerful surface modeling technique that can <phrase>model</phrase> arbitrarily shaped but <phrase>simply connected</phrase> <phrase>three dimensional</phrase> (3d) objects. because spharm based 3d models can derive functional <phrase>information</phrase> analysis and classify different pathological symptoms, it has been used in many applications in biomedical image <phrase>computing</phrase>. there is an urgent requirement for efficient spatio-temporal shape modeling to represent the dynamic anatomical structures in many applications (e.g., <phrase>medical</phrase> <phrase>image analysis</phrase>, geospatial <phrase>information</phrase> systems). in this <phrase>paper</phrase> we propose a novel real <phrase>spherical harmonics</phrase> based spatio-temporal shape modeling method to efficiently and flexibly represent the shapes <phrase>sequence</phrase> of anatomical structures in <phrase>medical</phrase> images. our method works well on the <phrase>simply connected</phrase> 3d objects and the effectiveness of our approach is demonstrated through theoretic and <phrase>experimental</phrase> exploration of a set of <phrase>medical</phrase> image applications. furthermore, an evaluation criterion for spatio-temporal shape modeling efficiency is proposed and the comparison <phrase>results</phrase> showed the good performance of our method.
efficient, precise, and accurate utilization of the uniqueness constraint in <phrase>multi-view</phrase> <phrase>stereo</phrase>.
in this <phrase>paper</phrase>, the depth cue due to the assumption of texture uniqueness is reviewed. the spatial direction over which a similarity measure is optimized, in <phrase>order</phrase> to establish a <phrase>stereo</phrase> correspondence, is considered and methods to increase the precision and accuracy of <phrase>stereo</phrase> reconstructions are presented. an efficient implementation of the above methods is offered, based on optimizations that evaluate potential correspondences hierarchically, in the spatial and angular dimensions. furthermore, the expansion of the above techniques in a <phrase>multi-view</phrase> framework where calibration errors cause the misregistration of individually obtained reconstructions are considered, and a treatment of the <phrase>data</phrase> is proposed for the elimination of duplicate reconstructions of a <phrase>single</phrase> surface point. finally, a processing step is proposed for the increase of <phrase>reconstruction</phrase> precision and post-processing of the final result. the above contributions are integrated in a generic and parallelizable implementation of the uniqueness constraint to observe speedup and increase in the fidelity of <phrase>surface reconstruction</phrase>.
orientation of fragments of rotationally symmetrical 3d-shapes for <phrase>archaeological</phrase> documentation.
motivated by the requirements of modern <phrase>archaeologists</phrase>, we are developing a documentation system based on <phrase>structured light</phrase> for acquisition of ceramics. fragments of ceramics are <phrase>daily</phrase> finds at excavations and important for <phrase>archaeological</phrase> <phrase>research</phrase>, because their shape leads to <phrase>information</phrase> about ancient cultures. the shapes used for documentation are called profile lines and estimated by a vertical cross-section of orientated fragments. as ceramics have been <phrase>produced</phrase> using rotational plates for several thousands of years, the rotational <phrase>axis</phrase> can be used for orientation. therefore we conducted experiments using existing methods for estimation of the rotational <phrase>axis</phrase>. the drawbacks of these methods are the requirement of either complete objects or industrialized quality of <phrase>symmetry</phrase>. therefore we show a new method using <phrase>circle</phrase> templates, which has been inspired by the manual method of <phrase>archaeologists</phrase>. in this work we present <phrase>results</phrase> using previous and related work in comparison with the estimation of the rotational <phrase>axis</phrase> using <phrase>circle</phrase> templates. the <phrase>results</phrase> of the presented methods are shown for synthetic <phrase>data</phrase>, well-known fragments and real <phrase>data</phrase> acquired at an <phrase>archaeological</phrase> excavation. finally a conclusion and an outlook is given.
point containment in discrete arbitrary <phrase>dimension</phrase>.
the point containment predicate which specifies if a point is part of a mathematically defined shape is one of the most <phrase>elementary</phrase> operations in computer graphics and is a natural way to perform the many <phrase>raster</phrase> calculations. it plays an essential role in several important processes such as filling, stroking, <phrase>anti-aliasing</phrase>, geometric modeling and <phrase>volume rendering</phrase>. this <phrase>paper</phrase> presents a generalized point containment <phrase>algorithm</phrase> for arbitrary <phrase>dimension</phrase> discrete objects whose main characteristics are low complexity, simple <phrase>data structures</phrase> and suitability for hardware implementation.
vistre: a visualization tool to evaluate errors in terrain representation.
new <phrase>data</phrase> sources and sensors bring new possibilities for terrain representations, and new types of characteristic errors. we develop a system to visualize and compare terrain representations and the errors they produce.
exploiting 3d spatial continuity for robust automatic horizon matching across faults.
<phrase>oil</phrase> and <phrase>gas</phrase> exploration decisions are made based on inferences obtained from seismic <phrase>data</phrase> interpretation. the interpretation task is getting very time-consuming as seismic <phrase>data</phrase> sets become larger. <phrase>image processing</phrase> tools such as auto-trackers assist manual interpretation of horizons-visible boundaries between certain <phrase>sediment</phrase> layers in seismic <phrase>data</phrase>. auto-trackers assume <phrase>data</phrase> continuities; therefore, their assistance is very limited in areas of discontinuities such as faults. in this <phrase>paper</phrase>, we present a method for automatic horizon matching across faults based on a <phrase>bayesian</phrase> approach. a <phrase>stochastic</phrase> matching <phrase>model</phrase> which integrates 3d spatial <phrase>information</phrase> of seismic <phrase>data</phrase> and prior geological <phrase>knowledge</phrase> is introduced. the optimal matching <phrase>solution</phrase> is found by map estimate of this <phrase>model</phrase>. a <phrase>simulated annealing</phrase> with reversible jump <phrase>markov chain monte carlo</phrase> <phrase>algorithm</phrase> is employed to sample from a-posteriori distribution. the <phrase>model</phrase> was applied to real 3d seismic <phrase>data</phrase>, and has shown to produce geologically acceptable horizons matchings.
a <phrase>range</phrase> <phrase>camera</phrase> collecting multi-spectral texture for <phrase>architecture</phrase> applications.
this work proposes a system for the automatic <phrase>construction</phrase> of multi-spectral 3d models of <phrase>architecture</phrase>. besides the specific application which concerns the interactive visualization and the <phrase>restoration</phrase> of historical buildings, the interest of the proposed techniques lays in the multi-spectral <phrase>nature</phrase> of the textures which allow rendering with faithful colors and in the automatism of 3d <phrase>model</phrase> <phrase>construction</phrase>. the proposed system is an effective tool for producing 3d content amenable to a great number of applications.
qualitative characterization of deforming surfaces.
this <phrase>paper</phrase> extends the idea of classification schemes for static surface <phrase>curvature</phrase> into the temporal domain. we seek to identify regions in sequences of depth <phrase>data</phrase> that exhibit variations in shape change, and to characterise the <phrase>nature</phrase> of the deformation. from observing the change in principle curvatures we show how it is possible to decouple the type of change into one of fifteen classes, and also reveal the extent of alteration. <phrase>results</phrase> are presented for <phrase>synthetic and real</phrase> <phrase>data</phrase> sequences, with additional alignment performed to accommodate global motion. this technique shows promise in analysing <phrase>data</phrase> from <phrase>video</phrase>-rate <phrase>range</phrase> sensors, with potential applications in biometric and <phrase>psychological</phrase> analysis of the face and other deformable objects.
constraint integration for multiview <phrase>pose estimation</phrase> of humans with self-occlusions.
detection of articulated objects such as humans is an important task in computer vision. we present a system that incorporates a <phrase>variety</phrase> of constraints in a unified <phrase>multi-view</phrase> framework to automatically detect humans in possibly crowded scenes. these constraints include the <phrase>kinematic</phrase> constraints, the occlusion of one part by another and the <phrase>high</phrase> correlation between the appearance of parts such as the two <phrase>arms</phrase>. the <phrase>graphical</phrase> structure (non-<phrase>tree</phrase>) obtained is optimized in a nonparametric <phrase>belief propagation</phrase> framework using prior based search.
<phrase>structured light</phrase> based <phrase>reconstruction</phrase> under local spatial coherence assumption.
3d scanning techniques based on <phrase>structured light</phrase> usually achieve robustness against <phrase>outliers</phrase> by performing multiple projections to simplify correspondence. however, for cases such as dynamic scenes, the number of frames captured from a certain view must be kept as low as possible, which makes it difficult to reconstruct complex scenes with <phrase>high</phrase> <phrase>frequency</phrase> shapes and inappropriate reflection properties. to tackle this problem, we present a novel set of color stripe patterns and a robust correspondence <phrase>algorithm</phrase> that assume local spatial coherence in the captured <phrase>data</phrase>. this assumption allows us to <phrase>design</phrase> our stripe sequences with globally unique <phrase>neighborhood</phrase> properties to effectively avoid wrong correspondences. the concept of local spatial coherence is further exploited to make the ensuing <phrase>surface reconstruction</phrase> practically insensitive to noise, <phrase>outliers</phrase>, and <phrase>anisotropic</phrase> sampling <phrase>density</phrase>. thus, the recovery of a <phrase>topologically</phrase> consistent <phrase>manifold</phrase> surface can be drastically simplified. we have successfully generated <phrase>high</phrase> quality meshes of various colored objects using a minimalistic projector-<phrase>camera</phrase> system. in particular, the full sampling capabilities of our devices can be exhausted by taking only three shots.
recognition of <phrase>free</phrase>-form objects in complex scenes using dgi-<phrase>bs</phrase> models.
object recognition in 3d scenes with occlusion means identifying an incomplete and unknown object, which is arbitrarily posed, in an <phrase>object database</phrase>. this hard computer vision problem is solved in this <phrase>paper</phrase> through a new 3d shape representation called depth <phrase>gradient</phrase> <phrase>image based</phrase> on <phrase>silhouette</phrase> (dgi-<phrase>bs</phrase>). dgi-<phrase>bs</phrase> can be used to obtain a complete <phrase>model</phrase> as well as a partial <phrase>model</phrase> of an object. this <phrase>property</phrase> allows us to use it in complex scenes where incomplete surfaces of objects are available. the complete dgi-<phrase>bs</phrase> version synthesizes surface <phrase>information</phrase> (through depth image) and shape <phrase>information</phrase> (through contour) of the whole object in a <phrase>single</phrase> image smaller than 1 mega <phrase>pixel</phrase>. object recognition is carried out by means of a simple matching <phrase>algorithm</phrase> in the dgi-<phrase>bs</phrase> space which yields a correspondence point-to-point between scene and <phrase>model</phrase>. this method has been successfully tested in real scenes with no special restrictions using <phrase>range</phrase> sensors.
<phrase>visual hull</phrase> <phrase>construction</phrase> in the presence of partial occlusion.
in this <phrase>paper</phrase>, we propose a <phrase>visual hull</phrase> <phrase>algorithm</phrase>, which guarantees a correct <phrase>construction</phrase> even in the presence of partial occlusion, while "correct" here means that the real shape is located inside the <phrase>visual hull</phrase>. the <phrase>algorithm</phrase> is based on a new idea of the "extended <phrase>silhouette</phrase>", which requires the <phrase>silhouette</phrase> from background <phrase>subtraction</phrase> and the "occlusion mask" of the same view. in <phrase>order</phrase> to prepare the occlusion mask, we also propose a novel concept of "effective boundary" of moving foreground objects in a <phrase>video</phrase> obtained from a static <phrase>camera</phrase>. the accumulation of the effective boundary through time automatically gives robust occluder boundaries. we theoretically prove that our <phrase>algorithm</phrase> deterministically computes the tightest, correct <phrase>visual hull</phrase> in the presence of occlusion. both <phrase>synthetic and real</phrase> examples are given as a demonstration of the correctness of the <phrase>algorithm</phrase>. finally we analyze that this new <phrase>algorithm</phrase> is still within the time complexity of the traditional method.
aerial <phrase>lidar</phrase> <phrase>data</phrase> classification using <phrase>support vector machines</phrase> (<phrase>svm</phrase>).
we classify 3d aerial <phrase>lidar</phrase> scattered height <phrase>data</phrase> into buildings, <phrase>trees</phrase>, roads, and <phrase>grass</phrase> using the <phrase>support vector machine</phrase> (<phrase>svm</phrase>) <phrase>algorithm</phrase>. to do so we use five features: height, height variation, normal variation, <phrase>lidar</phrase> return intensity, and image intensity. we also use only <phrase>lidar</phrase>-derived features to organize the <phrase>data</phrase> into three classes (the <phrase>road</phrase> and <phrase>grass</phrase> classes are merged). we have implemented and experimented with several variations of the <phrase>svm</phrase> <phrase>algorithm</phrase> with soft-margin classification to allow for the noise in the <phrase>data</phrase>. we have applied our <phrase>results</phrase> to classify aerial <phrase>lidar</phrase> <phrase>data</phrase> collected over approximately 8 square miles. we visualize the classification <phrase>results</phrase> along with the associated confidence using a variation of the <phrase>svm</phrase> <phrase>algorithm</phrase> producing probabilistic classifications. we observe that the <phrase>results</phrase> are stable and robust. we compare the <phrase>results</phrase> against the <phrase>ground truth</phrase> and obtain higher than 90% accuracy and convincing visual <phrase>results</phrase>.
resolution scalable coding and <phrase>region</phrase> of interest access with <phrase>three-dimensional</phrase> sbhp <phrase>algorithm</phrase>.
a low-complexity <phrase>three-dimensional</phrase> <phrase>image compression</phrase> <phrase>algorithm</phrase> based on <phrase>wavelet</phrase> transforms and set-partitioning strategy is presented. the subband block hierarchial partitioning (sbhp) <phrase>algorithm</phrase> is modified and extended to three dimensions, and applied to every code block independently. the resultant <phrase>algorithm</phrase>, 3d-sbhp, efficiently encodes 3d image <phrase>data</phrase> by the exploitation of the dependencies in all dimensions, while enabling progressive snr and resolution <phrase>decompression</phrase> and <phrase>region</phrase>-of-interest (roi) access from the same <phrase>bit</phrase> <phrase>stream</phrase>. the code-block selection method by which <phrase>random access</phrase> decoding can be achieved is outlined.the resolution scalable and <phrase>random access</phrase> performances are empirically investigated. the <phrase>results</phrase> show 3d-sbhp is a good candidate to compress 3d image <phrase>data</phrase> sets for <phrase>multimedia</phrase> applications.
estimating a-priori unknown 3d axially symmetric surfaces from noisy measurements of their fragments.
in this <phrase>paper</phrase>, we present a computationally efficient technique for solving the difficult problem of estimating the global shape of a <phrase>ceramic</phrase> pot from measurements of its fragments. each unknown pot is modeled as a surface of <phrase>revolution</phrase>, i.e., a 3d line-- the central <phrase>axis</phrase> of the pot -- and a 2d profile curve with respect to that <phrase>axis</phrase>. for each fragment, a probabilistic distribution is estimated which models both the geometric shape of the fragment and the variability of the estimated fragment shape. estimation of the global pot shape is then a <phrase>maximum likelihood estimation</phrase> (mle) problem where we seek the values of the <phrase>euclidean</phrase> transformation parameters that maximize the joint <phrase>probability</phrase> of the matched fragments' <phrase>axis</phrase>/profile-curvemodels (which includes the additional constraint that the matched fragments must share a common central <phrase>axis</phrase>). this is a new type of curve-analysis problem and our <phrase>solution</phrase> is a new and effective approach applicable for generic constrained 2d curve alignment and for modeling of 3d axially-symmetric surfaces and for comparing geometric models which may correspond over a <phrase>subset</phrase> of the complete <phrase>model</phrase>.
on 3d retrieval from photos.
in this <phrase>paper</phrase>, we propose a method for 3d-<phrase>model</phrase> retrieval from one or more photos. this method provides an "optimal" selection of 2d views to represent a 3d-<phrase>model</phrase>, and a probabilistic <phrase>bayesian</phrase> method for 3d-<phrase>model</phrase> retrieval from realistic photos and sketches using these views. the characteristic view selection <phrase>algorithm</phrase> is based on an adaptive clustering <phrase>algorithm</phrase> and uses <phrase>statistical model</phrase> distribution scores to select the optimal number of views. we also introduce a <phrase>bayesian</phrase> approach to score the <phrase>probability</phrase> of correspondence between the queries and the 3d-models. we present our <phrase>results</phrase> on the <phrase>princeton</phrase> 3d shape benchmark <phrase>database</phrase> (1814 3d-models) and 50 photos (real photographs, sketches, synthesised images). a practical on-line 3d-<phrase>model</phrase> retrieval system based on our approach is available on the web to asset our <phrase>results</phrase> [1].
the reverse projection correlation principle for depth from defocus.
in this <phrase>paper</phrase>, we address the problem of finding depth from defocus in a fundamentally new way. most previous methods have used an approximate <phrase>model</phrase> in which blurring is shift invariant and <phrase>pixel</phrase> <phrase>area</phrase> is negligible. our <phrase>model</phrase> avoids these assumptions. we consider the <phrase>area</phrase> in the scene whose <phrase>radiance</phrase> is recorded by a <phrase>pixel</phrase> on the <phrase>sensor</phrase>, and relate the size and shape of that <phrase>area</phrase> to the scene's position with respect to the plane of focus. this is the notion of reverse projection, which allows us to illustrate that, when out of focus, neighboring <phrase>pixels</phrase> will record <phrase>light</phrase> from overlapping regions in the scene. this overlap <phrase>results</phrase> in a measurable change in the correlation between the <phrase>pixels</phrase>' intensity values. we demonstrate that this relationship can be characterized in such a way as to recover depth from defocused images. <phrase>experimental</phrase> <phrase>results</phrase> show the ability of this relationship to accurately predict depth from correlation measurements.
direct and indirect 3-d <phrase>reconstruction</phrase> from opti-<phrase>acoustic</phrase> <phrase>stereo</phrase> imaging.
utilization of an <phrase>acoustic</phrase> <phrase>camera</phrase> for <phrase>range</phrase> measurements is a significant advantage for 3-d shape recovery of underwater targets by opti-<phrase>acoustic</phrase> <phrase>stereo</phrase> imaging, where the associated epipolar <phrase>geometry</phrase> of visual and <phrase>acoustic</phrase> image correspondences is described in terms of <phrase>conic sections</phrase> and <phrase>trigonometric functions</phrase>. in this <phrase>paper</phrase>, we propose and analyze a number of methods based on direct and indirect approaches that provide insight on the merits of the new imaging and 3-d object <phrase>reconstruction</phrase> <phrase>paradigm</phrase>. we have devised certain indirect methods, built on a regularization formulation, to first compute from noisy correspondences <phrase>maximum likelihood</phrase> estimates that satisfy the epipolar <phrase>geometry</phrase>. the 3-d <phrase>target</phrase> points can then be determined from a number of <phrase>closed-form</phrase> solutions applied to these ml estimates. an <phrase>alternative</phrase> direct approach is also presented for 3-d reocnstruction directly from noisy correspondences. computer simulations verify consistency between the analytical and <phrase>experimental</phrase> <phrase>reconstruction</phrase> snrs -- the criterion applied in performance assessment of these various solutions.
automatic registration of <phrase>multiple range</phrase> images by the local log-polar <phrase>range</phrase> images.
we propose a method for automatic registration of <phrase>multiple range</phrase> images by matching invariant feature vectors generated from the local log-polar <phrase>range</phrase> images. point pairs are corresponded by finding the nearest neighbor of invariant feature vectors. the correspondence is validated, and the pairwise transformations between the input <phrase>range</phrase> images are determined by using the ransac <phrase>algorithm</phrase>. the registration of all input <phrase>range</phrase> images are determined by constructing the view <phrase>tree</phrase> of the input <phrase>range</phrase> images. the registration result of <phrase>the proposed method</phrase> is used as the initial value of a fine registration methods for object shape modeling.
a modular scheme for 2d/3d conversion of <phrase>tv</phrase> <phrase>broadcast</phrase>.
the 3d <phrase>reconstruction</phrase> from 2d <phrase>broadcast</phrase> <phrase>video</phrase> is a challenging problem with many potential applications, such as 3dtv, <phrase>free</phrase>-viewpoint <phrase>video</phrase> or <phrase>augmented reality</phrase>. in this <phrase>paper</phrase>, a modular system capable of efficiently reconstructing 3d scenes from <phrase>broadcast</phrase> <phrase>video</phrase> is proposed. the system consists of four constitutive modules: tracking and segmentation, self-calibration, sparse <phrase>reconstruction</phrase> and, finally, dense <phrase>reconstruction</phrase>. this <phrase>paper</phrase> also introduces some novel approaches for moving object segmentation and sparse and dense <phrase>reconstruction</phrase> problems. according to the simulations for both <phrase>synthetic and real</phrase> <phrase>data</phrase>, the system achieves a promising performance for typical <phrase>tv</phrase> content, indicating that it is a significant step towards the 3d <phrase>reconstruction</phrase> of scenes from <phrase>broadcast</phrase> <phrase>video</phrase>.
fast and efficient dense variational <phrase>stereo</phrase> on <phrase>gpu</phrase>.
thanks to their <phrase>high</phrase> performance and programmability, the latest <phrase>graphics cards</phrase> can now be used for scientific purpose. they are indeed very efficient parallel <phrase>single</phrase> instruction multiple <phrase>data</phrase> (<phrase>simd</phrase>) machines. this new trend is called <phrase>general</phrase> purpose computation on <phrase>graphics processing unit</phrase> (gpgpu [4]). regarding the <phrase>stereo</phrase> problem, variational methods based on deformable models provide dense, smooth and accurate <phrase>results</phrase>. nevertheless, they prove to be slower than usual disparity-based approaches. in this <phrase>paper</phrase>, we present a dense <phrase>stereo</phrase> <phrase>algorithm</phrase>, handling occlusions, using three cameras as inputs and entirely implemented on a <phrase>graphics processing unit</phrase> (<phrase>gpu</phrase>). <phrase>experimental</phrase> speedups prove that our approach is efficient and perfectly adapted to the <phrase>gpu</phrase>, leading to nearly <phrase>video</phrase> <phrase>frame rate</phrase> <phrase>reconstruction</phrase>.
automatic 3d <phrase>face detection</phrase>, normalization and recognition.
a fully automatic 3d face recognition <phrase>algorithm</phrase> is presented. several novelties are introduced to make the recognition robust to facial expressions and efficient. these novelties include: (1) automatic 3d <phrase>face detection</phrase> by detecting the nose; (2) automatic pose correction and normalization of the 3d face as well as its corresponding 2d face using the hotelling transform; (3) a spherical face representation and its use as a rejection classifier to quickly reject a large number of candidate faces for efficient recognition; and (4) robustness to facial expressions by automatically segmenting the face into expression sensitive and insensitive regions. experiments performed on the frgc ver 2.0 dataset (9,500 2d/3d faces) show that our <phrase>algorithm</phrase> outperforms existing 3d recognition <phrase>algorithms</phrase>. we achieved verification rates of 99.47% and 94.09% at 0.001 far and identification rates of 98.03% and 89.25% for probes with neutral and non-neutral expression respectively.
extracting 3d shape features in discrete <phrase>scale-space</phrase>.
3d shape features are inherently scale-dependent. for instance, on a 3d <phrase>model</phrase> of a <phrase>human</phrase> body, the top of the head and a fingertip can both be detected as corner points, however, at entirely different scales. in this <phrase>paper</phrase>, we present a method for extracting and integrating 3d shape features in the discrete <phrase>scale-space</phrase> of a triangular <phrase>mesh model</phrase>. we first parameterize the surface of the <phrase>mesh model</phrase> on a 2d plane and then construct a dense <phrase>surface normal</phrase> map. in <phrase>general</phrase>, the <phrase>parametrization</phrase> is not isometric. to account for this, we compute the relative stretch of the original edge lengths. next, we compute a dense <phrase>distortion</phrase> map which is used to approximate the <phrase>geodesic</phrase> distances on the normal map. then, we construct a discrete <phrase>scale-space</phrase> of the original 3d shape by successively convolving the normal map with <phrase>distortion</phrase>-adapted gaussian kernels of increasing <phrase>standard deviation</phrase>. we derive corner and edge detectors to extract 3d features at each scale in the discrete <phrase>scale-space</phrase>. furthermore, we show how to combine the detector responses from different scales to form a unified representation of the 3d features.
<phrase>philips</phrase> 3d solutions: from content creation to visualization.
<phrase>philips</phrase> is realizing an end-to-end 3d display <phrase>solution</phrase> from 3d content creation to visualization. this development fits in our <phrase>long</phrase>-standing tradition of combining expertise in <phrase>video processing</phrase> with our strength in display development to create the most exciting and best viewing experience. <phrase>philips</phrase> developed several <phrase>high</phrase>-quality 3d displays, ranging in resolution, viewing angle, depth experience, and sizes from 4" to 40" and up. backwards compatibility with 2d content is enabled via <phrase>signal-processing</phrase> or opto-<phrase>electronic</phrase> 3d & 2d dual mode displays. content creation and conversion methods are provided, which are a key factor for the success of 3d displays. fully automatic conversion from monoscopic 2d content into 3d enables the re-use of all existing 2d <phrase>video</phrase> material. further methods enable 3d <phrase>animation</phrase>/<phrase>design</phrase>, 2d to 3d conversion in <phrase>post-production</phrase> and <phrase>live</phrase> capture of new 3d content. our efforts in <phrase>mpeg</phrase> standardization towards the "2d-plus-depth" format for 3d <phrase>video</phrase> enables a flexible interface between the <phrase>variety</phrase> in 3d content creation methods and the <phrase>range</phrase> in 3d displays. furthermore, the 3d format is compatible with existing 2d content, standards and <phrase>infrastructure</phrase>. currently, <phrase>philips</phrase> offers several commercial 3d <phrase>products</phrase> for <phrase>professional</phrase> use such as <phrase>digital signage</phrase>, and progress is being made towards <phrase>consumer</phrase> <phrase>products</phrase> such as 3dtv.
gaze tracking by using factorized likelihoods <phrase>particle</phrase> filtering and <phrase>stereo</phrase> vision.
this <phrase>paper</phrase> describes a non-intrusive method to estimate the gaze direction of a person by using <phrase>stereo</phrase> cameras. first, facial features are tracked with an adapted <phrase>particle</phrase> filtering <phrase>algorithm</phrase> using factorized likelihoods to estimate the 3d head pose. next the 3d gaze <phrase>vector</phrase> is calculated by estimating the eyeball <phrase>center</phrase> and the <phrase>cornea</phrase> <phrase>center</phrase> of both eyes. for the intended application of <phrase>visual perception</phrase> <phrase>research</phrase>, we also propose a new screen registration scheme to accurately locate a planar screen in world coordinates within 2 mm error. we combine the 3d screen location with the 3d gaze vectors from both eyes to establish a point of focus on the screen. the <phrase>experimental</phrase> <phrase>results</phrase> indicate that an <phrase>average</phrase> error of the gaze direction of about 4.6 can be achieved and an <phrase>average</phrase> error of about 4 mm for the focus point location at a viewing distance of 50 cm.
invariant <phrase>high</phrase> level reeb <phrase>graphs</phrase> of 3d polygonal meshes.
many applications in computer graphics need <phrase>high</phrase> level shape descriptions, in <phrase>order</phrase> to benefit from a global understanding of shapes. <phrase>topological</phrase> approaches enable pertinent surface decompositions, providing structural descriptions of 3d polygonal meshes; but in practice, their use raises several difficulties. in this <phrase>paper</phrase>, we present a novel method for the <phrase>construction</phrase> of invariant <phrase>high</phrase> level reeb <phrase>graphs</phrase>, <phrase>topological</phrase> entities that give a good overview of the shape structure. with this aim, we propose an accurate and straightforward feature point extraction <phrase>algorithm</phrase> for the computation of an invariant and meaningful quotient <phrase>function</phrase>. moreover, we propose a new <phrase>graph</phrase> <phrase>construction</phrase> <phrase>algorithm</phrase>, based on an analysis of the connectivity evolutions of discrete level lines. this <phrase>algorithm</phrase> brings a practical <phrase>solution</phrase> for the suppression of non-significant critical points over <phrase>piecewise</phrase> continuous functions, providing meaningful reeb <phrase>graphs</phrase>. presented method gives accurate <phrase>results</phrase>, with satisfactory execution times and without input parameter. the geometrical invariance of resulting <phrase>graphs</phrase> and their robustness to variation in <phrase>model</phrase> pose and mesh sampling make them good candidates for several applications, like shape deformation (experimented in this <phrase>paper</phrase>), recognition, compression, indexing, etc.
recovering illumination and texture using ratio images.
in this <phrase>paper</phrase> we consider the problem of factoring illumination and texture from a pair of images of a diffuse object of known <phrase>geometry</phrase>. this problem arises frequently in 3d <phrase>photography</phrase> applications that use images to acquire <phrase>photometric</phrase> properties of a scanned object. our approach uses the ratio of the images and the <phrase>geometry</phrase> <phrase>information</phrase> to compute the relative incident <phrase>irradiance</phrase> of one image with respect to the other. after the <phrase>irradiance</phrase> maps are recovered, we build a spatially varying <phrase>albedo</phrase> map, which can then be used to render the object under different illumination conditions. we present two <phrase>algorithms</phrase>, one for point-<phrase>light</phrase> source illumination, and another one based on <phrase>spherical harmonics</phrase> for more <phrase>general</phrase> illumination conditions.
a 3d outdoor scene scanner based on a <phrase>night-vision</phrase> <phrase>range</phrase>-gated active imaging system.
we present a 3d outdoor scene scanner for the acquisition of kilometers-deep scenes in night conditions. its imaging system is based on a compact and low-cost pulsed <phrase>laser</phrase> illuminator and a <phrase>light</phrase>-<phrase>intensifier</phrase> equipped <phrase>ccd camera</phrase>. by precisely synchronizing both the illuminator and the <phrase>camera</phrase> shutter, it is possible to acquire "slices" of the scene at specific known distances. we show that even with large <phrase>laser</phrase> pulses and without <phrase>megahertz</phrase>-capable <phrase>electronics</phrase>, the third <phrase>dimension</phrase> can be recovered for the whole <phrase>range</phrase> of the scene by processing only two images acquired in specific conditions. as the <phrase>pixel</phrase> intensities of the images <phrase>produced</phrase> by active imaging systems vary with the square of the <phrase>range</phrase>, and due to the limited dynamics of image sensors, scanning <phrase>long</phrase>-<phrase>range</phrase> scenes with shorter "slices" allows the <phrase>camera</phrase> gain to be adjusted with respect to the <phrase>range</phrase> and the accuracy to be enhanced. the imaging system as well as the different <phrase>image processing</phrase> steps are detailed in this <phrase>paper</phrase> and an example of typical <phrase>results</phrase> is given.
deformable <phrase>mesh model</phrase> for complex multi-object 3d <phrase>motion estimation</phrase> from multi-viewpoint <phrase>video</phrase>.
we propose a new <phrase>algorithm</phrase> using deformable <phrase>mesh model</phrase> for complex 3d <phrase>motion estimation</phrase> of multiple objects from multi-viewpoint <phrase>video</phrase>. in this <phrase>paper</phrase>, we define "complex motion" as motion which includes global change of the object shape <phrase>topology</phrase>. in complex motion, a part of the object may touch the other parts. to manage this effect, we introduce (1) "repulsive force" into deformable <phrase>mesh model</phrase> for simple <phrase>motion estimation</phrase> which integrates texture and <phrase>silhouette</phrase> <phrase>information</phrase> into unified computation scheme, and (2) efficient <phrase>collision detection</phrase> <phrase>algorithm</phrase> for deformable <phrase>mesh model</phrase>. our deformable <phrase>mesh model</phrase> with repulsive force keeps hidden, collided surfaces to be touched each other, and gives dense, non-rigid complex 3d motion of the object. some <phrase>experimental</phrase> <phrase>results</phrase> show that our deformation <phrase>model</phrase> can estimate motions of multiple objects and the object's motion with time-varying global <phrase>topology</phrase>, and gives <phrase>topologically</phrase>-consistent mesh models which can be compressed efficiently by conventional inter-frame 3d <phrase>data compression</phrase> <phrase>algorithms</phrase> and be used for 3d motion analysis.
<phrase>computing</phrase> the <phrase>camera</phrase> motion direction from many images.
we analyze the problem of estimating a camera's motion direction from a calibrated multi-image <phrase>sequence</phrase>. we assume that the <phrase>camera</phrase> moves roughly along a line and that its <phrase>velocity</phrase> and orientation are unknown and can vary over time. for <phrase>infinitesimal</phrase> <phrase>camera</phrase> motion (multiple flows rather than multiple images), we give a <phrase>closed-form expression</phrase> for the result of minimizing the true least-squares error over all variables but the camera's motion direction. our result includes the rigidity constraint that the scene stays fixed over time. for finite motion, we present a noniterative <phrase>algorithm</phrase> that approximates the exact multi-image coplanarity error to better than a percent. also, we define a new error contribution that incorporates the rigidity constraint and is analogous to the rigidity component of the error for <phrase>infinitesimal</phrase> motion. by adding this to the coplanarity error, we obtain a noniterative <phrase>algorithm</phrase> that approximates the complete finite motion least-squares error--including rigidity--as a <phrase>function</phrase> just of the <phrase>translation</phrase> direction.
<phrase>minimum spanning tree</phrase> <phrase>pose estimation</phrase>.
the extrinsic <phrase>camera</phrase> parameters from <phrase>video</phrase> <phrase>stream</phrase> images can be accurately estimated by tracking features through the image <phrase>sequence</phrase> and using these features to compute parameter estimates. the poses for <phrase>long</phrase> <phrase>video</phrase> sequences have been estimated in this manner. however, the poses of large sets of still images cannot be estimated using the same strategy because wide-baseline correspondences are not as robust as narrow-baseline feature tracks. moreover, <phrase>video</phrase> <phrase>pose estimation</phrase> requires a linear or hierarchically-linear ordering on the images to be calibrated, reducing the image matches to the neighboring <phrase>video</phrase> frames. we propose a novel generalization to the linear ordering requirement of <phrase>video</phrase> <phrase>pose estimation</phrase> by <phrase>computing</phrase> the <phrase>minimum spanning tree</phrase> of the <phrase>camera</phrase> adjacency <phrase>graph</phrase> and using the <phrase>tree</phrase> hierarchy to determine the calibration <phrase>order</phrase> for a set of input images. we validate the pose accuracy using an error metric that is functionally <phrase>independent</phrase> of the estimation process. because we do not rely on feature tracking for generating feature correspondences, our method can use internally calibrated wide- or narrow-baseline images as input, and can estimate the <phrase>camera</phrase> poses from multiple <phrase>video</phrase> streams without special pre-processing to concatenate the streams.
multiview 3d tracking with an incrementally constructed 3d <phrase>model</phrase>.
we propose a multiview tracking method for rigid objects. assuming that a part of the object is visible in at least two cameras, a partial 3d <phrase>model</phrase> is reconstructed in terms of a collection of small 3d planar patches of arbitrary <phrase>topology</phrase>. the 3d representation, recovered fully automatically, allows to formulate tracking as <phrase>gradient</phrase> minimization in pose (<phrase>translation</phrase>, rotation) space. as the object moves, the 3d <phrase>model</phrase> is incrementally updated. a virtuous <phrase>circle</phrase> emerges: tracking enables composition of the partial 3d <phrase>model</phrase>; the 3d <phrase>model</phrase> facilitates and robustifies the <phrase>multi-view</phrase> tracking. we demonstrate experimentally that the interleaved <phrase>track</phrase>-and-reconstruct approach successfully tracks a 360 degrees turn-around and a wide <phrase>range</phrase> of motions. <phrase>monocular</phrase> tracking is also possible after the <phrase>model</phrase> is constructed. using more cameras, however, significantly increases stability in critical poses and moves. we demonstrate how to exploit the 3d <phrase>model</phrase> to increases stability in the presence of uneven and/or changing illumination.
hemispherical <phrase>harmonic</phrase> surface description and applications to <phrase>medical</phrase> <phrase>image analysis</phrase>.
the use of surface <phrase>harmonics</phrase> for rigid and nonrigid shape description is well known. in this <phrase>paper</phrase> we define a set of complete hemispherical <phrase>harmonic</phrase> basis functions on a hemisphere domain and propose a novel parametric shape description method to efficiently and flexibly represent the surfaces of anatomical structures in <phrase>medical</phrase> images. as the first application of hemispherical <phrase>harmonic</phrase> theory in shape description, our technique differs from the previous surface <phrase>harmonics</phrase> shape descriptors, all of which don't work efficiently on the hemisphere-like objects that often exist in <phrase>medical</phrase> anatomical structures (e.g., ventricles, atriums, etc.). we demonstrate the effectiveness of our approach through theoretic and <phrase>experimental</phrase> exploration of a set of <phrase>medical</phrase> image applications. furthermore, an evaluation criterion for surface modeling efficiency is described and the comparison <phrase>results</phrase> demonstrated that our method outperformed the previous approaches using <phrase>spherical harmonic</phrase> models.
<phrase>belief propagation</phrase> for <phrase>panorama</phrase> generation.
we present an <phrase>algorithm</phrase> for generating panoramic images of complex scenes from a multi-<phrase>sensor</phrase> <phrase>camera</phrase>. we further present a programmable <phrase>graphics hardware</phrase> implementation to process the large <phrase>data</phrase> sets more quickly. because the sensors do not share the same <phrase>center</phrase> of projection, nearby objects may not be properly aligned, creating a ghosting or echoing effect in the generated <phrase>panorama</phrase>, unless correct depth <phrase>information</phrase> is taken into account. taking a cue from the similar problem of dense <phrase>stereo</phrase>, we approximate our scene with a <phrase>markov random field</phrase> and use <phrase>belief propagation</phrase> to estimate the maximum a posteriori panoramic image for that scene.
<phrase>reflectance</phrase> modeling for layered dielectrics with rough surface boundaries.
a new <phrase>model</phrase> for the <phrase>scattering</phrase> of <phrase>light</phrase> from layered dielectrics with rough surface boundaries is introduced. the <phrase>model</phrase> contains a surface <phrase>scattering</phrase> component together with a <phrase>subsurface scattering</phrase> component. the former component corresponds to the roughness on the <phrase>upper</phrase> surface boundary and is modeled using the modified beckmann <phrase>model</phrase>. the latter component accounts for both <phrase>refraction</phrase> due to <phrase>fresnel</phrase> transmission through the layer and rough <phrase>scattering</phrase> at the <phrase>lower</phrase> layer boundary. by allowing <phrase>independent</phrase> roughness parameters for each surface boundary we can achieve excellent fits of the <phrase>model</phrase> to the measured brdf <phrase>data</phrase>. using a well known method of testing <phrase>reflectance</phrase> models, we experiment with brdf <phrase>data</phrase> from <phrase>skin</phrase> surface samples (<phrase>human</phrase> volunteers) and show that the new <phrase>model</phrase> outperforms <phrase>alternative</phrase> variants of the beckmann <phrase>model</phrase> and the lafortune et al. <phrase>reflectance</phrase> <phrase>model</phrase>. as an application in computer graphics, we also show that realistic images of 3d surfaces can be generated using the new <phrase>model</phrase>, by setting the values of its physical parameters.
<phrase>multi-view</phrase> multi-exposure <phrase>stereo</phrase>.
<phrase>multi-view</phrase> <phrase>stereo</phrase> <phrase>algorithms</phrase> typically rely on same-exposure images as inputs due to the brightness constancy assumption. while <phrase>state</phrase>-of-the-<phrase>art</phrase> depth <phrase>results</phrase> are excellent, they do not produce <phrase>high</phrase>-<phrase>dynamic range</phrase> textures required for <phrase>high</phrase>-quality view <phrase>reconstruction</phrase>. in this <phrase>paper</phrase>, we propose a technique that adapts <phrase>multi-view</phrase> <phrase>stereo</phrase> for different exposure inputs to simultaneously recover reliable dense depth and <phrase>high</phrase> <phrase>dynamic range</phrase> textures. in our technique, we use an exposure-invariant similarity statistic to establish correspondences, through which we robustly extract the <phrase>camera</phrase> <phrase>radiometric</phrase> response <phrase>function</phrase> and the image exposures. this enables us to then convert all images to <phrase>radiance</phrase> space and selectively use the <phrase>radiance</phrase> <phrase>data</phrase> for dense depth and <phrase>high</phrase> <phrase>dynamic range</phrase> texture recovery. we show <phrase>results</phrase> for <phrase>synthetic and real</phrase> scenes.
synthesis of 3d <phrase>model</phrase> of a <phrase>magnetic field</phrase>-influenced body from a <phrase>single</phrase> image.
a method for recovery of a 3d <phrase>model</phrase> of a <phrase>planet</phrase>-sized <phrase>cloud</phrase>-like structure that is in motion and deforming but approximately governed by <phrase>magnetic field</phrase> properties is described. the method allows recovery of the <phrase>model</phrase> from a <phrase>single</phrase> intensity image in which the structure's <phrase>silhouette</phrase> can be observed. the method exploits envelope theory and a <phrase>magnetic field</phrase> <phrase>model</phrase>. given one intensity image and the segmented <phrase>silhouette</phrase> in the image, the method proceeds without <phrase>human</phrase> intervention to produce the 3d <phrase>model</phrase>. in addition to allowing 3d <phrase>model</phrase> synthesis, the method's capability to yield a very compact description offers further utility. application of the method to <phrase>real-world</phrase> <phrase>data</phrase> is also demonstrated.
a factorization based self-calibration for radially symmetric cameras.
the <phrase>paper</phrase> proposes a novel approach for planar self-calibration of radially symmetric cameras. we <phrase>model</phrase> these <phrase>camera</phrase> images using notions of <phrase>distortion</phrase> <phrase>center</phrase> and concentric <phrase>distortion</phrase> circles around it. the <phrase>rays</phrase> corresponding to <phrase>pixels</phrase> lying on a <phrase>single</phrase> <phrase>distortion</phrase> <phrase>circle</phrase> form a right circular cone. each of these cones is associated with two unknowns; optical <phrase>center</phrase> and <phrase>focal length</phrase> (opening angle). in the central case, we consider all <phrase>distortion</phrase> circles to have the same optical <phrase>center</phrase>, whereas in the non-central case they have different optical centers lying on the same <phrase>optical axis</phrase>. based on this <phrase>model</phrase> we provide a factorization based self-calibration <phrase>algorithm</phrase> for planar scenes from dense image matches. our formulation provides a rich set of constraints to validate the correctness of the <phrase>distortion</phrase> <phrase>center</phrase>. we also propose possible extensions of this <phrase>algorithm</phrase> in terms of non-planar scenes, non-unit <phrase>aspect ratio</phrase> and <phrase>multi-view</phrase> constraints. <phrase>experimental</phrase> <phrase>results</phrase> are shown.
3d <phrase>content-based</phrase> search based on 3d krawtchouk moments.
in this <phrase>paper</phrase> a novel method for 3d <phrase>content-based</phrase> search and retrieval is proposed. guided by the imperative need for a reliable 3d <phrase>content based</phrase> search tool and the very interesting <phrase>results</phrase> of <phrase>research</phrase> work done in the past on the performance of krawtchouk moments and krawtchouk moment invariants in <phrase>image processing</phrase>, weighted 3d krawtchouk moments are introduced for efficient 3d analysis which are suitable for <phrase>content-based</phrase> search and retrieval applications. <phrase>the proposed method</phrase> was tested on <phrase>princeton</phrase> shape benchmark. experiments have shown that <phrase>the proposed method</phrase> is <phrase>superior</phrase> in terms of precision-recall comparing with other well-known methods reported in the <phrase>literature</phrase>.
a csc based classification method for ct <phrase>bone</phrase> images.
the csc (color structure code) is a robust and fast two dimensional segmentation method which has been already generalized to <phrase>three dimensional</phrase> images. as the csc does not need any prior <phrase>knowledge</phrase> it can be used for different applications. in this <phrase>paper</phrase> we focus on the segmentation of <phrase>bones</phrase> from computer tomography <phrase>data</phrase> (ct) with the csc. in the postprocessing step csc segments will be classified according to their <phrase>average</phrase> hounsfield value. the classification is steered by some application specific <phrase>topological</phrase> rules.
illumination insensitive <phrase>model</phrase>-based 3d object tracking and texture refinement.
a common approach to <phrase>model</phrase>-based tracking is to use a <phrase>model</phrase> of the object to predict what will be observed, and then to compare that with real observations. for methods that make use of the object's <phrase>photometric</phrase> properties (appearance) in their measurements, illumination inconsistencies between the modeled and actual scene can cause tracking problems. in this <phrase>paper</phrase> we address one case: <phrase>model</phrase>-based tracking of lambertian objects under directional <phrase>light</phrase> sources. we present an iterative optimization method that uses a <phrase>kalman filter</phrase> to simultaneously refine estimates of the object motion, the illumination, and the <phrase>model</phrase> texture. we <phrase>model</phrase> the illumination <phrase>variance</phrase> between the real and predicted observation using the intensity ratios of corresponding surface points, which we then use to make <phrase>model</phrase>-based image predictions consistent with the real lighting. to demonstrate the effectiveness of our method we present <phrase>experimental</phrase> <phrase>results</phrase> using both synthetic (controlled) and real <phrase>image sequences</phrase>.
object centered <phrase>stereo</phrase>: displacement map estimation using texture and shading.
we consider the problem of recovering 3d surface displacements using both shading and <phrase>multi-view</phrase> <phrase>stereo</phrase> cues. in contrast to traditional disparity or <phrase>depth map</phrase> representations, the object centered displacement map representation enables the recovery of complete 3d objects while also ensuring the <phrase>reconstruction</phrase> is not biased towards a particular image. although displacement mapping requires a base surface, this base mesh is easily obtained using traditional computer vision techniques (e.g., <phrase>shape-from-silhouette</phrase> or <phrase>structure-from-motion</phrase>). our method exploits shading variation due to object rotation relative to the <phrase>light</phrase> source, allowing the recovery of displacements in both textured and textureless regions in a common framework. in particular, shading cues are integrated into a <phrase>multi-view</phrase> <phrase>stereo</phrase> photo-consistency <phrase>function</phrase> through the <phrase>surface normals</phrase> that are implied by the displacement map. the analytic <phrase>gradient</phrase> of this photo-consistency <phrase>function</phrase> is used to drive a multiresolution conjugate <phrase>gradient</phrase> optimization. we demonstrate the geometric quality of the reconstructed displacements on several example objects including a <phrase>human</phrase> face.
flatness and orientation signature for modeling and matching 3d objects.
this <phrase>paper</phrase> proposes a new technique for modeling and matching <phrase>three-dimensional</phrase> rigid objects by encoding the fluctuation of the surface and the variation of its normal around an oriented point on the surface as the surface expands. the surface of the object is encoded into two two-dimensional curves as the surface signature on each point, and then the collection of the signatures are used to <phrase>model</phrase> and match the object. the signatures implicitly encode the <phrase>curvature</phrase> and <phrase>symmetry</phrase> of the surface around an oriented point. this modeling technique is robust to scale, orientation, noise, patch resolution, occlusion, and cluttering.
<phrase>large-scale</phrase> modeling of parametric surfaces using <phrase>spherical harmonics</phrase>.
we present an approach for <phrase>large-scale</phrase> modeling of parametric surfaces using <phrase>spherical harmonics</phrase> (shs). a standard least square fitting (lsf) method for sh expansion is not scalable and cannot accurately <phrase>model</phrase> large 3d surfaces. we propose an iterative residual fitting (irf) <phrase>algorithm</phrase>, and demonstrate its effectiveness and <phrase>scalability</phrase> in creating accurate sh models for large 3d surfaces. these <phrase>large-scale</phrase> and accurate parametric models can be used in many applications in computer vision, graphics, and biomedical imaging. as a simple extension of lsf, irf is very easy to implement and requires few machine resources.
<phrase>graph</phrase> cut based <phrase>multiple view</phrase> segmentation for 3d <phrase>reconstruction</phrase>.
in this <phrase>paper</phrase> we propose a novel framework for efficiently extracting foreground objects in so called shortbaseline <phrase>image sequences</phrase>. we apply the obtained segmentation to improve subsequent 3d <phrase>reconstruction</phrase> <phrase>results</phrase>. essentially, our framework combines a <phrase>graph</phrase> cut based optimization <phrase>algorithm</phrase> with an intuitive <phrase>user interface</phrase>. at first a meanshift segmentation <phrase>algorithm</phrase> <phrase>partitions</phrase> each image of the <phrase>sequence</phrase> into a certain number of regions. additionally we provide an intelligent <phrase>graphical user interface</phrase> for easy specification of foreground as well as background regions across all images of the <phrase>sequence</phrase>. within the <phrase>graph</phrase> cut optimization <phrase>algorithm</phrase> we define new <phrase>energy</phrase> terms to increase the robustness and to keep the segmentation of the foreground object coherent across all images of the <phrase>sequence</phrase>. finally, a refined <phrase>graph</phrase> cut segmentation and several adjustment operations allow an accurate and effective foreground extraction. the obtained <phrase>results</phrase> are demonstrated on several <phrase>real world</phrase> <phrase>data</phrase> sets.
depth images: representations and <phrase>real-time rendering</phrase>.
depth images are viable representations that can be computed from the <phrase>real world</phrase> using cameras and/or other scanning devices. the <phrase>depth map</phrase> provides 2\frac{1}{2}d structure of the scene. a set of depth images can provide hole-<phrase>free</phrase> rendering of the scene. <phrase>multiple views</phrase> need to blended to provide smooth hole-<phrase>free</phrase> rendering, however. such a representation of the scene is bulky and needs good <phrase>algorithms</phrase> for <phrase>real-time rendering</phrase> and efficient representation. in this <phrase>paper</phrase>, we present a discussion on the depth image representation and provide a <phrase>gpu</phrase>-based <phrase>algorithm</phrase> that can render large models represented using dis in real time. we then present a proxy-based compression scheme for depth images and provide <phrase>results</phrase> for the same. <phrase>results</phrase> are shown on synthetic scenes under different conditions and on some scenes generated from images. lastly, we initiate discussion on varying quality levels in ibr and show a way to create representations using dis with different <phrase>trade</phrase>-offs between <phrase>model</phrase> size and rendering quality. this enables the use of this representation for a <phrase>variety</phrase> of rendering situations.
how far can we go with local optimization in real-time <phrase>stereo</phrase> matching.
applications such as <phrase>robot</phrase> <phrase>navigation</phrase> and <phrase>augmented reality</phrase> require <phrase>high</phrase>-accuracy dense disparity maps in real-time and online. due to time constraint, most real-time <phrase>stereo</phrase> applications rely on local winner-take-all optimization in the disparity computation process. these local approaches are generally outperformed by offline <phrase>global optimization</phrase> based <phrase>algorithms</phrase>. however, recent <phrase>research</phrase> shows that, through carefully selecting and aggregating the matching costs of neighboring <phrase>pixels</phrase>, the disparity maps <phrase>produced</phrase> by a local approach can be more accurate than those generated by many <phrase>global optimization</phrase> techniques. we are therefore motivated to investigate whether these cost aggregation approaches can be adopted in real-time <phrase>stereo</phrase> applications and, if so, how well they perform under the real-time constraint. the evaluation is conducted on a real-time <phrase>stereo</phrase> platform, which utilizes the processing power of programmable <phrase>graphics hardware</phrase>. several recent cost aggregation approaches are also implemented and optimized for <phrase>graphics hardware</phrase> so that real-time speed can be achieved. the performances of these aggregation approaches in terms of both processing speed and result quality are reported.
a <phrase>bayesian</phrase> approach to building footprint extraction from aerial <phrase>lidar</phrase> <phrase>data</phrase>.
building footprints have been shown to be extremely useful in <phrase>urban planning</phrase>, <phrase>infrastructure</phrase> development, and roof modeling. current methods for creating these footprints are often highly manual and rely largely on <phrase>architectural</phrase> blueprints or skilled modelers. in this work we will use aerial <phrase>lidar</phrase> <phrase>data</phrase> to generate building footprints automatically. existing automatic methods have been mostly unsuccessful due to large amounts of noise around building edges. we present a novel <phrase>bayesian</phrase> technique for automatically constructing building footprints from a pre-classified <phrase>lidar</phrase> <phrase>point cloud</phrase>. our <phrase>algorithm</phrase> first computes a boundederror approximate building footprint using an application of the <phrase>shortest path</phrase> <phrase>algorithm</phrase>. we then determine the most probable building footprint by maximizing the <phrase>posterior probability</phrase> using linear optimization and <phrase>simulated annealing</phrase> techniques. we have applied our <phrase>algorithm</phrase> to more than 300 buildings in our <phrase>data set</phrase> and observe that we obtain accurate building footprints compared to the <phrase>ground truth</phrase>. our <phrase>algorithm</phrase> is automatic and can be applied to other man-made shapes such as roads and <phrase>telecommunication</phrase> lines with minor modifications.
a system for reconstructing integrated texture maps for large structures.
we consider the problem of creating integrated texture maps of large structures scanned with a time-of-flight <phrase>laser</phrase> scanner and imaged with a <phrase>digital camera</phrase>. the key issue in creating integrated textures is correcting for the spatially varying illumination across the structure. in most cases, the illumination cannot be controlled, and dense spatial estimates of illumination are not possible. we present a system for processing multiple <phrase>color images</phrase> into an integrated texture that makes use of the <phrase>laser</phrase> scanner return intensity and the captured <phrase>geometry</phrase>, together with color balancing and mapping of illumination-corrected images onto the <phrase>target</phrase> <phrase>geometry</phrase> after filtering into two <phrase>spatial frequency</phrase> bands.
motion <phrase>editing</phrase> in 3d <phrase>video</phrase> <phrase>database</phrase>.
as the next generation of <phrase>media</phrase>, 3d <phrase>video</phrase> is attracting increased attention. 3d <phrase>video</phrase> is a <phrase>sequence</phrase> of <phrase>three dimensional</phrase> mesh models, captured and generated for a real dynamic object. in this <phrase>paper</phrase>, we present a simple framework of motion <phrase>editing</phrase> in 3d <phrase>video</phrase> <phrase>database</phrase> to re-use 3d <phrase>video</phrase> <phrase>data</phrase>. our system is composed of two modules. in the first module, a motion <phrase>database</phrase> is automatically set up from original 3d <phrase>video</phrase> sequences off-line by analyzing the feature vectors of each frame. it is observed that our original 3d <phrase>video</phrase> sequences have a two-level temporal structure. a fine-to-coarse method is proposed to extract such a structure. 3d <phrase>video</phrase> is segmented into the fine-level structure by a three <phrase>reference frame</phrase> strategy and then is clustered into the coarse-level structure. in the second module, users can synthesize the motions to edit a new 3d <phrase>video</phrase> <phrase>sequence</phrase> online. a cost <phrase>function</phrase> is optimized to transit between two motions with the users' requirements. all the <phrase>algorithms</phrase> in the system are based on the analysis in feature <phrase>vector space</phrase> and the edited 3d <phrase>video</phrase> <phrase>sequence</phrase> is played using <phrase>opengl</phrase>.
the asdmcon project: the challenge of detecting defects on <phrase>construction</phrase> sites.
techniques for <phrase>three dimensional</phrase> (3d) imaging and analysis of as-built conditions of buildings are gaining acceptance in the <phrase>architecture</phrase>, <phrase>engineering</phrase>, and <phrase>construction</phrase> (aec) <phrase>community</phrase>. early detection of defects on <phrase>construction</phrase> sites is one domain where these techniques have the potential to revolutionize an <phrase>industry</phrase>, since <phrase>construction</phrase> defects can consume a significant portion of a project's budget. the asdmcon project is developing methods to aid site managers in detecting and managing <phrase>construction</phrase> defects using 3d imaging and other advanced <phrase>sensor</phrase> technologies. this <phrase>paper</phrase> presents an overview of the project, its 4d visualization environment, and the 3d segmentation and recognition strategies that are being employed to automate defect detection.
<phrase>high</phrase>-performance <phrase>multi-view</phrase> <phrase>reconstruction</phrase>.
we present a <phrase>high</phrase> performance <phrase>reconstruction</phrase> approach, which generates true 3d models from <phrase>multiple views</phrase> with known <phrase>camera</phrase> parameters. the complete pipeline from <phrase>depth map</phrase> generation over depth image integration to the final 3d <phrase>model</phrase> visualization is performed on programmable graphics processing units (<phrase>gpus</phrase>). the proposed pipeline is suitable for <phrase>long</phrase> <phrase>image sequences</phrase> and uses a plane-sweep depth estimation procedure optionally employing robust image similarity functions to generate a set of depth images. the subsequent volumetric <phrase>fusion</phrase> step combines these depth maps into an impicit surface representation of the final <phrase>model</phrase>, which can be directly displayed using <phrase>gpu</phrase>-based raycasting methods. depending on the number of input views and the desired resolution of the final <phrase>model</phrase> the <phrase>computing</phrase> times <phrase>range</phrase> from several seconds to a few minutes. the quality of the obtained models is illustrated with <phrase>real-world</phrase> datasets.
multiple <phrase>camera</phrase> calibration using robust perspective factorization.
in this <phrase>paper</phrase> we address the problem of recovering structure and motion from a large number of intrinsically calibrated perspective cameras. we describe a method that combines (1) weak-perspective <phrase>reconstruction</phrase> in the presence of noisy and missing <phrase>data</phrase> and (2) an <phrase>algorithm</phrase> that updates weak-perspective <phrase>reconstruction</phrase> to perspective <phrase>reconstruction</phrase> by incrementally estimating the <phrase>projective</phrase> depths. the method also solves for the reversal ambiguity associated with affine factorization techniques. the method has been successfully applied to the problem of calibrating the external parameters (position and orientation) of several multiple-<phrase>camera</phrase> setups. <phrase>results</phrase> obtained with synthetic and <phrase>experimental</phrase> <phrase>data</phrase> compare favourably with <phrase>results</phrase> obtained with nonlinear minimization such as bundle adjustment.
angle <phrase>independent</phrase> bundle adjustment refinement.
obtaining a <phrase>digital</phrase> <phrase>model</phrase> of a <phrase>real-world</phrase> 3d scene is a challenging task pursued by computer vision and computer graphics. given an initial approximate 3d <phrase>model</phrase>, a popular refinement process is to perform a bundle adjustment of the estimated <phrase>camera</phrase> position, <phrase>camera</phrase> orientation, and scene points. unfortunately, simultaneously solving for both <phrase>camera</phrase> position and <phrase>camera</phrase> orientation is an ill-conditioned problem. to address this issue, we propose an improved, <phrase>camera</phrase>-orientation <phrase>independent</phrase> cost <phrase>function</phrase> that can be used instead of the standard bundle adjustment cost <phrase>function</phrase>. this yields a new bundle adjustment formulation which exhibits noticeably better numerical behavior, but at the expense of an increased computational cost. we alleviate the additional cost by automatically partitioning the dataset into smaller subsets. minimizing our cost <phrase>function</phrase> for these subsets still achieves significant error reduction over standard bundle adjustment. we empirically demonstrate our formulation using several different size models and <phrase>image sequences</phrase>.
<phrase>image based</phrase> localization in <phrase>urban</phrase> environments.
